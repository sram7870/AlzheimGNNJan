{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyPhxkymjd/tCj3I4fAk6ah+"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "**Interesting Title Here**\n",
        "Siva Subramanian Ram, Mary Institute and Saint Louis Country Day School\n",
        "\n",
        "Abstract\n",
        "--------\n",
        "Alzheimer's disease (AD) is increasingly characterized as a disorder of large-scale\n",
        "brain network disintegration rather than isolated regional dysfunction. This project\n",
        "models individual brains as time-evolving graphs derived from resting-state fMRI\n",
        "(rs-fMRI) and wires the codebase to run directly from ADNI-preprocessed ROI\n",
        "timeseries. Nodes represent anatomically defined brain regions and edges encode\n",
        "dynamic functional connectivity. Temporal evolution is modeled with continuous-time\n",
        "graph neural ODEs and attention-based interpretability is provided for node/edge\n",
        "attributions relevant to AD progression.\n",
        "\n",
        "This file contains utilities for: preprocessing ADNI ROI timeseries, constructing\n",
        "node features (ALFF, ReHo proxy, rolling activation statistics), empirical\n",
        "connectivity, a learned edge generator, and a dataset class `ADNIDataset` that\n",
        "loads ADNI-derived timeseries and clinical labels for longitudinal prediction.\n",
        "\n",
        "High-level motivation\n",
        "---------------------\n",
        "Alzheimerâ€™s disease (AD) is increasingly characterized as a disorder of large-scale brain network disintegration rather than isolated regional dysfunction. Neuroimaging work highlights early disruption of functional connectivity (particularly within hub-dominated systems such as the Default Mode Network), followed by compensatory reorganization and progressive network collapse. Standard ML approaches that use static or Euclidean representations can miss the temporally-evolving network-driven patterns that are clinically meaningful. This project reframes AD modeling as a graph learning problem: nodes are brain regions whose features capture neurophysiological descriptors (ALFF, ReHo proxies, rolling activation statistics), while edges encode dynamic functional connectivity. To capture deteriorating connectivity and smooth longitudinal changes, we model temporal evolution via continuous-time graph neural ODEs (Neural ODE GNNs). Neural ODEs naturally accommodate irregular longitudinal sampling and missing visits by integrating graph-parameterized instantaneous dynamics over arbitrary time grids, enabling smooth subject-level latent trajectories with variable inter-scan intervals common in longitudinal neuroimaging. For interpretability we provide attention-based edge weighting and node-level attributions (PX node-feature masks, PA edge attention, gradient/IG-based saliency) so clinicians and researchers can identify disease-relevant ROIs and subnetworks and inspect subject-level trajectories.\n",
        "\n",
        "Notes on ADNI\n",
        "-------------\n",
        "This code expects ADNI-derived ROI timeseries (per-subject files) organized as\n",
        "one file per session with a manifest CSV that lists subject IDs, session dates,\n",
        "diagnosis (CN/MCI/AD), and path to the ROI timeseries file (supported formats: .npy, .npz, .mat, .csv).\n",
        "See the `README.md` for instructions on pulling ADNI data and converting to ROI timeseries.\n",
        "\"\"\"\n",
        "\n",
        "import os\n",
        "import sys\n",
        "import json\n",
        "import csv\n",
        "import math\n",
        "import random\n",
        "import warnings\n",
        "from pathlib import Path\n",
        "import numpy as np\n",
        "from typing import Optional, Union, Dict, Tuple, List\n",
        "import logging\n",
        "import argparse\n",
        "try:\n",
        "    import pandas as pd\n",
        "except Exception:\n",
        "    pd = None\n",
        "    # pandas is optional for some utilities; continue even if it's not installed\n",
        "\n",
        "import scipy.io\n",
        "# nibabel is optional; import dynamically to avoid static analyzer/reportMissingImports errors when it's not installed\n",
        "try:\n",
        "    import importlib\n",
        "    nib = importlib.import_module('nibabel')\n",
        "except Exception:\n",
        "    nib = None\n",
        "\n",
        "# Enforce core required packages at import time with clear message\n",
        "_required_missing = []\n",
        "try:\n",
        "    import torch\n",
        "except Exception:\n",
        "    _required_missing.append('torch')\n",
        "try:\n",
        "    import scipy\n",
        "except Exception:\n",
        "    if 'scipy' not in _required_missing:\n",
        "        _required_missing.append('scipy')\n",
        "if _required_missing:\n",
        "    raise ModuleNotFoundError(\n",
        "        'Missing required Python packages: ' + ', '.join(_required_missing)\n",
        "        + \". Install them (e.g. `python3 -m pip install \" + ' '.join(_required_missing) + \"`) before importing this module.\"\n",
        "    )\n",
        "# ---------------------- Neuroscience utilities (features & connectivity) ----------------------\n",
        "\n",
        "def compute_empirical_fc_from_timeseries(timeseries: np.ndarray) -> np.ndarray:\n",
        "    \"\"\"Compute empirical functional connectivity (Pearson correlation) from ROI timeseries.\n",
        "\n",
        "    Args:\n",
        "        timeseries: (T, N) array of BOLD-like signals (time x regions)\n",
        "\n",
        "    Returns:\n",
        "        fc: (N, N) correlation matrix (values in [ -1, 1 ])\n",
        "    \"\"\"\n",
        "    if timeseries.ndim != 2:\n",
        "        raise ValueError('timeseries must be (T, N)')\n",
        "    # Transpose to (N, T) for np.corrcoef convenience\n",
        "    fc = np.corrcoef(timeseries.T)\n",
        "    # numerical stability\n",
        "    fc = np.nan_to_num(fc)\n",
        "    # clamp to [0,1] for connectivity strength (we'll keep absolute value)\n",
        "    return np.clip(np.abs(fc), 0.0, 1.0)\n",
        "\n",
        "\n",
        "def compute_alff(timeseries: np.ndarray, fs: float = 0.5, low: float = 0.01, high: float = 0.08) -> np.ndarray:\n",
        "    \"\"\"Compute ALFF (amplitude of low-frequency fluctuations) per ROI.\n",
        "\n",
        "    Args:\n",
        "        timeseries: (T, N)\n",
        "        fs: sampling frequency in Hz (typical fMRI TR=2s -> fs=0.5)\n",
        "        low, high: frequency band for ALFF\n",
        "\n",
        "    Returns:\n",
        "        alff: (N,) ALFF values normalized per subject\n",
        "    \"\"\"\n",
        "    try:\n",
        "        from scipy.signal import welch\n",
        "    except Exception:\n",
        "        raise RuntimeError('scipy required for ALFF calculation')\n",
        "\n",
        "    T, N = timeseries.shape\n",
        "    alff = np.zeros(N, dtype=np.float32)\n",
        "    for i in range(N):\n",
        "        f, Pxx = welch(timeseries[:, i], fs=fs, nperseg=min(256, max(8, T//2)))\n",
        "        band_mask = (f >= low) & (f <= high)\n",
        "        alff[i] = Pxx[band_mask].sum() if band_mask.any() else 0.0\n",
        "    # normalize\n",
        "    if alff.max() > 0:\n",
        "        alff = alff / (alff.max() + 1e-9)\n",
        "    return alff\n",
        "\n",
        "\n",
        "def compute_reho_proxy(timeseries: np.ndarray, adjacency: Optional[np.ndarray] = None, k: int = 6) -> np.ndarray:\n",
        "    \"\"\"Approximate ReHo by mean pairwise Spearman correlation among a node and its neighbors.\n",
        "\n",
        "    This is a pragmatic ROI-level proxy for voxel-wise Kendall W ReHo.\n",
        "\n",
        "    Args:\n",
        "        timeseries: (T, N)\n",
        "        adjacency: optional (N, N) adjacency to define neighbors; if None use correlation-based neighbors\n",
        "        k: number of neighbors to consider\n",
        "\n",
        "    Returns:\n",
        "        reho: (N,) values normalized\n",
        "    \"\"\"\n",
        "    try:\n",
        "        from scipy.stats import spearmanr\n",
        "    except Exception:\n",
        "        raise RuntimeError('scipy required for ReHo proxy')\n",
        "\n",
        "    T, N = timeseries.shape\n",
        "    if adjacency is None:\n",
        "        # derive adjacency via correlation\n",
        "        adj = compute_empirical_fc_from_timeseries(timeseries)\n",
        "    else:\n",
        "        adj = np.array(adjacency, dtype=float)\n",
        "\n",
        "    reho = np.zeros(N, dtype=np.float32)\n",
        "    for i in range(N):\n",
        "        neighbors = np.argsort(-adj[i, :])[: k + 1]\n",
        "        # include self\n",
        "        if i not in neighbors:\n",
        "            neighbors = np.concatenate(([i], neighbors[:-1]))\n",
        "        # extract time series for these nodes (T, m)\n",
        "        block = timeseries[:, neighbors]\n",
        "        # compute pairwise spearman correlations and average\n",
        "        if block.shape[1] <= 1:\n",
        "            reho[i] = 0.0\n",
        "            continue\n",
        "        rho_mat = np.corrcoef(block.T)  # approximate using Pearson on ranked signals\n",
        "        reho[i] = np.nanmean(np.abs(rho_mat))\n",
        "    if reho.max() > 0:\n",
        "        reho = reho / (reho.max() + 1e-9)\n",
        "    return reho\n",
        "\n",
        "\n",
        "def node_features_from_timeseries(timeseries: np.ndarray, adjacency: Optional[np.ndarray] = None, fs: float = 0.5) -> np.ndarray:\n",
        "    \"\"\"Construct node features (T, N, F) from raw ROI timeseries.\n",
        "\n",
        "    Features per node: mean activation (over short window), std, ALFF, ReHo.\n",
        "    For ALFF/ReHo we compute a subject-level value and broadcast across timepoints.\n",
        "    \"\"\"\n",
        "    # timeseries: (T, N)\n",
        "    T, N = timeseries.shape\n",
        "    # mean and std per timepoint: here we compute rolling window mean/std of window length 5 (or T if small)\n",
        "    w = min(5, T)\n",
        "    means = np.zeros((T, N), dtype=np.float32)\n",
        "    stds = np.zeros((T, N), dtype=np.float32)\n",
        "    for t in range(T):\n",
        "        s = max(0, t - w + 1)\n",
        "        block = timeseries[s: t + 1]\n",
        "        means[t] = block.mean(axis=0)\n",
        "        stds[t] = block.std(axis=0)\n",
        "\n",
        "    alff = compute_alff(timeseries, fs=fs)\n",
        "    reho = compute_reho_proxy(timeseries, adjacency=adjacency)\n",
        "\n",
        "    # broadcast alff/reho across time dimension\n",
        "    alff_t = np.tile(alff.reshape(1, N), (T, 1))\n",
        "    reho_t = np.tile(reho.reshape(1, N), (T, 1))\n",
        "\n",
        "    features = np.stack([means, stds, alff_t, reho_t], axis=-1)  # (T, N, 4)\n",
        "    return features.astype(np.float32)\n",
        "\n",
        "\n",
        "def _sparsify_adj(adj: np.ndarray, top_percent: float = 0.1) -> np.ndarray:\n",
        "    \"\"\"Sparsify adjacency by keeping top_percent of edges (by value) while ensuring\n",
        "    each node has at least one connection.\n",
        "    \"\"\"\n",
        "    A = adj.copy()\n",
        "    N = A.shape[0]\n",
        "    if top_percent <= 0 or top_percent >= 1.0:\n",
        "        return A\n",
        "    tri_idx = np.triu_indices(N, k=1)\n",
        "    vals = A[tri_idx]\n",
        "    k = max(1, int(len(vals) * top_percent))\n",
        "    thresh = np.partition(vals.ravel(), -k)[-k]\n",
        "    mask = (A >= thresh).astype(float)\n",
        "    # ensure symmetry\n",
        "    mask = np.maximum(mask, mask.T)\n",
        "    # ensure each node has at least one edge\n",
        "    for i in range(N):\n",
        "        if mask[i].sum() == 0:\n",
        "            j = int(np.argmax(A[i]))\n",
        "            mask[i, j] = 1.0\n",
        "            mask[j, i] = 1.0\n",
        "    A = A * mask\n",
        "    # renormalize\n",
        "    if A.max() > 0:\n",
        "        A = A / (A.max() + 1e-12)\n",
        "    return A\n",
        "\n",
        "\n",
        "def compute_mutual_info_matrix(timeseries: np.ndarray, n_bins: int = 16) -> np.ndarray:\n",
        "    \"\"\"Estimate pairwise mutual information by histogram discretization.\n",
        "    Returns (N,N) symmetric MI matrix.\n",
        "    \"\"\"\n",
        "    T, N = timeseries.shape\n",
        "    # discretize each signal\n",
        "    hist_bins = np.linspace(np.min(timeseries), np.max(timeseries), n_bins + 1)\n",
        "    disc = np.digitize(timeseries, hist_bins) - 1\n",
        "    mi = np.zeros((N, N), dtype=np.float32)\n",
        "    for i in range(N):\n",
        "        for j in range(i, N):\n",
        "            try:\n",
        "                m = mutual_info_score(disc[:, i], disc[:, j])\n",
        "            except Exception:\n",
        "                m = 0.0\n",
        "            mi[i, j] = m\n",
        "            mi[j, i] = m\n",
        "    # normalize\n",
        "    if mi.max() > 0:\n",
        "        mi = mi / (mi.max() + 1e-12)\n",
        "    return mi\n",
        "\n",
        "\n",
        "def compute_coherence_matrix(timeseries: np.ndarray, fs: float = 0.5) -> np.ndarray:\n",
        "    \"\"\"Compute pairwise coherence using scipy.signal.coherence; fallback to abs(corr).\n",
        "    \"\"\"\n",
        "    T, N = timeseries.shape\n",
        "    coh = np.zeros((N, N), dtype=np.float32)\n",
        "    try:\n",
        "        for i in range(N):\n",
        "            for j in range(i, N):\n",
        "                f, Cxy = coherence(timeseries[:, i], timeseries[:, j], fs=fs, nperseg=min(256, max(8, T//2)))\n",
        "                val = np.mean(Cxy)\n",
        "                coh[i, j] = val\n",
        "                coh[j, i] = val\n",
        "    except Exception:\n",
        "        # fallback: use absolute Pearson correlation\n",
        "        coh = compute_empirical_fc_from_timeseries(timeseries)\n",
        "    if coh.max() > 0:\n",
        "        coh = coh / (coh.max() + 1e-12)\n",
        "    return coh\n",
        "\n",
        "\n",
        "def compute_multimodal_adjacency(timeseries: np.ndarray, weights: Optional[Dict[str, float]] = None, fs: float = 0.5, sparsify_pct: float = 0.1) -> np.ndarray:\n",
        "    \"\"\"Combine Pearson correlation, MI, and coherence into a single adjacency.\n",
        "\n",
        "    weights: dict with keys 'pearson','mi','coh'\n",
        "    \"\"\"\n",
        "    if weights is None:\n",
        "        weights = {'pearson': 0.6, 'mi': 0.2, 'coh': 0.2}\n",
        "    R = compute_empirical_fc_from_timeseries(timeseries)\n",
        "    I = compute_mutual_info_matrix(timeseries)\n",
        "    C = compute_coherence_matrix(timeseries, fs=fs)\n",
        "    A = weights.get('pearson', 0.0) * R + weights.get('mi', 0.0) * I + weights.get('coh', 0.0) * C\n",
        "    A = np.clip(A, 0.0, None)\n",
        "    A = _sparsify_adj(A, top_percent=sparsify_pct)\n",
        "    return A.astype(np.float32)\n",
        "\n",
        "\n",
        "# ---------------------- Edge generator: disease-aware learned edges ----------------------\n",
        "\n",
        "from typing import Tuple, List, Dict, Any\n",
        "# Matplotlib is optional (only required for plotting/animation)\n",
        "try:\n",
        "    import matplotlib.pyplot as plt\n",
        "    from matplotlib import animation\n",
        "    _HAS_MATPLOTLIB = True\n",
        "except Exception:\n",
        "    plt = None\n",
        "    animation = None\n",
        "    _HAS_MATPLOTLIB = False\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from sklearn.metrics import mutual_info_score\n",
        "from scipy.signal import coherence\n",
        "\n",
        "logging.info(f'Torch version: {torch.__version__} | NumPy version: {np.__version__}')\n",
        "\n",
        "# Optional imports with graceful fallback (use importlib to avoid static \"from X import Y\" issues)\n",
        "import importlib as _importlib\n",
        "\n",
        "odeint = None\n",
        "pl = None\n",
        "h5py = None\n",
        "spearmanr = None\n",
        "roc_auc_score = None\n",
        "average_precision_score = None\n",
        "HTML = None\n",
        "\n",
        "try:\n",
        "    _td = _importlib.import_module('torchdiffeq')\n",
        "    odeint = getattr(_td, 'odeint', None)\n",
        "    _HAS_TORCHDIFFEQ = odeint is not None\n",
        "except Exception:\n",
        "    odeint = None\n",
        "    _HAS_TORCHDIFFEQ = False\n",
        "\n",
        "try:\n",
        "    pl = _importlib.import_module('pytorch_lightning')\n",
        "    _HAS_PL = True\n",
        "except Exception:\n",
        "    pl = None\n",
        "    _HAS_PL = False\n",
        "\n",
        "try:\n",
        "    h5py = _importlib.import_module('h5py')\n",
        "    _HAS_H5PY = True\n",
        "except Exception:\n",
        "    h5py = None\n",
        "    _HAS_H5PY = False\n",
        "\n",
        "try:\n",
        "    _scipystats = _importlib.import_module('scipy.stats')\n",
        "    spearmanr = getattr(_scipystats, 'spearmanr', None)\n",
        "    _HAS_SCIPY = spearmanr is not None\n",
        "except Exception:\n",
        "    spearmanr = None\n",
        "    _HAS_SCIPY = False\n",
        "\n",
        "try:\n",
        "    _skm = _importlib.import_module('sklearn.metrics')\n",
        "    roc_auc_score = getattr(_skm, 'roc_auc_score', None)\n",
        "    average_precision_score = getattr(_skm, 'average_precision_score', None)\n",
        "    _HAS_SKLEARN = (roc_auc_score is not None) and (average_precision_score is not None)\n",
        "except Exception:\n",
        "    roc_auc_score = None\n",
        "    average_precision_score = None\n",
        "    _HAS_SKLEARN = False\n",
        "\n",
        "try:\n",
        "    _ipyd = _importlib.import_module('IPython.display')\n",
        "    HTML = getattr(_ipyd, 'HTML', None)\n",
        "    _HAS_IPYTHON = HTML is not None\n",
        "except Exception:\n",
        "    HTML = None\n",
        "    _HAS_IPYTHON = False\n",
        "\n",
        "warnings.filterwarnings('ignore', category=DeprecationWarning)\n",
        "\n",
        "\n",
        "def set_seed(seed: int = 42) -> int:\n",
        "    \"\"\"Sets random seeds for reproducibility across numpy and torch.\"\"\"\n",
        "    random.seed(seed)\n",
        "    np.random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    if torch.cuda.is_available():\n",
        "        torch.cuda.manual_seed_all(seed)\n",
        "    return seed\n",
        "\n",
        "set_seed(42)\n",
        "\n",
        "\n",
        "def count_parameters(model: torch.nn.Module) -> int:\n",
        "    \"\"\"Return number of trainable parameters.\"\"\"\n",
        "    return int(sum(p.numel() for p in model.parameters() if p.requires_grad))\n",
        "\n",
        "\n",
        "def _postprocess_adj(tensor: torch.Tensor, mask_diagonal: bool = True, eps: float = 1e-6) -> torch.Tensor:\n",
        "    \"\"\"Symmetrize, apply softplus, normalize per-batch and optionally mask diagonal.\n",
        "\n",
        "    Args:\n",
        "        tensor: (B, N, N) adjacency-like tensor\n",
        "        mask_diagonal: whether to zero the diagonal\n",
        "        eps: small value to avoid divide-by-zero\n",
        "\n",
        "    Returns:\n",
        "        processed tensor same shape as input\n",
        "    \"\"\"\n",
        "    # ensure non-negativity\n",
        "    out = F.softplus(tensor)\n",
        "    # symmetrize\n",
        "    out = 0.5 * (out + out.transpose(1, 2))\n",
        "    # normalize per-batch\n",
        "    maxv = out.view(out.size(0), -1).max(dim=-1)[0].view(-1, 1, 1) + eps\n",
        "    out = out / maxv\n",
        "    if mask_diagonal:\n",
        "        diag_idx = torch.arange(out.size(1), device=out.device)\n",
        "        out[:, diag_idx, diag_idx] = 0.0\n",
        "    return out\n",
        "\n",
        "\n",
        "# ---------------------- ADNI dataset loader and helpers ----------------------\n",
        "def load_timeseries_file(path: str) -> np.ndarray:\n",
        "    \"\"\"Load per-session ROI timeseries from a variety of formats.\n",
        "\n",
        "    Supported formats: .npy, .npz (key 'timeseries' or 'data'), .mat (key 'timeseries' or 'data'), .csv\n",
        "    Returns array shaped (T, N)\n",
        "    \"\"\"\n",
        "    p = Path(path)\n",
        "    if not p.exists():\n",
        "        raise FileNotFoundError(f'timeseries file not found: {path}')\n",
        "\n",
        "    if p.suffix == '.npy':\n",
        "        return np.load(str(p))\n",
        "    if p.suffix == '.npz':\n",
        "        arr = np.load(str(p))\n",
        "        for k in ('timeseries', 'data', 'ts'):\n",
        "            if k in arr:\n",
        "                return arr[k]\n",
        "        # fallback to first array in archive\n",
        "        return arr[list(arr.keys())[0]]\n",
        "    if p.suffix == '.mat':\n",
        "        mat = scipy.io.loadmat(str(p))\n",
        "        for k in ('timeseries', 'data', 'ts'):\n",
        "            if k in mat:\n",
        "                return np.asarray(mat[k])\n",
        "        # try common keys\n",
        "        for v in mat.values():\n",
        "            if isinstance(v, np.ndarray) and v.ndim == 2:\n",
        "                return v\n",
        "        raise RuntimeError('No 2D array found in .mat file')\n",
        "    if p.suffix == '.csv' or p.suffix == '.txt':\n",
        "        return np.loadtxt(str(p), delimiter=',')\n",
        "\n",
        "    raise ValueError(f'Unsupported timeseries file format: {p.suffix}')\n",
        "\n",
        "\n",
        "def diagnosis_to_label(dx: str) -> int:\n",
        "    dx = str(dx).strip().upper()\n",
        "    if dx.startswith('CN') or 'CONTROL' in dx:\n",
        "        return 0\n",
        "    if 'MCI' in dx:\n",
        "        return 1\n",
        "    if 'AD' in dx or 'ALZ' in dx:\n",
        "        return 2\n",
        "    # fallback: unknown\n",
        "    return -1\n",
        "\n",
        "\n",
        "class ADNIDataset(Dataset):\n",
        "    \"\"\"PyTorch Dataset for loading ADNI ROI timeseries and producing model-ready sequences.\n",
        "\n",
        "    The `manifest_csv` should contain at minimum columns: `subject_id`, `session_id`, `diagnosis`, `timeseries_path`.\n",
        "    Each `timeseries_path` may be absolute or relative to `adni_root`.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        manifest_csv: str,\n",
        "        adni_root: Optional[str] = None,\n",
        "        max_timepoints: Optional[int] = None,\n",
        "        harmonize: bool = True,\n",
        "        harmonize_batch_col: str = 'site'\n",
        "        , use_multimodal: bool = False, multimodal_weights: Optional[Dict[str, float]] = None\n",
        "    ):\n",
        "        self.df = pd.read_csv(manifest_csv)\n",
        "        self.adni_root = Path(adni_root) if adni_root is not None else None\n",
        "        self.max_timepoints = max_timepoints\n",
        "        self.harmonize = harmonize\n",
        "        self.harmonize_batch_col = harmonize_batch_col\n",
        "        self.use_multimodal = use_multimodal\n",
        "        self.multimodal_weights = multimodal_weights\n",
        "\n",
        "        # validate required columns\n",
        "        required = {'subject_id', 'session_id', 'diagnosis', 'timeseries_path'}\n",
        "        if not required.issubset(set(self.df.columns)):\n",
        "            raise ValueError(f'manifest must contain columns: {required}')\n",
        "\n",
        "        # optional multimodal columns we will try to read if present\n",
        "        self.optional_columns = {\n",
        "            'mmse': 'mmse',\n",
        "            'adas_cog': 'adas_cog',\n",
        "            'csf_amyloid': 'csf_amyloid',\n",
        "            'csf_tau': 'csf_tau',\n",
        "            'apoe4': 'apoe4',\n",
        "            'site': 'site',\n",
        "            'sMRI_path': 'sMRI_path',\n",
        "            'pet_path': 'pet_path'\n",
        "        }\n",
        "\n",
        "        # expand timeseries paths\n",
        "        def resolve_path(p):\n",
        "            pp = Path(p)\n",
        "            if pp.exists():\n",
        "                return str(pp)\n",
        "            if self.adni_root is not None and (self.adni_root / p).exists():\n",
        "                return str(self.adni_root / p)\n",
        "            return str(pp)\n",
        "\n",
        "        self.df['timeseries_path'] = self.df['timeseries_path'].apply(resolve_path)\n",
        "\n",
        "        # harmonization setup: if harmonize and batch column present, we'll apply ComBat later\n",
        "        if self.harmonize and (self.harmonize_batch_col not in self.df.columns):\n",
        "            # disable harmonization if batch column missing\n",
        "            self.harmonize = False\n",
        "\n",
        "        # precompute cached containers (optionally extend to HDF5 caching)\n",
        "        self._cached_node_features = [None] * len(self.df)\n",
        "        self._cached_adjacencies = [None] * len(self.df)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.df)\n",
        "\n",
        "    def __getitem__(self, idx: int):\n",
        "        row = self.df.iloc[idx]\n",
        "        ts_path = row['timeseries_path']\n",
        "        ts = load_timeseries_file(ts_path)\n",
        "        # require (T, N)\n",
        "        if ts.ndim == 1:\n",
        "            ts = ts.reshape(-1, 1)\n",
        "\n",
        "        if self.max_timepoints is not None and ts.shape[0] > self.max_timepoints:\n",
        "            ts = ts[: self.max_timepoints]\n",
        "\n",
        "        # compute node features and empirical adjacency per timepoint\n",
        "        node_feats = node_features_from_timeseries(ts)\n",
        "        # per-timepoint adjacency via Pearson on each window/timepoint's short window\n",
        "        T = ts.shape[0]\n",
        "        adjs = []\n",
        "        for t in range(T):\n",
        "            # small window around t (include t-2..t if available)\n",
        "            s = max(0, t - 2)\n",
        "            e = min(T, t + 1)\n",
        "            window = ts[s:e]\n",
        "            if self.use_multimodal:\n",
        "                adj = compute_multimodal_adjacency(window, weights=self.multimodal_weights)\n",
        "                # also compute base components for downstream learnable combination\n",
        "                R = compute_empirical_fc_from_timeseries(window)\n",
        "                I = compute_mutual_info_matrix(window)\n",
        "                C = compute_coherence_matrix(window)\n",
        "            else:\n",
        "                adj = compute_empirical_fc_from_timeseries(window)\n",
        "                R, I, C = None, None, None\n",
        "            adjs.append(adj.astype(np.float32))\n",
        "        adjs = np.stack(adjs, axis=0)\n",
        "\n",
        "        label = diagnosis_to_label(row['diagnosis'])\n",
        "\n",
        "        # read optional multimodal fields\n",
        "        multimodal = {}\n",
        "        for key, col in self.optional_columns.items():\n",
        "            if col in self.df.columns:\n",
        "                multimodal[key] = row[col]\n",
        "            else:\n",
        "                multimodal[key] = None\n",
        "\n",
        "        sample = {\n",
        "            'subject_id': row['subject_id'],\n",
        "            'session_id': row['session_id'],\n",
        "            'timeseries': ts.astype(np.float32),\n",
        "            'node_features': node_feats,  # (T, N, F)\n",
        "            'adjacencies': adjs,  # (T, N, N)\n",
        "            'adj_R': R if self.use_multimodal else None,\n",
        "            'adj_MI': I if self.use_multimodal else None,\n",
        "            'adj_coh': C if self.use_multimodal else None,\n",
        "            'label': int(label),\n",
        "            'multimodal': multimodal\n",
        "        }\n",
        "        return sample\n",
        "\n",
        "\n",
        "def combat_harmonize(features: np.ndarray, batch: np.ndarray, covars: Optional[object] = None) -> np.ndarray:\n",
        "    \"\"\"Harmonize features matrix (samples x features) using ComBat if available.\n",
        "\n",
        "    Falls back to returning features unchanged if `neurocombat_sklearn` is not installed.\n",
        "    \"\"\"\n",
        "    try:\n",
        "        import importlib\n",
        "        module = importlib.import_module('neurocombat_sklearn')\n",
        "        Combat = getattr(module, 'Combat', None)\n",
        "        if Combat is None:\n",
        "            # module present but does not expose Combat\n",
        "            return features\n",
        "    except Exception:\n",
        "        # neurocombat not installed or failed to import; return input\n",
        "        return features\n",
        "\n",
        "    cb = Combat()\n",
        "    # batch must be 1D array-like\n",
        "    harmonized = cb.fit_transform(features, batch, covars)\n",
        "    return harmonized\n",
        "\n",
        "\n",
        "# Small CLI to preview ADNI manifest and a few samples\n",
        "def _cli_preview(manifest: str, adni_root: Optional[str] = None, n: int = 3):\n",
        "    ds = ADNIDataset(manifest, adni_root=adni_root)\n",
        "    print(f'Loaded manifest with {len(ds)} sessions. Previewing {n} samples...')\n",
        "    for i in range(min(len(ds), n)):\n",
        "        s = ds[i]\n",
        "        print(f\"Subject: {s['subject_id']} | Session: {s['session_id']} | Label: {s['label']} | T={s['timeseries'].shape[0]} | N={s['timeseries'].shape[1]}\")\n",
        "\n",
        "\n",
        "class ADNISubjectDataset(Dataset):\n",
        "    \"\"\"Groups ADNI sessions by `subject_id` and returns per-subject longitudinal sequences.\n",
        "\n",
        "    This dataset wraps `ADNIDataset` and yields one entry per subject. Each item is a\n",
        "    dictionary with keys:\n",
        "      - 'subject_id'\n",
        "      - 'session_ids' : list of session identifiers in chronological (or session_id) order\n",
        "      - 'sessions' : list of per-session sample dicts returned by `ADNIDataset.__getitem__`\n",
        "\n",
        "    Helpful utilities are provided for collating a batch of subjects into padded tensors\n",
        "    for use in model training (`collate_subject_batch`).\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        manifest_csv: str,\n",
        "        adni_root: Optional[str] = None,\n",
        "        max_timepoints: Optional[int] = None,\n",
        "        min_sessions: int = 1,\n",
        "        harmonize: bool = True,\n",
        "        sort_by_date: bool = True,\n",
        "        use_multimodal: bool = False,\n",
        "        multimodal_weights: Optional[Dict[str, float]] = None,\n",
        "    ):\n",
        "        self.base = ADNIDataset(manifest_csv, adni_root=adni_root, max_timepoints=max_timepoints,\n",
        "                                harmonize=harmonize, use_multimodal=use_multimodal, multimodal_weights=multimodal_weights)\n",
        "        df = self.base.df.copy()\n",
        "        # parse session_date if present for reliable chronological ordering\n",
        "        if 'session_date' in df.columns:\n",
        "            try:\n",
        "                df['session_date'] = pd.to_datetime(df['session_date'])\n",
        "            except Exception:\n",
        "                # leave as-is if parsing fails\n",
        "                pass\n",
        "\n",
        "        grouped = df.groupby('subject_id')\n",
        "        self.subject_ids: List[str] = []\n",
        "        self.subject_indices: List[List[int]] = []\n",
        "\n",
        "        for sid, g in grouped:\n",
        "            if sort_by_date and 'session_date' in g.columns:\n",
        "                order = g.sort_values('session_date').index.tolist()\n",
        "            else:\n",
        "                # fall back to session_id sorting (string-sortable)\n",
        "                order = g.sort_values('session_id').index.tolist()\n",
        "            if len(order) >= min_sessions:\n",
        "                self.subject_ids.append(sid)\n",
        "                self.subject_indices.append(order)\n",
        "\n",
        "        self.min_sessions = min_sessions\n",
        "\n",
        "    def __len__(self) -> int:\n",
        "        return len(self.subject_ids)\n",
        "\n",
        "    def __getitem__(self, idx: int) -> Dict[str, Any]:\n",
        "        indices = self.subject_indices[idx]\n",
        "        sessions = [self.base[int(i)] for i in indices]\n",
        "        return {\n",
        "            'subject_id': self.subject_ids[idx],\n",
        "            'session_ids': [s['session_id'] for s in sessions],\n",
        "            'sessions': sessions\n",
        "        }\n",
        "\n",
        "    def collate_subject_batch(self, subjects: List[Dict[str, Any]], pad_T: Optional[int] = None) -> Dict[str, Any]:\n",
        "        \"\"\"Collate a list of subject items (as returned by __getitem__) into padded tensors.\n",
        "\n",
        "        Returns a dict with:\n",
        "          - 'node_features': (B, S, T, N, F) float tensor\n",
        "          - 'adjacencies' : (B, S, T, N, N) float tensor\n",
        "          - 'mask' : (B, S, T) boolean tensor indicating valid timesteps\n",
        "          - 'labels' : (B, S) integer labels per session\n",
        "          - 'subject_ids' : list of subject ids in order\n",
        "        \"\"\"\n",
        "        B = len(subjects)\n",
        "        S = max(len(s['sessions']) for s in subjects)\n",
        "        # infer N,F,T\n",
        "        first = subjects[0]['sessions'][0]\n",
        "        N = first['node_features'].shape[1]\n",
        "        F = first['node_features'].shape[2]\n",
        "\n",
        "        max_T = 0\n",
        "        for subj in subjects:\n",
        "            for sess in subj['sessions']:\n",
        "                max_T = max(max_T, sess['node_features'].shape[0])\n",
        "        if pad_T is not None:\n",
        "            max_T = max(max_T, pad_T)\n",
        "\n",
        "        node_feats = np.zeros((B, S, max_T, N, F), dtype=np.float32)\n",
        "        adjs = np.zeros((B, S, max_T, N, N), dtype=np.float32)\n",
        "        mask = np.zeros((B, S, max_T), dtype=bool)\n",
        "        labels = np.zeros((B, S), dtype=np.int64)\n",
        "\n",
        "        for i, subj in enumerate(subjects):\n",
        "            for j, sess in enumerate(subj['sessions']):\n",
        "                t = sess['node_features'].shape[0]\n",
        "                node_feats[i, j, :t] = sess['node_features']\n",
        "                adjs[i, j, :t] = sess['adjacencies']\n",
        "                labels[i, j] = sess['label']\n",
        "                mask[i, j, :t] = True\n",
        "\n",
        "        return {\n",
        "            'node_features': torch.tensor(node_feats),\n",
        "            'adjacencies': torch.tensor(adjs),\n",
        "            'mask': torch.tensor(mask),\n",
        "            'labels': torch.tensor(labels),\n",
        "            'subject_ids': [s['subject_id'] for s in subjects]\n",
        "        }\n",
        "\n",
        "\n",
        "# EdgeGenerator: learned edge-generation function (placed after torch imports)\n",
        "class EdgeGenerator(nn.Module):\n",
        "    \"\"\"Learned edge generation function g_theta(x_i, x_j).\n",
        "\n",
        "    Produces a symmetric adjacency matrix from node embeddings and optional latent state.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, node_dim: int, latent_dim: Optional[int] = None, hidden: int = 128):\n",
        "        super().__init__()\n",
        "        self.node_dim = node_dim\n",
        "        self.latent_dim = latent_dim\n",
        "        in_dim = 2 * node_dim + (latent_dim or 0)\n",
        "        self.mlp = nn.Sequential(\n",
        "            nn.Linear(in_dim, hidden),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(hidden, hidden // 2),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(hidden // 2, 1)\n",
        "        )\n",
        "\n",
        "    def forward(self, node_embeddings: torch.Tensor, latent_state: Optional[torch.Tensor] = None) -> torch.Tensor:\n",
        "        # node_embeddings: (B, N, D), latent_state: (B, L)\n",
        "        B, N, D = node_embeddings.shape\n",
        "        h_i = node_embeddings.unsqueeze(2).expand(B, N, N, D)\n",
        "        h_j = node_embeddings.unsqueeze(1).expand(B, N, N, D)\n",
        "        if latent_state is not None:\n",
        "            z = latent_state.view(B, 1, 1, -1).expand(B, N, N, latent_state.size(-1))\n",
        "            inputs = torch.cat([h_i, h_j, z], dim=-1)\n",
        "        else:\n",
        "            inputs = torch.cat([h_i, h_j], dim=-1)\n",
        "\n",
        "        out = self.mlp(inputs.view(B * N * N, -1)).view(B, N, N)\n",
        "        # postprocess: softplus, symmetrize, normalize (do not mask diagonal here)\n",
        "        out = _postprocess_adj(out, mask_diagonal=False)\n",
        "        return out\n",
        "\n",
        "\n",
        "class ImportanceHead(nn.Module):\n",
        "    \"\"\"Produce node-level and edge-level importance probabilities.\n",
        "\n",
        "    - node_head outputs per-node probability in (0,1)\n",
        "    - edge_head outputs per-edge probability in (0,1)\n",
        "    \"\"\"\n",
        "    def __init__(self, node_dim: int, hidden: int = 64):\n",
        "        super().__init__()\n",
        "        self.node_mlp = nn.Sequential(\n",
        "            nn.Linear(node_dim, hidden),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(hidden, 1)\n",
        "        )\n",
        "        # edge head uses concatenated node embeddings\n",
        "        self.edge_mlp = nn.Sequential(\n",
        "            nn.Linear(2 * node_dim, hidden),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(hidden, 1)\n",
        "        )\n",
        "\n",
        "    def forward(self, node_embeddings: torch.Tensor) -> Tuple[torch.Tensor, torch.Tensor]:\n",
        "        # node_embeddings: (B, N, D)\n",
        "        B, N, D = node_embeddings.shape\n",
        "        node_logits = self.node_mlp(node_embeddings.view(B * N, D)).view(B, N)\n",
        "        node_prob = torch.sigmoid(node_logits)\n",
        "\n",
        "        h_i = node_embeddings.unsqueeze(2).expand(B, N, N, D)\n",
        "        h_j = node_embeddings.unsqueeze(1).expand(B, N, N, D)\n",
        "        edge_inputs = torch.cat([h_i, h_j], dim=-1)\n",
        "        edge_logits = self.edge_mlp(edge_inputs.view(B * N * N, 2 * D)).view(B, N, N)\n",
        "        edge_prob = torch.sigmoid(edge_logits)\n",
        "        return node_prob, edge_prob\n",
        "\n",
        "# ---------------------- Simulation: Synthetic Longitudinal Connectomes ----------------------\n",
        "\n",
        "def simulate_subject_sequence(\n",
        "    num_regions: int = 16,\n",
        "    num_timepoints: int = 6,\n",
        "    seed: Optional[int] = None,\n",
        "    degenerate_regions: Optional[List[int]] = None,\n",
        "    noise_level: float = 0.05,\n",
        "    disease_heterogeneity: bool = False,\n",
        "    cascade_hops: int = 2\n",
        ") -> Tuple[np.ndarray, np.ndarray, np.ndarray, Dict[str, Any]]:\n",
        "    \"\"\"Simulates a subject's sequence of adjacency matrices and node features with realistic disease progression.\n",
        "\n",
        "    Args:\n",
        "        num_regions: Number of brain regions (nodes).\n",
        "        num_timepoints: Number of timepoints in the sequence.\n",
        "        seed: Random seed for reproducibility.\n",
        "        degenerate_regions: List of indices for regions that start degenerating.\n",
        "        noise_level: Magnitude of measurement noise added to adjacency matrices.\n",
        "        disease_heterogeneity: If True, simulates non-linear/heterogeneous disease progression.\n",
        "        cascade_hops: Number of hops for disease propagation through the network.\n",
        "\n",
        "    Returns:\n",
        "        node_features_sequence: (T, N, F) Node features over time (F=3).\n",
        "        adjacency_matrices_sequence: (T, N, N) Adjacency matrices over time.\n",
        "        time_indices: (T,) Array of time indices.\n",
        "        metadata: Dictionary containing simulation parameters and disease state info.\n",
        "    \"\"\"\n",
        "    if seed is not None:\n",
        "        rng = np.random.RandomState(seed)\n",
        "    else:\n",
        "        rng = np.random\n",
        "\n",
        "    num_features = 3\n",
        "    # Base adjacency with realistic community structure (e.g., functional networks)\n",
        "    base_adjacency = rng.randn(num_regions, num_regions) * 0.3\n",
        "\n",
        "    # Create community structure\n",
        "    num_communities = max(2, num_regions // 6)\n",
        "    # Ensure labels array length equals num_regions by tiling label indices and slicing.\n",
        "    # Using ceil division guarantees we have enough repeats even when num_regions is not\n",
        "    # divisible by num_communities (previous code could produce a too-short labels array).\n",
        "    reps = int(math.ceil(num_regions / num_communities))\n",
        "    labels = np.tile(np.arange(num_communities), reps)[:num_regions]\n",
        "\n",
        "    for i in range(num_regions):\n",
        "        for j in range(num_regions):\n",
        "            if labels[i] == labels[j]:\n",
        "                base_adjacency[i, j] += 0.8\n",
        "\n",
        "    # Symmetrize and normalize\n",
        "    base_adjacency = 0.5 * (base_adjacency + base_adjacency.T)\n",
        "    base_adjacency = np.abs(base_adjacency)\n",
        "    base_adjacency = base_adjacency / (base_adjacency.max() + 1e-6)\n",
        "\n",
        "    if degenerate_regions is None:\n",
        "        # Randomly select regions to degenerate if not provided\n",
        "        num_degenerate = max(1, num_regions // 8)\n",
        "        degenerate_regions = rng.choice(num_regions, num_degenerate, replace=False).tolist()\n",
        "\n",
        "    assert degenerate_regions is not None\n",
        "    degenerate_regions = list(degenerate_regions)\n",
        "    # ensure affected_list is defined even if the time loop does not execute\n",
        "    affected_list = list(degenerate_regions)\n",
        "\n",
        "    # Disease progression trajectory\n",
        "    if disease_heterogeneity:\n",
        "        # Sigmoid-like curve: slow initial, rapid middle, plateau late\n",
        "        progression_curve = 1.0 / (1.0 + np.exp(-3.0 * (np.arange(num_timepoints) / num_timepoints - 0.5)))\n",
        "    else:\n",
        "        # Linear progression\n",
        "        progression_curve = np.linspace(0, 1, num_timepoints)\n",
        "\n",
        "    adjacency_matrices_sequence = []\n",
        "    node_features_sequence = []\n",
        "\n",
        "    for t in range(num_timepoints):\n",
        "        # Cascade effect: disease spreads via connectivity graph\n",
        "        affected_regions = set(degenerate_regions)\n",
        "        for _ in range(cascade_hops):\n",
        "            newly_affected = set()\n",
        "            for region in affected_regions:\n",
        "                # Find connected neighbors with significant weight\n",
        "                neighbors = np.where(base_adjacency[region, :] > 0.3)[0]\n",
        "                newly_affected.update(neighbors.tolist())\n",
        "            affected_regions.update(newly_affected)\n",
        "\n",
        "        affected_list = list(affected_regions)\n",
        "\n",
        "        # Calculate decay factor based on progression\n",
        "        current_progression = progression_curve[t]\n",
        "        if disease_heterogeneity:\n",
        "            decay_factor = 1.0 - 0.2 * (current_progression**1.5)\n",
        "        else:\n",
        "            decay_factor = 1.0 - 0.15 * current_progression\n",
        "\n",
        "        # Apply decay to adjacency matrix\n",
        "        current_adjacency = base_adjacency.copy()\n",
        "        for r in affected_list:\n",
        "            # Primary degenerate regions decay faster than secondary affected ones\n",
        "            region_factor = 0.7 if r in degenerate_regions else 0.85\n",
        "            region_decay = decay_factor ** region_factor\n",
        "            current_adjacency[r, :] *= region_decay\n",
        "            current_adjacency[:, r] *= region_decay\n",
        "\n",
        "        # Add measurement noise\n",
        "        current_adjacency += noise_level * rng.randn(num_regions, num_regions)\n",
        "        current_adjacency = 0.5 * (current_adjacency + current_adjacency.T)\n",
        "        current_adjacency = np.clip(current_adjacency, 0.0, None)\n",
        "\n",
        "        # Normalize\n",
        "        max_val = current_adjacency.max()\n",
        "        if max_val > 1e-6:\n",
        "            current_adjacency = current_adjacency / max_val\n",
        "        adjacency_matrices_sequence.append(current_adjacency)\n",
        "\n",
        "        # Generate node features: Volumetric, Biomarker, Functional\n",
        "        base_features = rng.randn(num_regions, num_features) * 0.2 + 0.5\n",
        "        for r in affected_list:\n",
        "            strength = 0.08 if r in degenerate_regions else 0.03\n",
        "            # Feature 0: Regional volume decline (MRI-like)\n",
        "            base_features[r, 0] -= strength * current_progression\n",
        "            # Feature 1: Pathological biomarker increase (tau/amyloid-like)\n",
        "            base_features[r, 1] += 0.06 * current_progression\n",
        "            # Feature 2: Functional connectivity decline\n",
        "            base_features[r, 2] -= 0.07 * current_progression\n",
        "\n",
        "        base_features = np.clip(base_features, -1.0, 2.0)\n",
        "        node_features_sequence.append(base_features)\n",
        "\n",
        "    adjacency_matrices_sequence = np.stack(adjacency_matrices_sequence, axis=0).astype(np.float32)\n",
        "    node_features_sequence = np.stack(node_features_sequence, axis=0).astype(np.float32)\n",
        "    time_indices = np.arange(num_timepoints, dtype=np.float32)\n",
        "\n",
        "    metadata = {\n",
        "        'degenerate_regions': degenerate_regions,\n",
        "        'affected_regions': affected_list,\n",
        "        'labels': labels.tolist(),\n",
        "        'disease_progression': progression_curve.tolist(),\n",
        "        'cascade_hops': cascade_hops\n",
        "    }\n",
        "    return node_features_sequence, adjacency_matrices_sequence, time_indices, metadata\n",
        "\n",
        "# Quick sanity checks removed. Use the CLI/validation at the bottom of the file for demos and validation.\n",
        "\n",
        "# ---------------------- Dataset and DataLoader ----------------------\n",
        "\n",
        "class SimulatedDDGDataset(Dataset):\n",
        "    \"\"\"Dataset class for generating and serving synthetic disease progression data.\"\"\"\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        num_subjects: int = 200,\n",
        "        num_regions: int = 16,\n",
        "        num_timepoints: int = 6,\n",
        "        seed: int = 0,\n",
        "        noise_level: float = 0.03\n",
        "    ):\n",
        "        self.num_subjects = num_subjects\n",
        "        self.num_regions = num_regions\n",
        "        self.num_timepoints = num_timepoints\n",
        "        self.seed = seed\n",
        "        self.data = []\n",
        "\n",
        "        rng = np.random.RandomState(seed)\n",
        "        for s in range(num_subjects):\n",
        "            sub_seed = rng.randint(0, 2**31 - 1)\n",
        "            # Vary number of degenerate regions per subject\n",
        "            num_degenerate = max(1, num_regions // 12)\n",
        "            degenerate_indices = rng.choice(num_regions, num_degenerate, replace=False).tolist()\n",
        "\n",
        "            node_features, adjacency_matrices, times, meta = simulate_subject_sequence(\n",
        "                num_regions=num_regions,\n",
        "                num_timepoints=num_timepoints,\n",
        "                seed=sub_seed,\n",
        "                degenerate_regions=degenerate_indices,\n",
        "                noise_level=noise_level\n",
        "            )\n",
        "\n",
        "            # Clinical target: e.g., final-stage cognitive score inversely related to mean strength of degenerate regions\n",
        "            final_adj = adjacency_matrices[-1]\n",
        "            deg_mean_strength = final_adj[degenerate_indices, :].mean()\n",
        "\n",
        "            # Simulate MMSE (Mini-Mental State Exam) score: 0-30 scale\n",
        "            # Lower connectivity in degenerate regions -> Lower MMSE\n",
        "            cognitive_score = 30.0 - 12.0 * (1.0 - deg_mean_strength) + rng.randn() * 0.8\n",
        "            cognitive_score = float(np.clip(cognitive_score, 0.0, 30.0))\n",
        "\n",
        "            self.data.append({\n",
        "                'node_features': node_features,\n",
        "                'adjacency_matrices': adjacency_matrices,\n",
        "                'times': times,\n",
        "                'metadata': meta,\n",
        "                'cognitive_score': cognitive_score\n",
        "            })\n",
        "\n",
        "    def __len__(self) -> int:\n",
        "        return len(self.data)\n",
        "\n",
        "    def __getitem__(self, idx: int) -> Dict[str, Any]:\n",
        "        return self.data[idx]\n",
        "\n",
        "\n",
        "def collate_batch(batch: List[Dict[str, Any]]) -> Dict[str, torch.Tensor]:\n",
        "    \"\"\"Collates a list of dataset items into a batch.\"\"\"\n",
        "    batch_size = len(batch)\n",
        "    num_timepoints = batch[0]['node_features'].shape[0]\n",
        "    num_regions = batch[0]['node_features'].shape[1]\n",
        "    num_features = batch[0]['node_features'].shape[2]\n",
        "\n",
        "    # Validate shapes\n",
        "    for i, item in enumerate(batch):\n",
        "        if item['node_features'].shape != (num_timepoints, num_regions, num_features):\n",
        "            raise RuntimeError(\n",
        "                f'Inconsistent shapes in batch item {i}: '\n",
        "                f'expected (T={num_timepoints}, N={num_regions}, F={num_features}), '\n",
        "                f'got {item[\"node_features\"].shape}'\n",
        "            )\n",
        "\n",
        "    # Stack items\n",
        "    node_features = np.stack([b['node_features'] for b in batch], axis=0)  # (B, T, N, F)\n",
        "    adjacency_matrices = np.stack([b['adjacency_matrices'] for b in batch], axis=0)  # (B, T, N, N)\n",
        "    cognitive_scores = np.array([b['cognitive_score'] for b in batch], dtype=np.float32)\n",
        "\n",
        "    return {\n",
        "        'node_features': torch.tensor(node_features),\n",
        "        'adjacency_matrices': torch.tensor(adjacency_matrices),\n",
        "        'cognitive_scores': torch.tensor(cognitive_scores)\n",
        "    }\n",
        "\n",
        "# (Removed duplicated quick dataset check) See CLI/validation at the bottom of the file for demos and checks.\n",
        "\n",
        "# ---------------------- Model components ----------------------\n",
        "\n",
        "class GraphEncoder(nn.Module):\n",
        "    \"\"\"Encodes node features into embeddings using a simple GCN-like layer.\"\"\"\n",
        "\n",
        "    def __init__(self, in_features: int, hidden_dim: int):\n",
        "        super().__init__()\n",
        "        # Replace simple MLP with a multi-head GAT encoder to perform proper message passing\n",
        "        self.gat = None\n",
        "        # we'll initialize lazily to keep signature simple\n",
        "        self._in_features = in_features\n",
        "        self._hidden_dim = hidden_dim\n",
        "\n",
        "    def forward(self, node_features: torch.Tensor, adjacency_matrix: Optional[torch.Tensor] = None) -> torch.Tensor:\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            node_features: (B, N, F) Input node features.\n",
        "            adjacency_matrix: (B, N, N) Adjacency matrix (optional).\n",
        "\n",
        "        Returns:\n",
        "            node_embeddings: (B, N, D) Encoded node embeddings.\n",
        "        \"\"\"\n",
        "        # Lazy init GAT (to know node feature dim)\n",
        "        if self.gat is None:\n",
        "            # use 4 heads by default, head dim = hidden_dim // heads\n",
        "            num_heads = 4\n",
        "            head_dim = max(8, self._hidden_dim // num_heads)\n",
        "            self.gat = GATEncoder(self._in_features, self._hidden_dim, num_heads=num_heads, head_dim=head_dim)\n",
        "\n",
        "        # node_features: (B, N, F)\n",
        "        if adjacency_matrix is None:\n",
        "            # If no adjacency provided, derive a lightweight empirical adjacency from features\n",
        "            # so the encoder always performs graph message passing (avoids silent MLP fallback).\n",
        "            # Compute unnormalized similarity (batchwise inner product)\n",
        "            x = node_features\n",
        "            sim = torch.matmul(x, x.transpose(1, 2))  # (B, N, N)\n",
        "            # remove self-similarity\n",
        "            diag_idx = torch.arange(sim.size(1), device=sim.device)\n",
        "            sim[:, diag_idx, diag_idx] = 0.0\n",
        "            adjacency_matrix = _postprocess_adj(sim, mask_diagonal=True)\n",
        "\n",
        "        return self.gat(node_features, adjacency_matrix)\n",
        "\n",
        "\n",
        "class GATEncoder(nn.Module):\n",
        "    \"\"\"Multi-head GAT encoder that performs attention-weighted message passing using adjacency as mask.\n",
        "\n",
        "    Inputs:\n",
        "        in_feats: input feature dim\n",
        "        out_feats: output feature dim (total, across heads)\n",
        "        num_heads: number of attention heads\n",
        "        head_dim: inner head dimension\n",
        "\n",
        "    Returns node embeddings of shape (B, N, out_feats)\n",
        "    \"\"\"\n",
        "    def __init__(self, in_feats: int, out_feats: int, num_heads: int = 4, head_dim: int = 16, dropout: float = 0.2):\n",
        "        super().__init__()\n",
        "        self.in_feats = in_feats\n",
        "        self.out_feats = out_feats\n",
        "        self.num_heads = num_heads\n",
        "        self.head_dim = head_dim\n",
        "        self.dropout = dropout\n",
        "\n",
        "        # linear projection to (num_heads * head_dim)\n",
        "        self.fc = nn.Linear(in_feats, num_heads * head_dim, bias=False)\n",
        "\n",
        "        # attention vectors: a_l and a_r per head (to compute e_ij = a_l^T Wh_i + a_r^T Wh_j)\n",
        "        self.a_l = nn.Parameter(torch.Tensor(1, num_heads, head_dim))\n",
        "        self.a_r = nn.Parameter(torch.Tensor(1, num_heads, head_dim))\n",
        "\n",
        "        self.leaky = nn.LeakyReLU(0.2)\n",
        "        self.out_proj = nn.Linear(num_heads * head_dim, out_feats)\n",
        "        self.reset_parameters()\n",
        "\n",
        "    def reset_parameters(self):\n",
        "        nn.init.xavier_uniform_(self.fc.weight)\n",
        "        nn.init.xavier_uniform_(self.out_proj.weight)\n",
        "        nn.init.xavier_uniform_(self.a_l)\n",
        "        nn.init.xavier_uniform_(self.a_r)\n",
        "\n",
        "    def forward(self, x: torch.Tensor, adj: torch.Tensor) -> torch.Tensor:\n",
        "        # x: (B, N, F), adj: (B, N, N)\n",
        "        B, N, _ = x.shape\n",
        "        device = x.device\n",
        "\n",
        "        Wh = self.fc(x)  # (B, N, H*num_heads)\n",
        "        Wh = Wh.view(B, N, self.num_heads, self.head_dim)  # (B, N, heads, head_dim)\n",
        "\n",
        "        # compute attention coefficients efficiently: e_ij = a_l*Wh_i + a_r*Wh_j\n",
        "        a_l = (Wh * self.a_l).sum(dim=-1)  # (B, N, heads)\n",
        "        a_r = (Wh * self.a_r).sum(dim=-1)  # (B, N, heads)\n",
        "\n",
        "        # e_ij = a_l_i + a_r_j\n",
        "        e = a_l.unsqueeze(2) + a_r.unsqueeze(1)  # (B, N, N, heads)\n",
        "        e = self.leaky(e)\n",
        "\n",
        "        # transpose to (B, heads, N, N)\n",
        "        e = e.permute(0, 3, 1, 2)\n",
        "\n",
        "        # mask with adjacency: where adj <= 0, set -inf\n",
        "        if adj.dtype != torch.bool:\n",
        "            mask = (adj > 0)\n",
        "        else:\n",
        "            mask = adj\n",
        "        mask = mask.unsqueeze(1)  # (B,1,N,N)\n",
        "        neg_inf = -9e15\n",
        "        e = torch.where(mask, e, torch.full_like(e, neg_inf))\n",
        "\n",
        "        # softmax across neighbors\n",
        "        alpha = torch.softmax(e, dim=-1)  # (B, heads, N, N)\n",
        "        alpha = F.dropout(alpha, p=self.dropout, training=self.training)\n",
        "\n",
        "        # aggregate values: values = Wh (B,N,heads,head_dim) -> (B, heads, N, head_dim)\n",
        "        V = Wh.permute(0, 2, 1, 3)  # (B, heads, N, head_dim)\n",
        "        out = torch.matmul(alpha, V)  # (B, heads, N, head_dim)\n",
        "        out = out.permute(0, 2, 1, 3).contiguous().view(B, N, self.num_heads * self.head_dim)\n",
        "\n",
        "        out = self.out_proj(out)\n",
        "        out = F.elu(out)\n",
        "        # small stochastic regularization on outputs\n",
        "        out = F.dropout(out, p=self.dropout, training=self.training)\n",
        "        return out\n",
        "\n",
        "class ZEncoderGRU(nn.Module):\n",
        "    \"\"\"Encodes the sequence of node embeddings into a latent disease state vector.\"\"\"\n",
        "\n",
        "    def __init__(self, node_dim: int, latent_dim: int, rnn_hidden: int = 64):\n",
        "        super().__init__()\n",
        "        # Pooling function: Average over nodes\n",
        "        self.pool = lambda embeddings: embeddings.mean(dim=2)  # (B, T, N, D) -> (B, T, D)\n",
        "        self.gru = nn.GRU(node_dim, rnn_hidden, batch_first=True)\n",
        "        self.fc = nn.Sequential(nn.Linear(rnn_hidden, latent_dim), nn.Tanh())\n",
        "\n",
        "    def forward(self, node_embeddings_sequence: torch.Tensor) -> torch.Tensor:\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            node_embeddings_sequence: (B, T, N, D) Sequence of node embeddings.\n",
        "\n",
        "        Returns:\n",
        "            latent_state: (B, latent_dim) Final latent disease state.\n",
        "        \"\"\"\n",
        "        # Pool over nodes to get graph-level sequence\n",
        "        graph_sequence = self.pool(node_embeddings_sequence)  # (B, T, D)\n",
        "        _, hidden_state = self.gru(graph_sequence)  # hidden_state: (1, B, H)\n",
        "        latent_state = self.fc(hidden_state.squeeze(0))\n",
        "        return latent_state\n",
        "\n",
        "\n",
        "class VAEZEncoder(nn.Module):\n",
        "    \"\"\"VAE-style graph-level encoder: pools node embeddings over nodes, runs GRU,\n",
        "    and outputs mu/logvar and sampled z (reparameterized).\n",
        "    \"\"\"\n",
        "    def __init__(self, node_dim: int, latent_dim: int, rnn_hidden: int = 64):\n",
        "        super().__init__()\n",
        "        self.pool = lambda embeddings: embeddings.mean(dim=2)\n",
        "        self.gru = nn.GRU(node_dim, rnn_hidden, batch_first=True)\n",
        "        self.fc_mu = nn.Linear(rnn_hidden, latent_dim)\n",
        "        self.fc_logvar = nn.Linear(rnn_hidden, latent_dim)\n",
        "\n",
        "    def forward(self, node_embeddings_sequence: torch.Tensor) -> Tuple[torch.Tensor, torch.Tensor, torch.Tensor]:\n",
        "        # node_embeddings_sequence: (B, T, N, D)\n",
        "        graph_sequence = self.pool(node_embeddings_sequence)  # (B, T, D)\n",
        "        _, hidden_state = self.gru(graph_sequence)\n",
        "        h = hidden_state.squeeze(0)\n",
        "        mu = self.fc_mu(h)\n",
        "        logvar = self.fc_logvar(h)\n",
        "        std = torch.exp(0.5 * logvar)\n",
        "        eps = torch.randn_like(std)\n",
        "        z = mu + eps * std\n",
        "        return z, mu, logvar\n",
        "\n",
        "\n",
        "class VAEReconstructor(nn.Module):\n",
        "    \"\"\"RNN-based decoder that maps graph-level latent `z` to reconstructed node-feature sequences.\n",
        "\n",
        "    Design:\n",
        "    - Map `z` -> initial hidden state for a temporal GRU decoder.\n",
        "    - Use a learned node embedding template; at each decode step, combine the node template with GRU output\n",
        "      to produce per-node features (B, T, N, F).\n",
        "    - This produces a compact, trainable temporal decoder without duplicating per-node RNNs.\n",
        "    \"\"\"\n",
        "    def __init__(self, latent_dim: int, node_dim: int, out_feats: int = 4, hidden: int = 128, T_out: int = 6):\n",
        "        super().__init__()\n",
        "        self.T_out = T_out\n",
        "        self.out_feats = out_feats\n",
        "        self.node_dim = node_dim\n",
        "\n",
        "        # Project latent z to GRU initial hidden\n",
        "        self.z_to_hidden = nn.Linear(latent_dim, hidden)\n",
        "        # Temporal GRU that produces a context vector per timestep\n",
        "        self.gru = nn.GRU(input_size=node_dim, hidden_size=hidden, batch_first=True)\n",
        "\n",
        "        # Learned node template embeddings (shared across batch/time)\n",
        "        self.node_template = nn.Parameter(torch.randn(1, 1, node_dim))\n",
        "\n",
        "        # Map GRU output + node template -> per-node features\n",
        "        self.readout = nn.Sequential(\n",
        "            nn.Linear(hidden + node_dim, hidden),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(hidden, out_feats)\n",
        "        )\n",
        "\n",
        "    def forward(self, z: torch.Tensor, num_nodes: int) -> torch.Tensor:\n",
        "        \"\"\"Return reconstructed node features of shape (B, T_out, N, F).\n",
        "\n",
        "        Args:\n",
        "            z: (B, L)\n",
        "            num_nodes: number of nodes N\n",
        "        \"\"\"\n",
        "        B = z.size(0)\n",
        "        device = z.device\n",
        "        # initial hidden for GRU: (1, B, hidden)\n",
        "        h0 = self.z_to_hidden(z).unsqueeze(0)\n",
        "\n",
        "        # prepare decoder inputs: repeat node_template as sequence input\n",
        "        # input to GRU is per-timestep input of dimension node_dim; we use the template repeated\n",
        "        inp = self.node_template.expand(B, self.T_out, -1).to(device)  # (B, T, node_dim)\n",
        "\n",
        "        gru_out, _ = self.gru(inp, h0)  # (B, T, hidden)\n",
        "\n",
        "        # expand node template to per-node and combine with gru_out per time\n",
        "        node_template_exp = self.node_template.expand(B, self.T_out, -1)  # (B, T, node_dim)\n",
        "        # concatenate per-timestep context with node template, then map to per-node features\n",
        "        # we'll broadcast over nodes: produce (B, T, 1, F) then expand to (B, T, N, F)\n",
        "        concat = torch.cat([gru_out, node_template_exp], dim=-1)  # (B, T, hidden+node_dim)\n",
        "        step_feat = self.readout(concat)  # (B, T, F)\n",
        "        step_feat = step_feat.unsqueeze(2).expand(B, self.T_out, num_nodes, self.out_feats)\n",
        "        return step_feat\n",
        "\n",
        "\n",
        "class _ODEFuncDecoder(nn.Module):\n",
        "    \"\"\"Simple MLP vector field for ODE reconstructor.\"\"\"\n",
        "    def __init__(self, hidden_dim: int, node_dim: int):\n",
        "        super().__init__()\n",
        "        self.net = nn.Sequential(\n",
        "            nn.Linear(hidden_dim, hidden_dim),\n",
        "            nn.Tanh(),\n",
        "            nn.Linear(hidden_dim, hidden_dim),\n",
        "            nn.Tanh()\n",
        "        )\n",
        "\n",
        "    def forward(self, t, h):\n",
        "        return self.net(h)\n",
        "\n",
        "\n",
        "class VAEODEReconstructor(nn.Module):\n",
        "    \"\"\"ODE-based decoder that integrates a latent initial state to produce temporal context\n",
        "    and decodes to node-feature sequences.\n",
        "\n",
        "    If `torchdiffeq.odeint` is not available at runtime, this class will fall back to the\n",
        "    RNN-based `VAEReconstructor` behavior for robustness in environments without the\n",
        "    optional dependency.\n",
        "    \"\"\"\n",
        "    def __init__(self, latent_dim: int, node_dim: int, out_feats: int = 4, hidden: int = 128, T_out: int = 6, ode_solver: str = 'rk4'):\n",
        "        super().__init__()\n",
        "        self.T_out = T_out\n",
        "        self.out_feats = out_feats\n",
        "        self.node_dim = node_dim\n",
        "        self.hidden = hidden\n",
        "        self.ode_solver = ode_solver\n",
        "\n",
        "        # projection z -> initial hidden for ODE\n",
        "        self.z_to_h0 = nn.Linear(latent_dim, hidden)\n",
        "        # ODE func\n",
        "        self.func = _ODEFuncDecoder(hidden, node_dim)\n",
        "\n",
        "        # learned node template\n",
        "        self.node_template = nn.Parameter(torch.randn(1, node_dim))\n",
        "\n",
        "        # readout from ODE hidden + node template to node features\n",
        "        self.readout = nn.Sequential(\n",
        "            nn.Linear(hidden + node_dim, hidden),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(hidden, out_feats)\n",
        "        )\n",
        "\n",
        "        # fallback RNN decoder for environments without torchdiffeq\n",
        "        self._fallback = VAEReconstructor(latent_dim, node_dim, out_feats=out_feats, hidden=hidden, T_out=T_out)\n",
        "\n",
        "    def forward(self, z: torch.Tensor, num_nodes: int) -> torch.Tensor:\n",
        "        # Try to use torchdiffeq. If not available, fall back to RNN reconstructor.\n",
        "        try:\n",
        "            from torchdiffeq import odeint\n",
        "        except Exception:\n",
        "            return self._fallback(z, num_nodes)\n",
        "\n",
        "        B = z.size(0)\n",
        "        device = z.device\n",
        "        h0 = self.z_to_h0(z)  # (B, hidden)\n",
        "\n",
        "        # integrate over normalized time [0,1] at T_out points\n",
        "        t = torch.linspace(0.0, 1.0, self.T_out, device=device)\n",
        "        # odeint expects (B, hidden) as initial, but returns (T, B, hidden) when batch_first False\n",
        "        h0_batch = h0\n",
        "        try:\n",
        "            H = odeint(self.func, h0_batch, t, method=self.ode_solver)  # (T, B, hidden)\n",
        "        except Exception:\n",
        "            # try with default solver\n",
        "            H = odeint(self.func, h0_batch, t)\n",
        "\n",
        "        # permute to (B, T, hidden)\n",
        "        H = H.permute(1, 0, 2)\n",
        "\n",
        "        # expand node template per timestep and decode\n",
        "        node_tmpl = self.node_template.unsqueeze(0).unsqueeze(0).expand(B, self.T_out, -1).to(device)  # (B,T,node_dim)\n",
        "        concat = torch.cat([H, node_tmpl], dim=-1)  # (B,T,hidden+node_dim)\n",
        "        step_feat = self.readout(concat)  # (B,T,out_feats)\n",
        "        step_feat = step_feat.unsqueeze(2).expand(B, self.T_out, num_nodes, self.out_feats)\n",
        "        return step_feat\n",
        "\n",
        "class PerEdgeMLP(nn.Module):\n",
        "    \"\"\"Predicts next-step adjacency matrix using an MLP on edge and node features.\"\"\"\n",
        "\n",
        "    def __init__(self, node_dim: int, latent_dim: int, hidden_dim: int = 128, mask_diagonal: bool = True):\n",
        "        super().__init__()\n",
        "        self.node_dim = node_dim\n",
        "        self.latent_dim = latent_dim\n",
        "        self.mask_diagonal = mask_diagonal\n",
        "        # reduce default hidden_dim and add stronger dropout for regularization\n",
        "        self.mlp = nn.Sequential(\n",
        "            nn.Linear(1 + 2 * node_dim + latent_dim, max(64, hidden_dim // 2)),\n",
        "            nn.BatchNorm1d(max(64, hidden_dim // 2)),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(0.25),\n",
        "            nn.Linear(max(64, hidden_dim // 2), max(64, hidden_dim // 2)),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(0.2),\n",
        "            nn.Linear(max(64, hidden_dim // 2), 1)\n",
        "        )\n",
        "\n",
        "    def forward(self, adjacency_matrix: torch.Tensor, node_embeddings: torch.Tensor, latent_state: torch.Tensor) -> torch.Tensor:\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            adjacency_matrix: (B, N, N) Current adjacency.\n",
        "            node_embeddings: (B, N, D) Current node embeddings.\n",
        "            latent_state: (B, latent_dim) Disease state.\n",
        "\n",
        "        Returns:\n",
        "            next_adjacency: (B, N, N) Predicted next adjacency.\n",
        "        \"\"\"\n",
        "        batch_size, num_nodes, _ = adjacency_matrix.shape\n",
        "        embedding_dim = node_embeddings.size(-1)\n",
        "\n",
        "        # Expand latent state to edges\n",
        "        z_expanded = latent_state.view(batch_size, 1, 1, -1).expand(batch_size, num_nodes, num_nodes, self.latent_dim)\n",
        "\n",
        "        # Expand node embeddings to edges (source and target)\n",
        "        h_i = node_embeddings.unsqueeze(2).expand(batch_size, num_nodes, num_nodes, embedding_dim)\n",
        "        h_j = node_embeddings.unsqueeze(1).expand(batch_size, num_nodes, num_nodes, embedding_dim)\n",
        "\n",
        "        edge_input = adjacency_matrix.unsqueeze(-1)\n",
        "\n",
        "        # Concatenate all features\n",
        "        inputs = torch.cat([edge_input, h_i, h_j, z_expanded], dim=-1)\n",
        "\n",
        "        # Pass through MLP (flatten first)\n",
        "        outputs = self.mlp(inputs.view(batch_size * num_nodes * num_nodes, -1))\n",
        "        outputs = outputs.view(batch_size, num_nodes, num_nodes, 1)\n",
        "\n",
        "        next_adjacency = outputs.squeeze(-1)\n",
        "        # use centralized postprocessing\n",
        "        next_adjacency = _postprocess_adj(next_adjacency, mask_diagonal=self.mask_diagonal)\n",
        "        return next_adjacency\n",
        "\n",
        "class EdgeGRUCell(nn.Module):\n",
        "    \"\"\"GRU Cell operating on edge features.\"\"\"\n",
        "\n",
        "    def __init__(self, input_dim: int, hidden_dim: int):\n",
        "        super().__init__()\n",
        "        self.cell = nn.GRUCell(input_dim, hidden_dim)\n",
        "        self.readout = nn.Linear(hidden_dim, 1)\n",
        "\n",
        "    def forward(self, edge_features: torch.Tensor, hidden_state: torch.Tensor) -> Tuple[torch.Tensor, torch.Tensor]:\n",
        "        # edge_features: (B, E, input_dim), hidden_state: (B, E, hidden_dim)\n",
        "        batch_size, num_edges, _ = edge_features.shape\n",
        "\n",
        "        h_flat = hidden_state.view(batch_size * num_edges, -1)\n",
        "        inp_flat = edge_features.view(batch_size * num_edges, -1)\n",
        "\n",
        "        h_next = self.cell(inp_flat, h_flat)\n",
        "        out = self.readout(h_next)\n",
        "\n",
        "        return h_next.view(batch_size, num_edges, -1), out.view(batch_size, num_edges)\n",
        "\n",
        "\n",
        "class EdgeGRU(nn.Module):\n",
        "    \"\"\"Recurrent edge evolution model.\"\"\"\n",
        "\n",
        "    def __init__(self, node_dim: int, latent_dim: int, hidden_dim: int = 32, mask_diagonal: bool = True):\n",
        "        super().__init__()\n",
        "        self.node_dim = node_dim\n",
        "        self.latent_dim = latent_dim\n",
        "        self.hidden_dim = hidden_dim\n",
        "        self.mask_diagonal = mask_diagonal\n",
        "        self.cell = EdgeGRUCell(input_dim=1 + 2 * node_dim + latent_dim, hidden_dim=hidden_dim)\n",
        "\n",
        "    def forward(\n",
        "        self,\n",
        "        adjacency_matrix: torch.Tensor,\n",
        "        node_embeddings: torch.Tensor,\n",
        "        latent_state: torch.Tensor,\n",
        "        prev_hidden_state: Optional[torch.Tensor] = None\n",
        "    ) -> Tuple[torch.Tensor, torch.Tensor]:\n",
        "\n",
        "        batch_size, num_nodes, _ = adjacency_matrix.shape\n",
        "        embedding_dim = node_embeddings.size(-1)\n",
        "\n",
        "        h_i = node_embeddings.unsqueeze(2).expand(batch_size, num_nodes, num_nodes, embedding_dim)\n",
        "        h_j = node_embeddings.unsqueeze(1).expand(batch_size, num_nodes, num_nodes, embedding_dim)\n",
        "        z_expanded = latent_state.view(batch_size, 1, 1, -1).expand(batch_size, num_nodes, num_nodes, self.latent_dim)\n",
        "\n",
        "        edge_input = adjacency_matrix.unsqueeze(-1)\n",
        "        features = torch.cat([edge_input, h_i, h_j, z_expanded], dim=-1)\n",
        "\n",
        "        num_edges_total = num_nodes * num_nodes\n",
        "        features_flat = features.view(batch_size, num_edges_total, -1)\n",
        "\n",
        "        if prev_hidden_state is None:\n",
        "            prev_hidden_state = torch.zeros(\n",
        "                batch_size, num_edges_total, self.hidden_dim,\n",
        "                device=adjacency_matrix.device, dtype=adjacency_matrix.dtype\n",
        "            )\n",
        "\n",
        "        next_hidden_state, out_flat = self.cell(features_flat, prev_hidden_state)\n",
        "\n",
        "        out = out_flat.view(batch_size, num_nodes, num_nodes)\n",
        "        out = _postprocess_adj(out, mask_diagonal=self.mask_diagonal)\n",
        "        return out, next_hidden_state\n",
        "\n",
        "\n",
        "class CovariateParamNet(nn.Module):\n",
        "    \"\"\"Map subject covariates to positive dynamical scalars (kappa, gamma, sigma).\"\"\"\n",
        "    def __init__(self, cov_dim: int, hidden: int = 32, eps: float = 1e-6):\n",
        "        super().__init__()\n",
        "        self.net = nn.Sequential(\n",
        "            nn.Linear(cov_dim, hidden),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(0.15),\n",
        "            nn.Linear(hidden, 3)\n",
        "        )\n",
        "        self.softplus = nn.Softplus()\n",
        "        self.eps = eps\n",
        "\n",
        "    def forward(self, covariates: torch.Tensor) -> Tuple[torch.Tensor, torch.Tensor, torch.Tensor]:\n",
        "        # covariates: (B, C) -> return (kappa, gamma, sigma) each (B,1)\n",
        "        out = self.net(covariates)\n",
        "        out = self.softplus(out) + self.eps\n",
        "        kappa = out[:, 0:1]\n",
        "        gamma = out[:, 1:2]\n",
        "        sigma = out[:, 2:3]\n",
        "        return kappa, gamma, sigma\n",
        "\n",
        "\n",
        "class BiophysicalDiffusionEvolution(nn.Module):\n",
        "    \"\"\"Deterministic biophysical diffusion prior on adjacency matrices.\n",
        "\n",
        "    - Implements simple graph diffusion dynamics on adjacency matrices:\n",
        "        dE/dt = -kappa * (L E + E L) / 2 - gamma * E + R(E, H, z)\n",
        "      where L is graph Laplacian of E (symmetric), and R is a learnable residual (PerEdgeMLP) to capture non-biophysical effects.\n",
        "\n",
        "    - Integration done with simple forward-Euler steps for robustness if torchdiffeq missing.\n",
        "    - kappa/gamma are provided per-subject via covariates using CovariateParamNet.\n",
        "    \"\"\"\n",
        "    def __init__(self, node_dim: int, latent_dim: int, cov_dim: int = 3, residual_hidden: int = 128, n_steps: int = 8, dt: float = 1.0):\n",
        "        super().__init__()\n",
        "        self.node_dim = node_dim\n",
        "        self.latent_dim = latent_dim\n",
        "        self.n_steps = int(max(1, n_steps))\n",
        "        self.dt = float(dt)\n",
        "        self.param_net = CovariateParamNet(cov_dim, hidden=32)\n",
        "        # residual model operates per-edge and returns correction to adjacency\n",
        "        self.residual = PerEdgeMLP(node_dim=node_dim, latent_dim=latent_dim, hidden_dim=residual_hidden, mask_diagonal=True)\n",
        "\n",
        "    def _laplacian(self, A: torch.Tensor) -> torch.Tensor:\n",
        "        # A: (B,N,N) -> L = D - A, computed per-batch\n",
        "        deg = torch.sum(A, dim=-1)  # (B,N)\n",
        "        D = torch.zeros_like(A)\n",
        "        idx = torch.arange(A.size(-1), device=A.device)\n",
        "        D[:, idx, idx] = deg\n",
        "        L = D - A\n",
        "        return L\n",
        "\n",
        "    def forward(self, E_t: torch.Tensor, H_t: Optional[torch.Tensor], z_t: torch.Tensor, covariates: Optional[torch.Tensor] = None, t_span: Optional[torch.Tensor] = None) -> torch.Tensor:\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            E_t: (B,N,N) current adjacency\n",
        "            H_t: (B,N,D) node embeddings (optional)\n",
        "            z_t: (B, latent_dim) subject latent\n",
        "            covariates: (B, cov_dim) subject covariates (APOE, CSF, etc.)\n",
        "        Returns:\n",
        "            E_next: (B,N,N) evolved adjacency after integration window\n",
        "        \"\"\"\n",
        "        B, N, _ = E_t.shape\n",
        "        device = E_t.device\n",
        "\n",
        "        if covariates is None:\n",
        "            # default neutral covariates\n",
        "            covariates = torch.zeros(B, 3, device=device, dtype=E_t.dtype)\n",
        "\n",
        "        kappa, gamma, _ = self.param_net(covariates)  # (B,1)\n",
        "        # broadcast scalars\n",
        "        kappa = kappa.view(B, 1, 1)\n",
        "        gamma = gamma.view(B, 1, 1)\n",
        "\n",
        "        E = E_t.clone()\n",
        "        for _ in range(self.n_steps):\n",
        "            L = self._laplacian(E)  # (B,N,N)\n",
        "            # diffusion term: symmetrized Laplacian action\n",
        "            diffusion = -0.5 * (torch.matmul(L, E) + torch.matmul(E, L))\n",
        "            # decay term\n",
        "            decay = - gamma * E\n",
        "            # residual correction (learned)\n",
        "            if H_t is None:\n",
        "                # create placeholder node embeddings by averaging adjacency rows\n",
        "                H_fake = E.mean(dim=-1, keepdim=True).repeat(1, 1, self.node_dim)\n",
        "            else:\n",
        "                H_fake = H_t\n",
        "\n",
        "            residual = self.residual(E, H_fake, z_t)  # (B,N,N)\n",
        "            dE = kappa * diffusion + decay + residual\n",
        "            E = E + (self.dt / float(self.n_steps)) * dE\n",
        "            # keep symmetric and non-negative\n",
        "            E = 0.5 * (E + E.transpose(1, 2))\n",
        "            E = torch.clamp(E, min=0.0)\n",
        "            # renormalize per-batch to keep numerical stability\n",
        "            maxv = E.view(B, -1).max(dim=-1)[0].view(B, 1, 1).clamp(min=1e-12)\n",
        "            E = E / maxv\n",
        "\n",
        "        E = _postprocess_adj(E, mask_diagonal=False)\n",
        "        return E\n",
        "\n",
        "    def compute_residual(self, E: torch.Tensor, H: Optional[torch.Tensor], z: torch.Tensor) -> torch.Tensor:\n",
        "        \"\"\"Return the learned residual R(E,H,z) without integrating dynamics.\"\"\"\n",
        "        if H is None:\n",
        "            H_fake = E.mean(dim=-1, keepdim=True).repeat(1, 1, self.node_dim)\n",
        "        else:\n",
        "            H_fake = H\n",
        "        return self.residual(E, H_fake, z)\n",
        "\n",
        "    def get_params_from_covariates(self, covariates: torch.Tensor) -> Tuple[torch.Tensor, torch.Tensor, torch.Tensor]:\n",
        "        return self.param_net(covariates)\n",
        "\n",
        "\n",
        "class PersonalizedSDEEvolution(nn.Module):\n",
        "    \"\"\"Stochastic evolution: personalized drift (biophysical + learned residual) and diffusion noise scaled by covariates.\n",
        "\n",
        "    Integrates using Euler-Maruyama.\n",
        "    \"\"\"\n",
        "    def __init__(self, node_dim: int, latent_dim: int, cov_dim: int = 3, residual_hidden: int = 128, n_steps: int = 12, dt: float = 1.0):\n",
        "        super().__init__()\n",
        "        self.node_dim = node_dim\n",
        "        self.latent_dim = latent_dim\n",
        "        self.n_steps = int(max(1, n_steps))\n",
        "        self.dt = float(dt)\n",
        "        self.param_net = CovariateParamNet(cov_dim, hidden=32)\n",
        "        # residual drift\n",
        "        self.residual = PerEdgeMLP(node_dim=node_dim, latent_dim=latent_dim, hidden_dim=residual_hidden, mask_diagonal=True)\n",
        "\n",
        "    def _laplacian(self, A: torch.Tensor) -> torch.Tensor:\n",
        "        deg = torch.sum(A, dim=-1)\n",
        "        D = torch.zeros_like(A)\n",
        "        idx = torch.arange(A.size(-1), device=A.device)\n",
        "        D[:, idx, idx] = deg\n",
        "        return D - A\n",
        "\n",
        "    def forward(self, E_t: torch.Tensor, H_t: Optional[torch.Tensor], z_t: torch.Tensor, covariates: Optional[torch.Tensor] = None, t_span: Optional[torch.Tensor] = None) -> torch.Tensor:\n",
        "        B, N, _ = E_t.shape\n",
        "        device = E_t.device\n",
        "\n",
        "        if covariates is None:\n",
        "            covariates = torch.zeros(B, 3, device=device, dtype=E_t.dtype)\n",
        "        kappa, gamma, sigma = self.param_net(covariates)  # (B,1)\n",
        "        kappa = kappa.view(B, 1, 1)\n",
        "        gamma = gamma.view(B, 1, 1)\n",
        "        sigma = sigma.view(B, 1, 1)\n",
        "\n",
        "        E = E_t.clone()\n",
        "        sqrt_dt = math.sqrt(self.dt / max(1, self.n_steps))\n",
        "        for _ in range(self.n_steps):\n",
        "            L = self._laplacian(E)\n",
        "            diffusion = -0.5 * (torch.matmul(L, E) + torch.matmul(E, L))\n",
        "            decay = - gamma * E\n",
        "\n",
        "            if H_t is None:\n",
        "                H_fake = E.mean(dim=-1, keepdim=True).repeat(1, 1, self.node_dim)\n",
        "            else:\n",
        "                H_fake = H_t\n",
        "\n",
        "            drift_resid = self.residual(E, H_fake, z_t)\n",
        "            drift = kappa * diffusion + decay + drift_resid\n",
        "\n",
        "            noise = torch.randn_like(E) * sigma\n",
        "            E = E + (self.dt / float(self.n_steps)) * drift + sqrt_dt * noise\n",
        "            E = 0.5 * (E + E.transpose(1, 2))\n",
        "            E = torch.clamp(E, min=0.0)\n",
        "            maxv = E.view(B, -1).max(dim=-1)[0].view(B, 1, 1).clamp(min=1e-12)\n",
        "            E = E / maxv\n",
        "\n",
        "        E = _postprocess_adj(E, mask_diagonal=False)\n",
        "        return E\n",
        "\n",
        "    def compute_residual(self, E: torch.Tensor, H: Optional[torch.Tensor], z: torch.Tensor) -> torch.Tensor:\n",
        "        if H is None:\n",
        "            H_fake = E.mean(dim=-1, keepdim=True).repeat(1, 1, self.node_dim)\n",
        "        else:\n",
        "            H_fake = H\n",
        "        return self.residual(E, H_fake, z)\n",
        "\n",
        "    def get_params_from_covariates(self, covariates: torch.Tensor) -> Tuple[torch.Tensor, torch.Tensor, torch.Tensor]:\n",
        "        return self.param_net(covariates)\n",
        "\n",
        "\n",
        "class StructuralCausalModel:\n",
        "    \"\"\"Lightweight SCM on node-level pathology loads enabling do()-interventions and counterfactual rollouts.\n",
        "\n",
        "    - The SCM represents node values X (B,N) evolving by linear SEM:\n",
        "        X = phi * A_norm @ X + b + noise\n",
        "      This is intentionally simple yet sufficient to run do()-style interventions and compute counterfactuals for attribution.\n",
        "    \"\"\"\n",
        "    def __init__(self, num_nodes: int, phi: float = 0.6, bias: float = 0.01, noise_scale: float = 0.01):\n",
        "        self.num_nodes = num_nodes\n",
        "        self.phi = float(phi)\n",
        "        self.bias = float(bias)\n",
        "        self.noise_scale = float(noise_scale)\n",
        "\n",
        "    def step(self, X: np.ndarray, A: np.ndarray) -> np.ndarray:\n",
        "        # X: (N,) or (B,N), A: (N,N) or (B,N,N)\n",
        "        if X.ndim == 1:\n",
        "            X = X[None, :]\n",
        "            single = True\n",
        "        else:\n",
        "            single = False\n",
        "        B = X.shape[0]\n",
        "        if A.ndim == 2:\n",
        "            A = np.tile(A[None, :, :], (B, 1, 1))\n",
        "        # normalize adjacency rows\n",
        "        row_sums = A.sum(axis=-1, keepdims=True)\n",
        "        A_norm = np.divide(A, (row_sums + 1e-12))\n",
        "        X_next = self.phi * (A_norm @ X[..., None]).squeeze(-1) + self.bias + self.noise_scale * np.random.randn(*X.shape)\n",
        "        if single:\n",
        "            return X_next[0]\n",
        "        return X_next\n",
        "\n",
        "    def do_intervention(self, X: np.ndarray, do_pairs: List[Tuple[int, float]]) -> np.ndarray:\n",
        "        # apply hard setting of node values\n",
        "        X_cf = X.copy()\n",
        "        for (i, val) in do_pairs:\n",
        "            X_cf[..., i] = val\n",
        "        return X_cf\n",
        "\n",
        "    def counterfactual_rollout(self, X0: np.ndarray, A_seq: List[np.ndarray], do_pairs: Optional[List[Tuple[int, float]]] = None) -> np.ndarray:\n",
        "        # X0: (N,) initial state, A_seq: list of adjacency matrices over time\n",
        "        X = X0.copy()\n",
        "        if do_pairs is not None:\n",
        "            X = self.do_intervention(X, do_pairs)\n",
        "        traj = [X.copy()]\n",
        "        for A in A_seq:\n",
        "            X = self.step(X, A)\n",
        "            traj.append(X.copy())\n",
        "        return np.stack(traj, axis=0)  # (T+1, N)\n",
        "\n",
        "\n",
        "class GraphTransformer(nn.Module):\n",
        "    \"\"\"Graph Transformer with disease-state modulated attention.\"\"\"\n",
        "\n",
        "    def __init__(self, node_dim: int, latent_dim: Optional[int] = None, num_heads: int = 4):\n",
        "        super().__init__()\n",
        "        self.node_dim = node_dim\n",
        "        self.latent_dim = latent_dim\n",
        "        self.num_heads = num_heads\n",
        "\n",
        "        if node_dim % num_heads != 0:\n",
        "            raise ValueError(f\"node_dim {node_dim} must be divisible by num_heads {num_heads}\")\n",
        "\n",
        "        self.head_dim = node_dim // num_heads\n",
        "\n",
        "        self.query_proj = nn.Linear(node_dim, node_dim)\n",
        "        self.key_proj = nn.Linear(node_dim, node_dim)\n",
        "        self.value_proj = nn.Linear(node_dim, node_dim)\n",
        "        self.out_proj = nn.Linear(node_dim, node_dim)\n",
        "\n",
        "        self.norm = nn.LayerNorm(node_dim)\n",
        "        self.feed_forward = nn.Sequential(\n",
        "            nn.Linear(node_dim, node_dim * 2),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(node_dim * 2, node_dim)\n",
        "        )\n",
        "\n",
        "        # Disease state modulation of attention\n",
        "        if latent_dim is not None:\n",
        "            self.latent_to_attention_scale = nn.Sequential(\n",
        "                nn.Linear(latent_dim, 64),\n",
        "                nn.ReLU(),\n",
        "                nn.Linear(64, num_heads),\n",
        "                nn.Softplus()  # Ensure positive scaling\n",
        "            )\n",
        "\n",
        "    def forward(\n",
        "        self,\n",
        "        node_embeddings: torch.Tensor,\n",
        "        adjacency_matrix: Optional[torch.Tensor] = None,\n",
        "        latent_state: Optional[torch.Tensor] = None\n",
        "    ) -> Tuple[torch.Tensor, torch.Tensor]:\n",
        "\n",
        "        batch_size, num_nodes, _ = node_embeddings.shape\n",
        "\n",
        "        # Calculate Q, K, V\n",
        "        queries = self.query_proj(node_embeddings).view(batch_size, num_nodes, self.num_heads, self.head_dim).permute(0, 2, 1, 3)\n",
        "        keys = self.key_proj(node_embeddings).view(batch_size, num_nodes, self.num_heads, self.head_dim).permute(0, 2, 1, 3)\n",
        "        values = self.value_proj(node_embeddings).view(batch_size, num_nodes, self.num_heads, self.head_dim).permute(0, 2, 1, 3)\n",
        "\n",
        "        # Scaled Dot-Product Attention\n",
        "        scores = torch.matmul(queries, keys.transpose(-2, -1)) / math.sqrt(self.head_dim)  # (B, h, N, N)\n",
        "\n",
        "        # Modulate attention with disease state\n",
        "        if latent_state is not None and self.latent_dim is not None:\n",
        "            latent_scales = self.latent_to_attention_scale(latent_state)  # (B, num_heads)\n",
        "            scores = scores * latent_scales.view(batch_size, self.num_heads, 1, 1)\n",
        "\n",
        "        # If adjacency provided, mask out non-edges to prevent attention across absent edges\n",
        "        if adjacency_matrix is not None:\n",
        "            # adjacency_matrix: (B, N, N) -> mask where zero/negative entries should be masked\n",
        "            mask = (adjacency_matrix > 0).unsqueeze(1)  # (B,1,N,N)\n",
        "            neg_inf = -9e15\n",
        "            scores = torch.where(mask, scores, torch.full_like(scores, neg_inf))\n",
        "\n",
        "        attention_weights = torch.softmax(scores, dim=-1)\n",
        "\n",
        "        context = torch.matmul(attention_weights, values)  # (B, h, N, d_k)\n",
        "        context = context.permute(0, 2, 1, 3).contiguous().view(batch_size, num_nodes, self.node_dim)\n",
        "\n",
        "        output = self.out_proj(context)\n",
        "\n",
        "        # Residual + Norm + Feed Forward\n",
        "        output = self.norm(output + node_embeddings)\n",
        "        output = self.feed_forward(output) + output\n",
        "\n",
        "        # produce averaged per-edge importance across heads\n",
        "        edge_importance = attention_weights.mean(dim=1)  # (B, N, N)\n",
        "        return output, attention_weights, edge_importance\n",
        "\n",
        "class EdgeDecoder(nn.Module):\n",
        "    # Decodes edge weights from node embeddings and latent state.\n",
        "\n",
        "    def __init__(self, node_dim: int, latent_dim: int, mask_diagonal: bool = True):\n",
        "        super().__init__()\n",
        "        self.readout = nn.Sequential(\n",
        "            nn.Linear(2 * node_dim + latent_dim, 128),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(0.2),\n",
        "            nn.Linear(128, 1)\n",
        "        )\n",
        "        self.mask_diagonal = mask_diagonal\n",
        "\n",
        "    def forward(self, node_embeddings: torch.Tensor, latent_state: torch.Tensor) -> torch.Tensor:\n",
        "        batch_size, num_nodes, embedding_dim = node_embeddings.shape\n",
        "\n",
        "        h_i = node_embeddings.unsqueeze(2).expand(batch_size, num_nodes, num_nodes, embedding_dim)\n",
        "        h_j = node_embeddings.unsqueeze(1).expand(batch_size, num_nodes, num_nodes, embedding_dim)\n",
        "        z_expanded = latent_state.view(batch_size, 1, 1, -1).expand(batch_size, num_nodes, num_nodes, latent_state.size(-1))\n",
        "\n",
        "        inputs = torch.cat([h_i, h_j, z_expanded], dim=-1)\n",
        "        output = self.readout(inputs).squeeze(-1)\n",
        "        output = _postprocess_adj(output, mask_diagonal=self.mask_diagonal)\n",
        "        return output\n",
        "\n",
        "\n",
        "class LowRankEdgeDecoder(nn.Module):\n",
        "    \"\"\"Parameter-efficient low-rank edge decoder: predict adjacency as U V^T where U,V are node embeddings projected from node features and latent z.\n",
        "\n",
        "    This reduces parameters from O(N^2) to O(N r) for rank r.\n",
        "    \"\"\"\n",
        "    def __init__(self, node_dim: int, latent_dim: int, rank: int = 8, mask_diagonal: bool = True):\n",
        "        super().__init__()\n",
        "        self.rank = rank\n",
        "        self.mask_diagonal = mask_diagonal\n",
        "        self.left_proj = nn.Linear(node_dim + latent_dim, rank)\n",
        "        self.right_proj = nn.Linear(node_dim + latent_dim, rank)\n",
        "\n",
        "    def forward(self, node_embeddings: torch.Tensor, latent_state: torch.Tensor) -> torch.Tensor:\n",
        "        # node_embeddings: (B,N,D), latent_state: (B,L)\n",
        "        B, N, D = node_embeddings.shape\n",
        "        z_exp = latent_state.view(B, 1, -1).expand(B, N, latent_state.size(-1))\n",
        "        inp = torch.cat([node_embeddings, z_exp], dim=-1)  # (B,N,D+L)\n",
        "        U = self.left_proj(inp)  # (B,N,r)\n",
        "        V = self.right_proj(inp)  # (B,N,r)\n",
        "        # adjacency approx: U @ V^T per-batch\n",
        "        A = torch.matmul(U, V.transpose(1, 2))  # (B,N,N)\n",
        "        A = _postprocess_adj(A, mask_diagonal=self.mask_diagonal)\n",
        "        return A\n",
        "\n",
        "\n",
        "class ClinicalDecoder(nn.Module):\n",
        "    \"\"\"Predicts clinical scores (e.g., MMSE) from graph embeddings and latent state.\"\"\"\n",
        "\n",
        "    def __init__(self, node_dim: int, latent_dim: int, out_dim: int = 1):\n",
        "        super().__init__()\n",
        "        self.mlp = nn.Sequential(\n",
        "            nn.Linear(node_dim + latent_dim, 64),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(64, out_dim)\n",
        "        )\n",
        "\n",
        "    def forward(self, node_embeddings: torch.Tensor, latent_state: torch.Tensor) -> torch.Tensor:\n",
        "        # Pool node embeddings to get graph representation\n",
        "        graph_embedding = node_embeddings.mean(dim=1)\n",
        "# ---------------------- Model components ----------------------\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "class _ODEFuncDecoder(nn.Module):\n",
        "    \"\"\"Simple MLP vector field for ODE reconstructor.\"\"\"\n",
        "    def __init__(self, hidden_dim: int, node_dim: int):\n",
        "        super().__init__()\n",
        "        self.net = nn.Sequential(\n",
        "            nn.Linear(hidden_dim, hidden_dim),\n",
        "            nn.Tanh(),\n",
        "            nn.Linear(hidden_dim, hidden_dim),\n",
        "            nn.Tanh()\n",
        "        )\n",
        "\n",
        "    def forward(self, t, h):\n",
        "        return self.net(h)\n",
        "\n",
        "\n",
        "class VAEODEReconstructor(nn.Module):\n",
        "    \"\"\"ODE-based decoder that integrates a latent initial state to produce temporal context\n",
        "    and decodes to node-feature sequences.\n",
        "\n",
        "    If `torchdiffeq.odeint` is not available at runtime, this class will fall back to the\n",
        "    RNN-based `VAEReconstructor` behavior for robustness in environments without the\n",
        "    optional dependency.\n",
        "    \"\"\"\n",
        "    def __init__(self, latent_dim: int, node_dim: int, out_feats: int = 4, hidden: int = 128, T_out: int = 6, ode_solver: str = 'rk4'):\n",
        "        super().__init__()\n",
        "        self.T_out = T_out\n",
        "        self.out_feats = out_feats\n",
        "        self.node_dim = node_dim\n",
        "        self.hidden = hidden\n",
        "        self.ode_solver = ode_solver\n",
        "\n",
        "        # projection z -> initial hidden for ODE\n",
        "        self.z_to_h0 = nn.Linear(latent_dim, hidden)\n",
        "        # ODE func\n",
        "        self.func = _ODEFuncDecoder(hidden, node_dim)\n",
        "\n",
        "        # learned node template\n",
        "        self.node_template = nn.Parameter(torch.randn(1, node_dim))\n",
        "\n",
        "        # readout from ODE hidden + node template to node features\n",
        "        self.readout = nn.Sequential(\n",
        "            nn.Linear(hidden + node_dim, hidden),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(hidden, out_feats)\n",
        "        )\n",
        "\n",
        "        # fallback RNN decoder for environments without torchdiffeq\n",
        "        self._fallback = VAEReconstructor(latent_dim, node_dim, out_feats=out_feats, hidden=hidden, T_out=T_out)\n",
        "\n",
        "    def forward(self, z: torch.Tensor, num_nodes: int) -> torch.Tensor:\n",
        "        # Try to use torchdiffeq. If not available, fall back to RNN reconstructor.\n",
        "        try:\n",
        "            from torchdiffeq import odeint\n",
        "        except Exception:\n",
        "            return self._fallback(z, num_nodes)\n",
        "\n",
        "        B = z.size(0)\n",
        "        device = z.device\n",
        "        h0 = self.z_to_h0(z)  # (B, hidden)\n",
        "\n",
        "        # integrate over normalized time [0,1] at T_out points\n",
        "        t = torch.linspace(0.0, 1.0, self.T_out, device=device)\n",
        "        # odeint expects (B, hidden) as initial, but returns (T, B, hidden) when batch_first False\n",
        "        h0_batch = h0\n",
        "        try:\n",
        "            H = odeint(self.func, h0_batch, t, method=self.ode_solver)  # (T, B, hidden)\n",
        "        except Exception:\n",
        "            # try with default solver\n",
        "            H = odeint(self.func, h0_batch, t)\n",
        "\n",
        "        # permute to (B, T, hidden)\n",
        "        H = H.permute(1, 0, 2)\n",
        "\n",
        "        # expand node template per timestep and decode\n",
        "        node_tmpl = self.node_template.unsqueeze(0).unsqueeze(0).expand(B, self.T_out, -1).to(device)  # (B,T,node_dim)\n",
        "        concat = torch.cat([H, node_tmpl], dim=-1)  # (B,T,hidden+node_dim)\n",
        "        step_feat = self.readout(concat)  # (B,T,out_feats)\n",
        "        step_feat = step_feat.unsqueeze(2).expand(B, self.T_out, num_nodes, self.out_feats)\n",
        "        return step_feat\n",
        "\n",
        "class CovariateParamNet(nn.Module):\n",
        "    \"\"\"Map subject covariates to positive dynamical scalars (kappa, gamma, sigma).\"\"\"\n",
        "    def __init__(self, cov_dim: int, hidden: int = 32, eps: float = 1e-6):\n",
        "        super().__init__()\n",
        "        self.net = nn.Sequential(\n",
        "            nn.Linear(cov_dim, hidden),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(hidden, 3)\n",
        "        )\n",
        "        self.softplus = nn.Softplus()\n",
        "        self.eps = eps\n",
        "\n",
        "    def forward(self, covariates: torch.Tensor) -> Tuple[torch.Tensor, torch.Tensor, torch.Tensor]:\n",
        "        # covariates: (B, C) -> return (kappa, gamma, sigma) each (B,1)\n",
        "        out = self.net(covariates)\n",
        "        out = self.softplus(out) + self.eps\n",
        "        kappa = out[:, 0:1]\n",
        "        gamma = out[:, 1:2]\n",
        "        sigma = out[:, 2:3]\n",
        "        return kappa, gamma, sigma\n",
        "\n",
        "\n",
        "class BiophysicalDiffusionEvolution(nn.Module):\n",
        "    \"\"\"Deterministic biophysical diffusion prior on adjacency matrices.\n",
        "\n",
        "    - Implements simple graph diffusion dynamics on adjacency matrices:\n",
        "        dE/dt = -kappa * (L E + E L) / 2 - gamma * E + R(E, H, z)\n",
        "      where L is graph Laplacian of E (symmetric), and R is a learnable residual (PerEdgeMLP) to capture non-biophysical effects.\n",
        "\n",
        "    - Integration done with simple forward-Euler steps for robustness if torchdiffeq missing.\n",
        "    - kappa/gamma are provided per-subject via covariates using CovariateParamNet.\n",
        "    \"\"\"\n",
        "    def __init__(self, node_dim: int, latent_dim: int, cov_dim: int = 3, residual_hidden: int = 128, n_steps: int = 8, dt: float = 1.0):\n",
        "        super().__init__()\n",
        "        self.node_dim = node_dim\n",
        "        self.latent_dim = latent_dim\n",
        "        self.n_steps = int(max(1, n_steps))\n",
        "        self.dt = float(dt)\n",
        "        self.param_net = CovariateParamNet(cov_dim, hidden=32)\n",
        "        # residual model operates per-edge and returns correction to adjacency\n",
        "        self.residual = PerEdgeMLP(node_dim=node_dim, latent_dim=latent_dim, hidden_dim=residual_hidden, mask_diagonal=True)\n",
        "\n",
        "    def _laplacian(self, A: torch.Tensor) -> torch.Tensor:\n",
        "        # A: (B,N,N) -> L = D - A, computed per-batch\n",
        "        deg = torch.sum(A, dim=-1)  # (B,N)\n",
        "        D = torch.zeros_like(A)\n",
        "        idx = torch.arange(A.size(-1), device=A.device)\n",
        "        D[:, idx, idx] = deg\n",
        "        L = D - A\n",
        "        return L\n",
        "\n",
        "    def forward(self, E_t: torch.Tensor, H_t: Optional[torch.Tensor], z_t: torch.Tensor, covariates: Optional[torch.Tensor] = None, t_span: Optional[torch.Tensor] = None) -> torch.Tensor:\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            E_t: (B,N,N) current adjacency\n",
        "            H_t: (B,N,D) node embeddings (optional)\n",
        "            z_t: (B, latent_dim) subject latent\n",
        "            covariates: (B, cov_dim) subject covariates (APOE, CSF, etc.)\n",
        "        Returns:\n",
        "            E_next: (B,N,N) evolved adjacency after integration window\n",
        "        \"\"\"\n",
        "        B, N, _ = E_t.shape\n",
        "        device = E_t.device\n",
        "\n",
        "        if covariates is None:\n",
        "            # default neutral covariates\n",
        "            covariates = torch.zeros(B, 3, device=device, dtype=E_t.dtype)\n",
        "\n",
        "        kappa, gamma, _ = self.param_net(covariates)  # (B,1)\n",
        "        # broadcast scalars\n",
        "        kappa = kappa.view(B, 1, 1)\n",
        "        gamma = gamma.view(B, 1, 1)\n",
        "\n",
        "        E = E_t.clone()\n",
        "        for _ in range(self.n_steps):\n",
        "            L = self._laplacian(E)  # (B,N,N)\n",
        "            # diffusion term: symmetrized Laplacian action\n",
        "            diffusion = -0.5 * (torch.matmul(L, E) + torch.matmul(E, L))\n",
        "            # decay term\n",
        "            decay = - gamma * E\n",
        "            # residual correction (learned)\n",
        "            if H_t is None:\n",
        "                # create placeholder node embeddings by averaging adjacency rows\n",
        "                H_fake = E.mean(dim=-1, keepdim=True).repeat(1, 1, self.node_dim)\n",
        "            else:\n",
        "                H_fake = H_t\n",
        "\n",
        "            residual = self.residual(E, H_fake, z_t)  # (B,N,N)\n",
        "            dE = kappa * diffusion + decay + residual\n",
        "            E = E + (self.dt / float(self.n_steps)) * dE\n",
        "            # keep symmetric and non-negative\n",
        "            E = 0.5 * (E + E.transpose(1, 2))\n",
        "            E = torch.clamp(E, min=0.0)\n",
        "            # renormalize per-batch to keep numerical stability\n",
        "            maxv = E.view(B, -1).max(dim=-1)[0].view(B, 1, 1).clamp(min=1e-12)\n",
        "            E = E / maxv\n",
        "\n",
        "        E = _postprocess_adj(E, mask_diagonal=False)\n",
        "        return E\n",
        "\n",
        "\n",
        "class PersonalizedSDEEvolution(nn.Module):\n",
        "    \"\"\"Stochastic evolution: personalized drift (biophysical + learned residual) and diffusion noise scaled by covariates.\n",
        "\n",
        "    Integrates using Euler-Maruyama.\n",
        "    \"\"\"\n",
        "    def __init__(self, node_dim: int, latent_dim: int, cov_dim: int = 3, residual_hidden: int = 128, n_steps: int = 12, dt: float = 1.0):\n",
        "        super().__init__()\n",
        "        self.node_dim = node_dim\n",
        "        self.latent_dim = latent_dim\n",
        "        self.n_steps = int(max(1, n_steps))\n",
        "        self.dt = float(dt)\n",
        "        self.param_net = CovariateParamNet(cov_dim, hidden=32)\n",
        "        # residual drift\n",
        "        self.residual = PerEdgeMLP(node_dim=node_dim, latent_dim=latent_dim, hidden_dim=residual_hidden, mask_diagonal=True)\n",
        "\n",
        "    def _laplacian(self, A: torch.Tensor) -> torch.Tensor:\n",
        "        deg = torch.sum(A, dim=-1)\n",
        "        D = torch.zeros_like(A)\n",
        "        idx = torch.arange(A.size(-1), device=A.device)\n",
        "        D[:, idx, idx] = deg\n",
        "        return D - A\n",
        "\n",
        "    def forward(self, E_t: torch.Tensor, H_t: Optional[torch.Tensor], z_t: torch.Tensor, covariates: Optional[torch.Tensor] = None, t_span: Optional[torch.Tensor] = None) -> torch.Tensor:\n",
        "        B, N, _ = E_t.shape\n",
        "        device = E_t.device\n",
        "\n",
        "        if covariates is None:\n",
        "            covariates = torch.zeros(B, 3, device=device, dtype=E_t.dtype)\n",
        "        kappa, gamma, sigma = self.param_net(covariates)  # (B,1)\n",
        "        kappa = kappa.view(B, 1, 1)\n",
        "        gamma = gamma.view(B, 1, 1)\n",
        "        sigma = sigma.view(B, 1, 1)\n",
        "\n",
        "        E = E_t.clone()\n",
        "        sqrt_dt = math.sqrt(self.dt / max(1, self.n_steps))\n",
        "        for _ in range(self.n_steps):\n",
        "            L = self._laplacian(E)\n",
        "            diffusion = -0.5 * (torch.matmul(L, E) + torch.matmul(E, L))\n",
        "            decay = - gamma * E\n",
        "\n",
        "            if H_t is None:\n",
        "                H_fake = E.mean(dim=-1, keepdim=True).repeat(1, 1, self.node_dim)\n",
        "            else:\n",
        "                H_fake = H_t\n",
        "\n",
        "            drift_resid = self.residual(E, H_fake, z_t)\n",
        "            drift = kappa * diffusion + decay + drift_resid\n",
        "\n",
        "            noise = torch.randn_like(E) * sigma\n",
        "            E = E + (self.dt / float(self.n_steps)) * drift + sqrt_dt * noise\n",
        "            E = 0.5 * (E + E.transpose(1, 2))\n",
        "            E = torch.clamp(E, min=0.0)\n",
        "            maxv = E.view(B, -1).max(dim=-1)[0].view(B, 1, 1).clamp(min=1e-12)\n",
        "            E = E / maxv\n",
        "\n",
        "        E = _postprocess_adj(E, mask_diagonal=False)\n",
        "        return E\n",
        "\n",
        "\n",
        "class StructuralCausalModel:\n",
        "    \"\"\"Lightweight SCM on node-level pathology loads enabling do()-interventions and counterfactual rollouts.\n",
        "\n",
        "    - The SCM represents node values X (B,N) evolving by linear SEM:\n",
        "        X = phi * A_norm @ X + b + noise\n",
        "      This is intentionally simple yet sufficient to run do()-style interventions and compute counterfactuals for attribution.\n",
        "    \"\"\"\n",
        "    def __init__(self, num_nodes: int, phi: float = 0.6, bias: float = 0.01, noise_scale: float = 0.01):\n",
        "        self.num_nodes = num_nodes\n",
        "        self.phi = float(phi)\n",
        "        self.bias = float(bias)\n",
        "        self.noise_scale = float(noise_scale)\n",
        "\n",
        "    def step(self, X: np.ndarray, A: np.ndarray) -> np.ndarray:\n",
        "        # X: (N,) or (B,N), A: (N,N) or (B,N,N)\n",
        "        if X.ndim == 1:\n",
        "            X = X[None, :]\n",
        "            single = True\n",
        "        else:\n",
        "            single = False\n",
        "        B = X.shape[0]\n",
        "        if A.ndim == 2:\n",
        "            A = np.tile(A[None, :, :], (B, 1, 1))\n",
        "        # normalize adjacency rows\n",
        "        row_sums = A.sum(axis=-1, keepdims=True)\n",
        "        A_norm = np.divide(A, (row_sums + 1e-12))\n",
        "        X_next = self.phi * (A_norm @ X[..., None]).squeeze(-1) + self.bias + self.noise_scale * np.random.randn(*X.shape)\n",
        "        if single:\n",
        "            return X_next[0]\n",
        "        return X_next\n",
        "\n",
        "    def do_intervention(self, X: np.ndarray, do_pairs: List[Tuple[int, float]]) -> np.ndarray:\n",
        "        # apply hard setting of node values\n",
        "        X_cf = X.copy()\n",
        "        for (i, val) in do_pairs:\n",
        "            X_cf[..., i] = val\n",
        "        return X_cf\n",
        "\n",
        "    def counterfactual_rollout(self, X0: np.ndarray, A_seq: List[np.ndarray], do_pairs: Optional[List[Tuple[int, float]]] = None) -> np.ndarray:\n",
        "        # X0: (N,) initial state, A_seq: list of adjacency matrices over time\n",
        "        X = X0.copy()\n",
        "        if do_pairs is not None:\n",
        "            X = self.do_intervention(X, do_pairs)\n",
        "        traj = [X.copy()]\n",
        "        for A in A_seq:\n",
        "            X = self.step(X, A)\n",
        "            traj.append(X.copy())\n",
        "        return np.stack(traj, axis=0)  # (T+1, N)\n",
        "\n",
        "\n",
        "# ---------------------- DDGModel: assemble everything ----------------------\n",
        "\n",
        "class DDGModel(nn.Module):\n",
        "    \"\"\"\n",
        "    Dynamic Disease Graph Model (DDG).\n",
        "\n",
        "    Combines graph encoding, latent trajectory modeling, and edge evolution to forecast\n",
        "    disease progression in brain networks.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        in_feats: int = 3,\n",
        "        node_dim: int = 32,\n",
        "        latent_dim: int = 16,\n",
        "        use_edge_gru: bool = False,\n",
        "        use_gde: bool = False,\n",
        "        use_adaptive_edges: bool = False,\n",
        "        alpha_init: float = 0.5,\n",
        "        use_vae: bool = False,\n",
        "        kl_beta: float = 1.0,\n",
        "        use_vae_recon: bool = False,\n",
        "        recon_T: int = 6,\n",
        "        use_ode_recon: bool = False,\n",
        "        use_stage_conditioned: bool = False,\n",
        "        num_stages: int = 3\n",
        "    ):\n",
        "        super().__init__()\n",
        "        self.encoder = GraphEncoder(in_feats, node_dim)\n",
        "        self.use_vae = use_vae\n",
        "        self.kl_beta = float(kl_beta)\n",
        "        if self.use_vae:\n",
        "            self.z_encoder = VAEZEncoder(node_dim, latent_dim)\n",
        "        else:\n",
        "            self.z_encoder = ZEncoderGRU(node_dim, latent_dim)\n",
        "        self.use_vae_recon = use_vae_recon\n",
        "        self.use_ode_recon = use_ode_recon\n",
        "        if self.use_vae_recon:\n",
        "            if self.use_ode_recon:\n",
        "                # instantiate ODE-based reconstructor (falls back internally if odeint missing)\n",
        "                self.reconstructor = VAEODEReconstructor(latent_dim, node_dim, out_feats=in_feats, T_out=recon_T)\n",
        "            else:\n",
        "                self.reconstructor = VAEReconstructor(latent_dim, node_dim, out_feats=in_feats, T_out=recon_T)\n",
        "        self.transformer = GraphTransformer(node_dim, latent_dim=latent_dim, num_heads=4)\n",
        "\n",
        "        self.use_edge_gru = use_edge_gru\n",
        "        self.use_gde = use_gde and _HAS_TORCHDIFFEQ\n",
        "        self.use_adaptive_edges = use_adaptive_edges\n",
        "        self.use_stage_conditioned = use_stage_conditioned\n",
        "        self.num_stages = num_stages\n",
        "        # New options: biophysical prior and personalized SDE (disabled by default)\n",
        "        self.use_biophysical_prior = False\n",
        "        self.use_personalized_sde = False\n",
        "\n",
        "        # disease-aware edge generator and learnable alpha (in [0,1])\n",
        "        if self.use_adaptive_edges:\n",
        "            self.edge_generator = EdgeGenerator(node_dim=node_dim, latent_dim=latent_dim)\n",
        "            # parameterize unconstrained and use sigmoid in forward\n",
        "            self._alpha_param = nn.Parameter(torch.tensor(float(alpha_init)))\n",
        "            # multimodal combination logits for [pearson, mi, coh]\n",
        "            self._multimodal_logits = nn.Parameter(torch.tensor([0.6, 0.2, 0.2], dtype=torch.float32))\n",
        "            # importance head\n",
        "            self.importance_head = ImportanceHead(node_dim)\n",
        "\n",
        "        if use_edge_gru:\n",
        "            self.evolution = EdgeGRU(node_dim, latent_dim, hidden_dim=64)\n",
        "        else:\n",
        "            # priority: personalized SDE -> biophysical -> GDE -> PerEdgeMLP\n",
        "            if self.use_personalized_sde:\n",
        "                self.evolution = PersonalizedSDEEvolution(node_dim, latent_dim, cov_dim=3, residual_hidden=128)\n",
        "            elif self.use_biophysical_prior:\n",
        "                self.evolution = BiophysicalDiffusionEvolution(node_dim, latent_dim, cov_dim=3, residual_hidden=128)\n",
        "            elif self.use_gde:\n",
        "                # choose stage-conditioned GDE if requested\n",
        "                if self.use_stage_conditioned and GraphGDEEvolution is not None:\n",
        "                    self.evolution = GraphGDEStageEvolution(node_dim, latent_dim, num_stages=num_stages)\n",
        "                else:\n",
        "                    self.evolution = GraphGDEEvolution(node_dim, latent_dim)\n",
        "            else:\n",
        "                self.evolution = PerEdgeMLP(node_dim, latent_dim, hidden_dim=128)\n",
        "\n",
        "        self.edge_decoder = EdgeDecoder(node_dim, latent_dim)\n",
        "        self.clinical_decoder = ClinicalDecoder(node_dim, latent_dim)\n",
        "\n",
        "        self.node_dim = node_dim\n",
        "        self.latent_dim = latent_dim\n",
        "        # Expose an SCM instance for counterfactuals / causal attribution\n",
        "        self.scm = None\n",
        "\n",
        "    # Helpers to enable biophysical / personalized SDE at runtime (backwards-compatible)\n",
        "    def enable_biophysical_prior(self, covariate_dim: int = 3, residual_hidden: int = 128, n_steps: int = 8):\n",
        "        self.use_biophysical_prior = True\n",
        "        self.use_personalized_sde = False\n",
        "        self.evolution = BiophysicalDiffusionEvolution(self.node_dim, self.latent_dim, cov_dim=covariate_dim, residual_hidden=residual_hidden, n_steps=n_steps)\n",
        "\n",
        "    def enable_personalized_sde(self, covariate_dim: int = 3, residual_hidden: int = 128, n_steps: int = 12):\n",
        "        self.use_personalized_sde = True\n",
        "        self.use_biophysical_prior = False\n",
        "        self.evolution = PersonalizedSDEEvolution(self.node_dim, self.latent_dim, cov_dim=covariate_dim, residual_hidden=residual_hidden, n_steps=n_steps)\n",
        "\n",
        "    def attach_structural_causal_model(self, scm: StructuralCausalModel):\n",
        "        self.scm = scm\n",
        "\n",
        "    def run_counterfactual(self, X0: Union[np.ndarray, torch.Tensor], A_seq: List[Union[np.ndarray, torch.Tensor]], do_pairs: Optional[List[Tuple[int, float]]] = None) -> np.ndarray:\n",
        "        \"\"\"Run counterfactual rollout using the attached StructuralCausalModel.\n",
        "\n",
        "        Args:\n",
        "            X0: initial node values (N,) or (B,N)\n",
        "            A_seq: list of adjacency matrices over time (numpy or torch)\n",
        "            do_pairs: optional list of (node_index, value) interventions\n",
        "\n",
        "        Returns:\n",
        "            np.ndarray trajectory of shape (T+1, N) or (B, T+1, N)\n",
        "        \"\"\"\n",
        "        if self.scm is None:\n",
        "            raise RuntimeError('No StructuralCausalModel attached; call attach_structural_causal_model() first')\n",
        "\n",
        "        # convert X0 and A_seq to numpy\n",
        "        if isinstance(X0, torch.Tensor):\n",
        "            X0_np = X0.detach().cpu().numpy()\n",
        "        else:\n",
        "            X0_np = np.asarray(X0)\n",
        "\n",
        "        A_seq_np = []\n",
        "        for A in A_seq:\n",
        "            if isinstance(A, torch.Tensor):\n",
        "                A_seq_np.append(A.detach().cpu().numpy())\n",
        "            else:\n",
        "                A_seq_np.append(np.asarray(A))\n",
        "\n",
        "        traj = self.scm.counterfactual_rollout(X0_np, A_seq_np, do_pairs=do_pairs)\n",
        "        return traj\n",
        "\n",
        "    def synthesize_adjacency_sequence(self,\n",
        "                                      initial_adj: torch.Tensor,\n",
        "                                      node_embeddings_seq: Optional[List[torch.Tensor]] = None,\n",
        "                                      latent_seq: Optional[List[torch.Tensor]] = None,\n",
        "                                      covariates_seq: Optional[List[torch.Tensor]] = None,\n",
        "                                      steps: int = 4) -> List[torch.Tensor]:\n",
        "        \"\"\"Produce a sequence of adjacency matrices by repeatedly applying the evolution module.\n",
        "\n",
        "        Args:\n",
        "            initial_adj: (B,N,N) starting adjacency\n",
        "            node_embeddings_seq: optional list length >= steps of (B,N,D) node embeddings\n",
        "            latent_seq: optional list length >= steps of (B,latent_dim)\n",
        "            covariates_seq: optional list length >= steps of (B,cov_dim)\n",
        "            steps: number of steps to roll out\n",
        "\n",
        "        Returns:\n",
        "            list of adjacency tensors length `steps` (each (B,N,N))\n",
        "        \"\"\"\n",
        "        A = initial_adj\n",
        "        seq = []\n",
        "        for t in range(steps):\n",
        "            H_t = node_embeddings_seq[t] if (node_embeddings_seq is not None and t < len(node_embeddings_seq)) else None\n",
        "            z_t = latent_seq[t] if (latent_seq is not None and t < len(latent_seq)) else None\n",
        "            cov_t = covariates_seq[t] if (covariates_seq is not None and t < len(covariates_seq)) else None\n",
        "            with torch.no_grad():\n",
        "                # ensure latent present for evolutions that expect it\n",
        "                if z_t is None:\n",
        "                    z_t = torch.zeros(A.size(0), self.latent_dim, device=A.device, dtype=A.dtype)\n",
        "                try:\n",
        "                    A_next = self.evolution(A, H_t, z_t, cov_t)\n",
        "                except TypeError:\n",
        "                    A_next = self.evolution(A, H_t, z_t)\n",
        "            seq.append(A_next)\n",
        "            A = A_next\n",
        "        return seq\n",
        "\n",
        "    def compute_mechanistic_loss(self,\n",
        "                                 E_t: torch.Tensor,\n",
        "                                 H_t: Optional[torch.Tensor],\n",
        "                                 z_t: torch.Tensor,\n",
        "                                 covariates: Optional[torch.Tensor] = None,\n",
        "                                 target_adj: Optional[torch.Tensor] = None,\n",
        "                                 residual_weight: float = 1.0,\n",
        "                                 param_weight: float = 0.1) -> Dict[str, torch.Tensor]:\n",
        "        \"\"\"Compute regularization losses encouraging small learned residuals and interpretable params.\n",
        "\n",
        "        Returns dict with keys: 'residual_l2', 'param_l2', 'total'\n",
        "        \"\"\"\n",
        "        losses: Dict[str, torch.Tensor] = {}\n",
        "        # compute residual\n",
        "        if hasattr(self.evolution, 'compute_residual'):\n",
        "            residual = self.evolution.compute_residual(E_t, H_t, z_t)\n",
        "            residual_l2 = residual.pow(2).mean()\n",
        "        else:\n",
        "            residual_l2 = torch.tensor(0.0, device=E_t.device)\n",
        "\n",
        "        # parameter regularization if available\n",
        "        if hasattr(self.evolution, 'get_params_from_covariates') and covariates is not None:\n",
        "            kappa, gamma, sigma = self.evolution.get_params_from_covariates(covariates)\n",
        "            param_l2 = (kappa.pow(2).mean() + gamma.pow(2).mean())\n",
        "        else:\n",
        "            param_l2 = torch.tensor(0.0, device=E_t.device)\n",
        "\n",
        "        total = residual_weight * residual_l2 + param_weight * param_l2\n",
        "        # optional adjacency reconstruction loss\n",
        "        if target_adj is not None:\n",
        "            recon_l2 = F.mse_loss(self.evolution(E_t, H_t, z_t, covariates), target_adj)\n",
        "            total = total + 0.5 * recon_l2\n",
        "            losses['recon_l2'] = recon_l2\n",
        "\n",
        "        losses['residual_l2'] = residual_l2\n",
        "        losses['param_l2'] = param_l2\n",
        "        losses['total'] = total\n",
        "        return losses\n",
        "\n",
        "\n",
        "# ---------------------- Advanced mechanistic / causal modules ----------------------\n",
        "\n",
        "\n",
        "class DisentangledVAE(nn.Module):\n",
        "    \"\"\"VAE with factorized latent subspaces: disease vs confounders.\n",
        "\n",
        "    - Encodes node-pooled graph features to two latents z_disease and z_confound.\n",
        "    - Reconstruction decoder consumes both.\n",
        "    - Provides adversarial discriminator to force z_confound to predict site/scanner,\n",
        "      while z_disease is discouraged from containing confound information via gradient reversal.\n",
        "    \"\"\"\n",
        "    def __init__(self, input_dim: int, z_d_dim: int = 8, z_c_dim: int = 8, hidden: int = 64):\n",
        "        super().__init__()\n",
        "        self.enc = nn.Sequential(nn.Linear(input_dim, hidden), nn.ReLU(), nn.Linear(hidden, hidden))\n",
        "        # disease latents\n",
        "        self.fc_mu_d = nn.Linear(hidden, z_d_dim)\n",
        "        self.fc_logvar_d = nn.Linear(hidden, z_d_dim)\n",
        "        # confound latents\n",
        "        self.fc_mu_c = nn.Linear(hidden, z_c_dim)\n",
        "        self.fc_logvar_c = nn.Linear(hidden, z_c_dim)\n",
        "\n",
        "        self.decoder = nn.Sequential(nn.Linear(z_d_dim + z_c_dim, hidden), nn.ReLU(), nn.Linear(hidden, input_dim))\n",
        "\n",
        "        # adversarial discriminator to predict site from confound latent\n",
        "        self.discriminator = nn.Sequential(nn.Linear(z_c_dim, hidden//2), nn.ReLU(), nn.Linear(hidden//2, 1))\n",
        "\n",
        "    def reparam(self, mu: torch.Tensor, logvar: torch.Tensor) -> torch.Tensor:\n",
        "        std = (0.5 * logvar).exp()\n",
        "        eps = torch.randn_like(std)\n",
        "        return mu + eps * std\n",
        "\n",
        "    def forward(self, x: torch.Tensor) -> Dict[str, torch.Tensor]:\n",
        "        h = self.enc(x)\n",
        "        mu_d = self.fc_mu_d(h); logvar_d = self.fc_logvar_d(h)\n",
        "        mu_c = self.fc_mu_c(h); logvar_c = self.fc_logvar_c(h)\n",
        "        z_d = self.reparam(mu_d, logvar_d)\n",
        "        z_c = self.reparam(mu_c, logvar_c)\n",
        "        recon = self.decoder(torch.cat([z_d, z_c], dim=-1))\n",
        "        disc_logits = self.discriminator(z_c.detach())\n",
        "        return {'recon': recon, 'mu_d': mu_d, 'logvar_d': logvar_d, 'mu_c': mu_c, 'logvar_c': logvar_c, 'z_d': z_d, 'z_c': z_c, 'disc_logits': disc_logits}\n",
        "\n",
        "\n",
        "class MultimodalCausalFusion(nn.Module):\n",
        "    \"\"\"Simple causal chain between modalities with modality-specific encoders.\n",
        "\n",
        "    Example causal chain: sMRI -> atrophy -> connectivity -> cognition\n",
        "    The module enforces that encoded representations follow this causal order via a contrastive loss\n",
        "    pushing downstream representations to be predictable from upstream ones.\n",
        "    \"\"\"\n",
        "    def __init__(self, modalities: Dict[str, int], embed_dim: int = 32):\n",
        "        super().__init__()\n",
        "        # modalities: name -> input_dim\n",
        "        self.encoders = nn.ModuleDict({k: nn.Sequential(nn.Linear(v, embed_dim), nn.ReLU()) for k, v in modalities.items()})\n",
        "        # simple predictors between modalities\n",
        "        self.predictors = nn.ModuleDict()\n",
        "        keys = list(modalities.keys())\n",
        "        for i in range(len(keys)-1):\n",
        "            self.predictors[f\"{keys[i]}_to_{keys[i+1]}\"] = nn.Linear(embed_dim, embed_dim)\n",
        "\n",
        "    def forward(self, inputs: Dict[str, torch.Tensor]) -> Dict[str, torch.Tensor]:\n",
        "        embeds = {k: self.encoders[k](inputs[k]) for k in inputs.keys()}\n",
        "        preds = {}\n",
        "        keys = list(embeds.keys())\n",
        "        for i in range(len(keys)-1):\n",
        "            up = keys[i]; down = keys[i+1]\n",
        "            preds[f\"{up}_to_{down}\"] = self.predictors[f\"{up}_to_{down}\"](embeds[up])\n",
        "        return {'embeds': embeds, 'preds': preds}\n",
        "\n",
        "    def causal_contrastive_loss(self, embeds: Dict[str, torch.Tensor], preds: Dict[str, torch.Tensor]) -> torch.Tensor:\n",
        "        loss = 0.0\n",
        "        keys = list(embeds.keys())\n",
        "        for i in range(len(keys)-1):\n",
        "            up = keys[i]; down = keys[i+1]\n",
        "            loss = loss + F.mse_loss(preds[f\"{up}_to_{down}\"], embeds[down])\n",
        "        return loss\n",
        "\n",
        "\n",
        "def attention_structural_regularization(attn: torch.Tensor, anatomical_prior: torch.Tensor, weight: float = 1.0) -> torch.Tensor:\n",
        "    \"\"\"Encourage attention to align with an anatomical prior adjacency.\n",
        "\n",
        "    - attn: (B, heads, N, N) attention weights (probabilities)\n",
        "    - anatomical_prior: (N,N) binary or soft adjacency\n",
        "    Returns scalar loss (MSE between mean attention and prior)\n",
        "    \"\"\"\n",
        "    # reduce heads and batch\n",
        "    mean_attn = attn.mean(dim=1).mean(dim=0) if attn.dim() == 4 else attn.mean(dim=0)\n",
        "    prior = anatomical_prior.to(mean_attn.device)\n",
        "    return weight * F.mse_loss(mean_attn, prior)\n",
        "\n",
        "\n",
        "class CounterfactualOptimizer:\n",
        "    \"\"\"Gradient-based continuous relaxation optimizer to find minimal interventions.\n",
        "\n",
        "    - Learns a continuous mask M in [0,1] per edge/node and optimizes objective: loss_predicted_after_do + lambda * L1(M)\n",
        "    - Returns a thresholded sparse intervention.\n",
        "    \"\"\"\n",
        "    def __init__(self, model: DDGModel, lr: float = 1e-2, iters: int = 200, sparsity_lambda: float = 1.0):\n",
        "        self.model = model\n",
        "        self.lr = lr\n",
        "        self.iters = iters\n",
        "        self.sparsity_lambda = sparsity_lambda\n",
        "\n",
        "    def optimize_edge_intervention(self, A_init: torch.Tensor, node_features: Optional[torch.Tensor], target_improvement: float = 0.1) -> torch.Tensor:\n",
        "        # continuous mask per edge\n",
        "        M = torch.zeros_like(A_init, requires_grad=True)\n",
        "        opt = torch.optim.Adam([M], lr=self.lr)\n",
        "        best_mask = None\n",
        "        best_obj = float('inf')\n",
        "        # precompute node embeddings if a raw node feature matrix is provided\n",
        "        if node_features is not None and hasattr(self.model, 'encoder'):\n",
        "            with torch.no_grad():\n",
        "                node_emb_pref = self.model.encoder(node_features)\n",
        "        else:\n",
        "            node_emb_pref = node_features\n",
        "        for _ in range(self.iters):\n",
        "            opt.zero_grad()\n",
        "            # apply sigmoid to get [0,1]\n",
        "            mask = torch.sigmoid(M)\n",
        "            A_cf = A_init * (1 - mask)  # decreasing edges; can be changed\n",
        "            # synthesize short seq and decode cognition prediction via clinical decoder\n",
        "            seq = self.model.synthesize_adjacency_sequence(A_cf, node_embeddings_seq=[node_emb_pref] if node_emb_pref is not None else None, steps=1)\n",
        "            A1 = seq[0]\n",
        "            # get predicted cognition (use clinical_decoder expecting node embeddings; use encoder then decoder)\n",
        "            with torch.no_grad():\n",
        "                node_emb = self.model.encoder(node_features) if node_features is not None else None\n",
        "            # naive prediction: average node decoded score\n",
        "            try:\n",
        "                clinical = self.model.clinical_decoder(node_emb, A1)\n",
        "                pred = clinical.mean()\n",
        "            except Exception:\n",
        "                pred = torch.tensor(0.0, device=A_init.device)\n",
        "\n",
        "            obj = -pred + self.sparsity_lambda * mask.abs().mean()\n",
        "            obj.backward()\n",
        "            opt.step()\n",
        "            if obj.item() < best_obj:\n",
        "                best_obj = obj.item()\n",
        "                best_mask = mask.detach().cpu().numpy()\n",
        "\n",
        "        # return thresholded mask\n",
        "        return (best_mask > 0.5).astype(float)\n",
        "\n",
        "\n",
        "class MAMLTrainer:\n",
        "    \"\"\"Simple MAML-like adapter: inner-loop finetune on small support, outer updates via meta-gradient.\n",
        "\n",
        "    This is a minimal wrapper demonstrating inner-loop adaptation; for real training use higher-quality implementation.\n",
        "    \"\"\"\n",
        "    def __init__(self, model: nn.Module, inner_lr: float = 1e-2, inner_steps: int = 1):\n",
        "        self.model = model\n",
        "        self.inner_lr = inner_lr\n",
        "        self.inner_steps = inner_steps\n",
        "\n",
        "    def adapt(self, loss_fn, support_batch):\n",
        "        # create copy of parameters\n",
        "        fast_weights = {n: p.clone() for n, p in self.model.named_parameters()}\n",
        "        for _ in range(self.inner_steps):\n",
        "            loss = loss_fn(self.model, support_batch)\n",
        "            grads = torch.autograd.grad(loss, list(self.model.parameters()), create_graph=True, allow_unused=True)\n",
        "            for (name, p), g in zip(self.model.named_parameters(), grads):\n",
        "                if g is None:\n",
        "                    g = torch.zeros_like(p)\n",
        "                fast_weights[name] = fast_weights[name] - self.inner_lr * g\n",
        "            # assign fast weights back (simple approach)\n",
        "            for name, p in self.model.named_parameters():\n",
        "                p.data.copy_(fast_weights[name].data)\n",
        "        return\n",
        "\n",
        "\n",
        "def physics_monotonicity_loss(traj: torch.Tensor, vulnerable_mask: torch.Tensor, weight: float = 1.0) -> torch.Tensor:\n",
        "    \"\"\"Penalize non-monotonic increases in vulnerable edges or nodes.\n",
        "\n",
        "    - traj: (T, B, N, N) adjacency sequence or (T, B, N) node loads\n",
        "    - vulnerable_mask: (N,N) or (N,) boolean mask\n",
        "    \"\"\"\n",
        "    # compute differences\n",
        "    diffs = traj[1:] - traj[:-1]\n",
        "    # Penalize positive diffs on vulnerable entries if biology expects decline (here example)\n",
        "    mask = vulnerable_mask\n",
        "    if traj.dim() == 4:\n",
        "        mask = mask.to(traj.device)\n",
        "        penal = diffs.clamp(min=0.0) * mask\n",
        "    else:\n",
        "        penal = diffs.clamp(min=0.0) * mask\n",
        "    return weight * penal.pow(2).mean()\n",
        "\n",
        "\n",
        "class MultiTimescaleEncoder(nn.Module):\n",
        "    \"\"\"Hierarchical encoder with fast and slow GRUs to capture multiple temporal scales.\"\"\"\n",
        "    def __init__(self, input_dim: int, fast_hidden: int = 32, slow_hidden: int = 64):\n",
        "        super().__init__()\n",
        "        self.fast = nn.GRU(input_dim, fast_hidden, batch_first=True)\n",
        "        self.slow = nn.GRU(fast_hidden, slow_hidden, batch_first=True)\n",
        "\n",
        "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
        "        # x: (B, T, N*F) or (B, T, F)\n",
        "        h_fast, _ = self.fast(x)\n",
        "        h_slow, _ = self.slow(h_fast)\n",
        "        # return last slow hidden\n",
        "        return h_slow[:, -1, :]\n",
        "\n",
        "\n",
        "class StructuredCounterfactualExplainer:\n",
        "    \"\"\"Approximate minimal subnetwork edits to flip model prediction via sparse optimization.\n",
        "\n",
        "    - Uses L1-regularized continuous mask optimization then optionally solves integer rounding.\n",
        "    \"\"\"\n",
        "    def __init__(self, model: DDGModel, lr: float = 1e-2, iters: int = 200, sparsity_lambda: float = 1.0):\n",
        "        self.model = model\n",
        "        self.lr = lr\n",
        "        self.iters = iters\n",
        "        self.sparsity_lambda = sparsity_lambda\n",
        "\n",
        "    def explain(self, A_init: torch.Tensor, node_features: Optional[torch.Tensor], target_fn) -> np.ndarray:\n",
        "        # reuse CounterfactualOptimizer for initial mask\n",
        "        cf = CounterfactualOptimizer(self.model, lr=self.lr, iters=self.iters, sparsity_lambda=self.sparsity_lambda)\n",
        "        mask = cf.optimize_edge_intervention(A_init, node_features)\n",
        "        return mask\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "class BaselineMLPLSTM(nn.Module):\n",
        "    \"\"\"Simple baseline: per-node MLP -> global pooling -> LSTM -> regression.\"\"\"\n",
        "    def __init__(self, in_feats: int = 4, node_hidden: int = 32, rnn_hidden: int = 64, out_dim: int = 1):\n",
        "        super().__init__()\n",
        "        self.node_mlp = nn.Sequential(nn.Linear(in_feats, node_hidden), nn.ReLU(), nn.Linear(node_hidden, node_hidden))\n",
        "        self.pool = lambda x: x.mean(dim=2)  # (B,T,N,D)->(B,T,D)\n",
        "        self.gru = nn.GRU(node_hidden, rnn_hidden, batch_first=True)\n",
        "        self.head = nn.Sequential(nn.Linear(rnn_hidden, 64), nn.ReLU(), nn.Linear(64, out_dim))\n",
        "\n",
        "    def forward(self, node_features_sequence: torch.Tensor):\n",
        "        # node_features_sequence: (B,T,N,F)\n",
        "        B, T, N, F = node_features_sequence.shape\n",
        "        x = node_features_sequence.view(B * T, N, F)\n",
        "        x = self.node_mlp(x)  # (B*T, N, D)\n",
        "        x = x.mean(dim=1).view(B, T, -1)  # (B, T, D)\n",
        "        _, h = self.gru(x)\n",
        "        out = self.head(h.squeeze(0))\n",
        "        return out\n",
        "\n",
        "\n",
        "class StaticGNNBaseline(nn.Module):\n",
        "    \"\"\"Static GNN baseline: apply GAT per timepoint, pool, feed to LSTM/regressor.\"\"\"\n",
        "    def __init__(self, in_feats: int = 4, node_dim: int = 32, latent_dim: int = 32, rnn_hidden: int = 64):\n",
        "        super().__init__()\n",
        "        self.gat = GATEncoder(in_feats, node_dim, num_heads=4, head_dim=max(8, node_dim // 4))\n",
        "        self.pool = lambda x: x.mean(dim=2)\n",
        "        self.gru = nn.GRU(node_dim, rnn_hidden, batch_first=True)\n",
        "        self.head = nn.Sequential(nn.Linear(rnn_hidden, 64), nn.ReLU(), nn.Linear(64, 1))\n",
        "        def forward(self, node_features_sequence: torch.Tensor, adjacency_sequence: torch.Tensor):\n",
        "            B, T, N, F = node_features_sequence.shape\n",
        "            feats = []\n",
        "            for t in range(T):\n",
        "                x_t = node_features_sequence[:, t]\n",
        "                A_t = adjacency_sequence[:, t]\n",
        "                h = self.gat(x_t, A_t)\n",
        "                feats.append(h.mean(dim=1))\n",
        "            feats = torch.stack(feats, dim=1)\n",
        "            _, h = self.gru(feats)\n",
        "            return self.head(h.squeeze(0))\n",
        "\n",
        "# ---------------------- Losses + training utils ----------------------\n",
        "\n",
        "# Replace compute_losses function\n",
        "def compute_losses(\n",
        "    batch: Dict[str, torch.Tensor],\n",
        "    outputs: List[Dict[str, Any]],\n",
        "    latent_sequence: Optional[torch.Tensor] = None,\n",
        "    lambda_edge: float = 1.0,\n",
        "    lambda_clinical: float = 0.5,\n",
        "    lambda_latent: float = 0.1,\n",
        "    kl_beta: float = 1.0,\n",
        "    lambda_grad: float = 0.01,\n",
        "    lambda_importance: float = 0.01,\n",
        "    lambda_entropy: float = 0.01,\n",
        "    lambda_mi: float = 0.01\n",
        ") -> Tuple[torch.Tensor, Dict[str, float]]:\n",
        "    \"\"\"Compute comprehensive loss with edge forecasting, clinical prediction, and latent regularization.\n",
        "\n",
        "    Args:\n",
        "        batch: Dictionary with 'node_features', 'adjacency_matrices', 'cognitive_scores'.\n",
        "        outputs: List of output dictionaries from rollout steps.\n",
        "        latent_sequence: (B, T, latent_dim) Latent trajectory for regularization.\n",
        "        lambda_edge: Weight for edge forecasting loss.\n",
        "        lambda_clinical: Weight for clinical prediction loss.\n",
        "        lambda_latent: Weight for latent trajectory regularization.\n",
        "        lambda_grad: Weight for gradient smoothing in edge evolution.\n",
        "\n",
        "    Returns:\n",
        "        total_loss: Combined loss tensor.\n",
        "        metrics: Dictionary of individual loss components.\n",
        "    \"\"\"\n",
        "    true_next_adjacency = batch['adjacency_matrices'][:, -1]\n",
        "    true_scores = batch['cognitive_scores']\n",
        "\n",
        "    # Edge loss: compare predicted vs true\n",
        "    predicted_adjacency = outputs[0]['predicted_adjacency']\n",
        "    loss_edge = F.mse_loss(predicted_adjacency, true_next_adjacency)\n",
        "\n",
        "    # Edge smoothness: consecutive predictions should be smooth\n",
        "    if len(outputs) > 1:\n",
        "        edge_diffs = []\n",
        "        for i in range(len(outputs) - 1):\n",
        "            adj_i = outputs[i]['evolved_adjacency']\n",
        "            adj_next = outputs[i+1]['evolved_adjacency']\n",
        "            edge_diffs.append(torch.mean((adj_next - adj_i)**2))\n",
        "        loss_edge_smooth = torch.stack(edge_diffs).mean()\n",
        "        loss_edge = loss_edge + lambda_grad * loss_edge_smooth\n",
        "\n",
        "    # Clinical loss\n",
        "    predicted_score = outputs[0]['predicted_score'].squeeze(-1)\n",
        "    loss_clinical = F.mse_loss(predicted_score, true_scores)\n",
        "\n",
        "    # Latent regularization: smoothness + boundedness\n",
        "    loss_latent = torch.tensor(0.0, device=predicted_adjacency.device)\n",
        "    if latent_sequence is not None and lambda_latent > 0:\n",
        "        if latent_sequence.shape[1] > 1:\n",
        "            # Smoothness: latent should evolve gradually\n",
        "            latent_diff = torch.diff(latent_sequence, dim=1)\n",
        "            loss_smooth = torch.mean(latent_diff ** 2)\n",
        "            loss_latent = loss_latent + loss_smooth\n",
        "\n",
        "        # Boundedness: encourage z near 0\n",
        "        loss_bound = 0.1 * torch.mean(latent_sequence ** 2)\n",
        "        loss_latent = loss_latent + loss_bound\n",
        "\n",
        "        # L1 sparsity optional\n",
        "        loss_sparsity = 0.01 * torch.mean(torch.abs(latent_sequence))\n",
        "        loss_latent = loss_latent + loss_sparsity\n",
        "\n",
        "    total_loss = lambda_edge * loss_edge + lambda_clinical * loss_clinical + lambda_latent * loss_latent\n",
        "\n",
        "    # KL term for VAE-style latent encoder (if provided in outputs)\n",
        "    loss_kl = torch.tensor(0.0, device=predicted_adjacency.device)\n",
        "    try:\n",
        "        mu = outputs[0].get('latent_mu', None)\n",
        "        logvar = outputs[0].get('latent_logvar', None)\n",
        "        if mu is not None and logvar is not None:\n",
        "            # compute KL divergence to N(0, I) per batch then average\n",
        "            kl = -0.5 * torch.sum(1.0 + logvar - mu.pow(2) - logvar.exp(), dim=1)\n",
        "            loss_kl = torch.mean(kl)\n",
        "            total_loss = total_loss + float(kl_beta) * loss_kl\n",
        "    except Exception:\n",
        "        loss_kl = torch.tensor(0.0, device=predicted_adjacency.device)\n",
        "\n",
        "    # Importance probability regularization (sparsity + entropy)\n",
        "    loss_importance = torch.tensor(0.0, device=predicted_adjacency.device)\n",
        "    try:\n",
        "        PX = outputs[0].get('PX', None)\n",
        "        PA = outputs[0].get('PA', None)\n",
        "        if PX is not None:\n",
        "            # sparsity: encourage small fraction of selected nodes\n",
        "            loss_importance = loss_importance + torch.mean(PX)\n",
        "            # entropy: encourage low entropy (push toward 0/1)\n",
        "            eps = 1e-8\n",
        "            ent = - (PX * torch.log(PX + eps) + (1.0 - PX) * torch.log(1.0 - PX + eps))\n",
        "            loss_importance = loss_importance + lambda_entropy * torch.mean(ent)\n",
        "        if PA is not None:\n",
        "            loss_importance = loss_importance + torch.mean(PA)\n",
        "            eps = 1e-8\n",
        "            ent_e = - (PA * torch.log(PA + eps) + (1.0 - PA) * torch.log(1.0 - PA + eps))\n",
        "            loss_importance = loss_importance + lambda_entropy * torch.mean(ent_e)\n",
        "        total_loss = total_loss + lambda_importance * loss_importance\n",
        "    except Exception:\n",
        "        loss_importance = torch.tensor(0.0, device=predicted_adjacency.device)\n",
        "\n",
        "    # Mutual Information objective between importance masks and labels (optional)\n",
        "    loss_mi = torch.tensor(0.0, device=predicted_adjacency.device)\n",
        "    try:\n",
        "        PX = outputs[0].get('PX', None)\n",
        "        PA = outputs[0].get('PA', None)\n",
        "        if PX is not None:\n",
        "            try:\n",
        "                from alzheimers_gnn.importance import MINELoss\n",
        "                B = PX.size(0)\n",
        "                x = PX.view(B, -1)\n",
        "                y = true_scores.view(B, 1)\n",
        "                mine = MINELoss(x_dim=x.size(1), y_dim=1)\n",
        "                mi_loss_val = mine(x, y)\n",
        "                loss_mi = mi_loss_val\n",
        "                total_loss = total_loss + lambda_mi * loss_mi\n",
        "            except Exception:\n",
        "                loss_mi = torch.tensor(0.0, device=predicted_adjacency.device)\n",
        "        elif PA is not None:\n",
        "            try:\n",
        "                from alzheimers_gnn.importance import MINELoss\n",
        "                B = PA.size(0)\n",
        "                x = PA.view(B, -1)\n",
        "                y = true_scores.view(B, 1)\n",
        "                mine = MINELoss(x_dim=x.size(1), y_dim=1)\n",
        "                mi_loss_val = mine(x, y)\n",
        "                loss_mi = mi_loss_val\n",
        "                total_loss = total_loss + lambda_mi * loss_mi\n",
        "            except Exception:\n",
        "                loss_mi = torch.tensor(0.0, device=predicted_adjacency.device)\n",
        "    except Exception:\n",
        "        loss_mi = torch.tensor(0.0, device=predicted_adjacency.device)\n",
        "\n",
        "    # Reconstruction loss (if reconstructions provided in outputs or separately)\n",
        "    loss_recon = torch.tensor(0.0, device=predicted_adjacency.device)\n",
        "    try:\n",
        "        # outputs may be from model.forward returning (preds, latent_sequence, recon)\n",
        "        recon = None\n",
        "        if isinstance(outputs, tuple) and len(outputs) == 3:\n",
        "            # legacy: model returned (outputs, latent_sequence, recon)\n",
        "            recon = outputs[2]\n",
        "        else:\n",
        "            recon = outputs[0].get('recon', None)\n",
        "        if recon is not None and 'node_features' in batch:\n",
        "            # recon: (B, T, N, F) or broadcastable\n",
        "            true_feats = batch['node_features'].to(predicted_adjacency.device)\n",
        "            if recon.size() != true_feats.size():\n",
        "                # try to broadcast/repeat along time or nodes\n",
        "                recon_adj = recon\n",
        "                # if recon is (B, T, N, F) expected; otherwise skip\n",
        "            loss_recon = F.mse_loss(recon, true_feats)\n",
        "            total_loss = total_loss + 0.5 * loss_recon\n",
        "    except Exception:\n",
        "        loss_recon = torch.tensor(0.0, device=predicted_adjacency.device)\n",
        "\n",
        "    # Biological regularization (optional): penalize increases in connectivity on vulnerable edges\n",
        "    loss_bio = torch.tensor(0.0, device=predicted_adjacency.device)\n",
        "    if 'vulnerable_edges' in batch and batch.get('vulnerable_edges') is not None and batch.get('lambda_bio', 0.0) > 0.0:\n",
        "        try:\n",
        "            vulnerable = batch['vulnerable_edges']  # expected (K,2) or list of tuples per-batch or global\n",
        "            lambda_bio = float(batch.get('lambda_bio', 0.0))\n",
        "            # prev adjacency (last observed) -- require adjacency_matrices in batch\n",
        "            if 'adjacency_matrices' in batch and batch['adjacency_matrices'] is not None:\n",
        "                prev_adj = batch['adjacency_matrices'][:, -1].to(predicted_adjacency.device)\n",
        "            else:\n",
        "                prev_adj = predicted_adjacency.detach() * 0.0\n",
        "            # compute dt = predicted - prev; penalize positive dt on vulnerable edges only\n",
        "            dt = predicted_adjacency - prev_adj\n",
        "            if isinstance(vulnerable, torch.Tensor):\n",
        "                vuln_idx = vulnerable.long()\n",
        "            else:\n",
        "                vuln_idx = torch.tensor(vulnerable, device=predicted_adjacency.device, dtype=torch.long)\n",
        "\n",
        "            # Support either global list of pairs or per-batch mask\n",
        "            if vuln_idx.dim() == 2 and vuln_idx.size(-1) == 2:\n",
        "                # gather dt values at vulnerable pairs for each batch\n",
        "                i_idx = vuln_idx[:, 0]\n",
        "                j_idx = vuln_idx[:, 1]\n",
        "                # dt: (B, N, N) -> select (B, K)\n",
        "                dt_vals = dt[:, i_idx, j_idx]\n",
        "                # penalize positive increases\n",
        "                loss_bio = torch.mean(F.relu(dt_vals))\n",
        "            else:\n",
        "                # if mask provided as (B,N,N) boolean tensor\n",
        "                mask = vuln_idx.bool()\n",
        "                masked_dt = dt * mask\n",
        "                loss_bio = torch.mean(F.relu(masked_dt))\n",
        "\n",
        "            total_loss = total_loss + lambda_bio * loss_bio\n",
        "        except Exception:\n",
        "            # ignore malformed vulnerable specification\n",
        "            loss_bio = torch.tensor(0.0, device=predicted_adjacency.device)\n",
        "\n",
        "    return total_loss, {\n",
        "        'loss_edge': loss_edge.item(),\n",
        "        'loss_clinical': loss_clinical.item(),\n",
        "        'loss_latent': loss_latent.item() if isinstance(loss_latent, torch.Tensor) else loss_latent,\n",
        "        'loss_kl': float(loss_kl.item()) if isinstance(loss_kl, torch.Tensor) else float(loss_kl),\n",
        "        'loss_importance': float(loss_importance.item()) if isinstance(loss_importance, torch.Tensor) else float(loss_importance),\n",
        "        'loss_mi': float(loss_mi.item()) if isinstance(loss_mi, torch.Tensor) else float(loss_mi),\n",
        "        'loss_recon': float(loss_recon.item()) if isinstance(loss_recon, torch.Tensor) else float(loss_recon),\n",
        "        'loss_bio': float(loss_bio.item()) if isinstance(loss_bio, torch.Tensor) else float(loss_bio),\n",
        "        'total': total_loss.item()\n",
        "    }\n",
        "\n",
        "# ---------------------- Lightning wrapper (optional) ----------------------\n",
        "\n",
        "if _HAS_PL:\n",
        "    try:\n",
        "        import importlib as _importlib_local\n",
        "        torchmetrics = _importlib_local.import_module('torchmetrics')\n",
        "    except Exception:\n",
        "        torchmetrics = None\n",
        "\n",
        "    class DDGLightning(pl.LightningModule):\n",
        "        def __init__(self, model: DDGModel, lr: float = 1e-3, weight_decay: float = 1e-5):\n",
        "            super().__init__()\n",
        "            self.model = model\n",
        "            self.lr = lr\n",
        "            self.weight_decay = weight_decay\n",
        "            self.save_hyperparameters()\n",
        "\n",
        "        def forward(self, node_features, adjacency_matrices):\n",
        "            return self.model(node_features, adjacency_matrices, rollout_steps=1, teacher_forcing_prob=1.0)\n",
        "\n",
        "        def training_step(self, batch, batch_idx):\n",
        "            node_features = batch['node_features']\n",
        "            adjacency_matrices = batch['adjacency_matrices']\n",
        "\n",
        "            # compute kl_beta (allow model to expose schedule)\n",
        "            kl_beta = getattr(self.model, 'kl_beta', 1.0)\n",
        "            outputs, latent_sequence = self.model(node_features, adjacency_matrices, rollout_steps=1, teacher_forcing_prob=0.8)\n",
        "            loss, sublogs = compute_losses(batch, outputs, latent_sequence=latent_sequence, lambda_latent=0.1, kl_beta=kl_beta)\n",
        "\n",
        "            self.log('train/loss', loss, prog_bar=True)\n",
        "            self.log('train/edge_mse', sublogs['loss_edge'])\n",
        "            self.log('train/clinical_mse', sublogs['loss_clinical'])\n",
        "            self.log('train/latent_reg', sublogs['loss_latent'])\n",
        "            return loss\n",
        "\n",
        "        def validation_step(self, batch, batch_idx):\n",
        "            node_features = batch['node_features']\n",
        "            adjacency_matrices = batch['adjacency_matrices']\n",
        "            true_scores = batch['cognitive_scores']\n",
        "\n",
        "            kl_beta = getattr(self.model, 'kl_beta', 1.0)\n",
        "            outputs, latent_sequence = self.model(node_features, adjacency_matrices, rollout_steps=1, teacher_forcing_prob=1.0)\n",
        "            loss, sublogs = compute_losses(batch, outputs, latent_sequence=latent_sequence, lambda_latent=0.1, kl_beta=kl_beta)\n",
        "\n",
        "            self.log('val/loss', loss, prog_bar=True)\n",
        "            self.log('val/edge_mse', sublogs['loss_edge'])\n",
        "            self.log('val/clinical_mse', sublogs['loss_clinical'])\n",
        "            self.log('val/latent_reg', sublogs['loss_latent'])\n",
        "\n",
        "            predicted_score = outputs[0]['predicted_score'].squeeze(-1)\n",
        "            self.log('val/clinical_mae', F.l1_loss(predicted_score, true_scores))\n",
        "            return loss\n",
        "\n",
        "        def configure_optimizers(self):\n",
        "            optimizer = torch.optim.Adam(self.parameters(), lr=self.lr, weight_decay=self.weight_decay)\n",
        "            scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
        "                optimizer, mode='min', factor=0.5, patience=3\n",
        "            )\n",
        "            return {\n",
        "                \"optimizer\": optimizer,\n",
        "                \"lr_scheduler\": {\n",
        "                    \"scheduler\": scheduler,\n",
        "                    \"monitor\": \"val/loss\",\n",
        "                    \"interval\": \"epoch\",\n",
        "                    \"frequency\": 1\n",
        "                }\n",
        "            }\n",
        "\n",
        "# ---------------------- Animation & visualization helpers ----------------------\n",
        "\n",
        "def animate_adjacency_sequence(E_seq, title='Adjacency evolution', vmin=0.0, vmax=None, save_path=None, show_colorbar=True):\n",
        "    # E_seq: list or array of (N,N)\n",
        "    if not _HAS_MATPLOTLIB:\n",
        "        print('Warning: matplotlib not installed, skipping animation. Install with: pip install matplotlib')\n",
        "        return None\n",
        "\n",
        "    IPython_HTML = None\n",
        "    if _HAS_IPYTHON:\n",
        "        from IPython.display import HTML as IPython_HTML  # import only if IPython is present\n",
        "\n",
        "    if isinstance(E_seq, (list, tuple)):\n",
        "        E_seq = np.stack(E_seq, axis=0)\n",
        "    if vmax is None:\n",
        "        vmax = float(np.max(E_seq))\n",
        "\n",
        "    # Safe to use plt/animation because _HAS_MATPLOTLIB was checked above\n",
        "    # Import matplotlib modules locally to ensure they are present and not the global None sentinel\n",
        "    import importlib as _local_importlib\n",
        "    plt_local = _local_importlib.import_module('matplotlib.pyplot')\n",
        "    anim_local = _local_importlib.import_module('matplotlib.animation')\n",
        "\n",
        "    fig, ax = plt_local.subplots(figsize=(4,4))\n",
        "    im = ax.imshow(E_seq[0], vmin=vmin, vmax=vmax)\n",
        "    ax.set_title(f'{title} (t=0)')\n",
        "    if show_colorbar:\n",
        "        try:\n",
        "            plt_local.colorbar(im, ax=ax)\n",
        "        except Exception:\n",
        "            # non-fatal: continue without colorbar if it fails\n",
        "            pass\n",
        "\n",
        "    def update(i):\n",
        "        im.set_data(E_seq[i])\n",
        "        ax.set_title(f'{title} (t={i})')\n",
        "        return (im,)\n",
        "\n",
        "    anim = anim_local.FuncAnimation(fig, update, frames=E_seq.shape[0], interval=600, blit=True)\n",
        "    plt_local.close(fig)\n",
        "    if save_path:\n",
        "        try:\n",
        "            Writer = anim_local.writers['ffmpeg']\n",
        "            writer = Writer(fps=1, metadata=dict(artist='ddg'), bitrate=1800)\n",
        "            anim.save(save_path, writer=writer)\n",
        "            print(f'saved animation to {save_path}')\n",
        "        except Exception as e:\n",
        "            print('warning: could not save mp4 (ffmpeg may not be installed):', e)\n",
        "    if _HAS_IPYTHON and IPython_HTML is not None:\n",
        "        try:\n",
        "            return IPython_HTML(anim.to_jshtml())\n",
        "        except Exception:\n",
        "            return None\n",
        "    return None\n",
        "\n",
        "\n",
        "# ---------------------- Edge-Level Interpretability & Analysis ----------------------\n",
        "\n",
        "def compute_edge_importance(E_seq_true, E_seq_pred=None):\n",
        "    \"\"\"Compute per-edge degeneration importance from a true adjacency sequence.\n",
        "\n",
        "    Args:\n",
        "        E_seq_true: list or array of (N,N) true adjacency matrices\n",
        "\n",
        "    Returns:\n",
        "        edge_importance: (N,N) matrix of total absolute changes per edge\n",
        "        top_edges: list of top-k (i,j) pairs sorted by importance\n",
        "    \"\"\"\n",
        "    if isinstance(E_seq_true, (list, tuple)):\n",
        "        E_seq_true = np.stack(E_seq_true, axis=0)\n",
        "    # E_seq_pred is accepted for backward compatibility but not used\n",
        "    _ = E_seq_pred\n",
        "\n",
        "    # Compute per-edge changes\n",
        "    edge_changes = np.abs(np.diff(E_seq_true, axis=0))  # (T-1, N, N)\n",
        "    edge_importance = np.sum(edge_changes, axis=0)  # (N, N)\n",
        "\n",
        "    # Find top edges\n",
        "    N = edge_importance.shape[0]\n",
        "    flat_idx = np.argsort(edge_importance.ravel())[::-1]\n",
        "    top_edges = [(int(i), int(j)) for i, j in zip(*np.unravel_index(flat_idx[:10], (N, N)))]\n",
        "\n",
        "    return edge_importance, top_edges\n",
        "\n",
        "def compute_node_degeneration_rate(E_seq):\n",
        "    \"\"\"Compute per-node strength changes over time (regional degeneration).\n",
        "\n",
        "    Args:\n",
        "        E_seq: list or array of (N,N) adjacency matrices\n",
        "\n",
        "    Returns:\n",
        "        node_strength: (N, T) strength of each node over time\n",
        "        degeneration_rate: (N,) rate of strength decline per node\n",
        "    \"\"\"\n",
        "    if isinstance(E_seq, (list, tuple)):\n",
        "        E_seq = np.stack(E_seq, axis=0)\n",
        "\n",
        "    # Node strength = sum of incident edges\n",
        "    node_strength = np.sum(E_seq, axis=2)  # (T, N)\n",
        "    node_strength = node_strength.T  # (N, T)\n",
        "\n",
        "    # Degeneration rate: negative slope of strength over time\n",
        "    degeneration_rate = np.zeros(E_seq.shape[1])\n",
        "    for i in range(E_seq.shape[1]):\n",
        "        coeffs = np.polyfit(np.arange(E_seq.shape[0]), node_strength[i, :], 1)\n",
        "        degeneration_rate[i] = -coeffs[0]  # negative slope\n",
        "\n",
        "    return node_strength, degeneration_rate\n",
        "\n",
        "def compute_forecast_stability(E_seq_multi_rollout, k_steps=3):\n",
        "    \"\"\"Check if multi-step forecasts converge or diverge.\n",
        "\n",
        "    Args:\n",
        "        E_seq_multi_rollout: list of E sequences with different rollout steps\n",
        "\n",
        "    Returns:\n",
        "        stability: measure of forecast divergence (lower = more stable)\n",
        "    \"\"\"\n",
        "    if len(E_seq_multi_rollout) < 2:\n",
        "        return 1.0\n",
        "\n",
        "    # Compare predictions at overlapping future timepoints\n",
        "    diffs = []\n",
        "    for i in range(1, len(E_seq_multi_rollout)):\n",
        "        # Only compare first k steps\n",
        "        min_len = min(len(E_seq_multi_rollout[0]), len(E_seq_multi_rollout[i]))\n",
        "        for t in range(min(3, min_len - 1)):\n",
        "            diff = np.abs(E_seq_multi_rollout[0][t] - E_seq_multi_rollout[i][t]).mean()\n",
        "            diffs.append(diff)\n",
        "\n",
        "    return float(np.mean(diffs)) if diffs else 1.0\n",
        "\n",
        "def attention_flow_analysis(attn_seq, H_seq):\n",
        "    \"\"\"Analyze how information flows through attention heads over time.\n",
        "\n",
        "    Args:\n",
        "        attn_seq: list of (B,h,N,N) attention matrices\n",
        "        H_seq: (B,T,N,D) node embeddings\n",
        "\n",
        "    Returns:\n",
        "        info_flow: (N,) measure of information flow importance per node\n",
        "    \"\"\"\n",
        "    # attn_seq: list of attention from rollout\n",
        "    if len(attn_seq) == 0:\n",
        "        return None\n",
        "\n",
        "    avg_attn_seq = []\n",
        "    for att in attn_seq:\n",
        "        # convert to numpy safely (handles torch.Tensor or numpy arrays)\n",
        "        if isinstance(att, torch.Tensor):\n",
        "            att_np = att.detach().cpu().numpy()\n",
        "        else:\n",
        "            att_np = np.array(att)\n",
        "\n",
        "        # att_np can be (B, h, N, N), (h, N, N), or (N, N)\n",
        "        if att_np.ndim == 4:\n",
        "            # average over batch and heads\n",
        "            att_avg = att_np.mean(axis=(0, 1))  # (N, N)\n",
        "        elif att_np.ndim == 3:\n",
        "            # average over heads\n",
        "            att_avg = att_np.mean(axis=0)  # (N, N)\n",
        "        elif att_np.ndim == 2:\n",
        "            att_avg = att_np  # already (N,N)\n",
        "        else:\n",
        "            raise ValueError(f'Unsupported attention tensor shape: {att_np.shape}')\n",
        "\n",
        "        avg_attn_seq.append(att_avg)\n",
        "\n",
        "    # Integrate attention over time: which nodes receive most information?\n",
        "    total_attn = np.sum(avg_attn_seq, axis=0)  # (N, N)\n",
        "    info_flow = np.sum(total_attn, axis=0)  # (N,) in-degree\n",
        "\n",
        "    return info_flow / (info_flow.sum() + 1e-6)  # normalize\n",
        "\n",
        "\n",
        "def compute_node_importance_gradients(model: nn.Module, node_features: torch.Tensor, adjacency_matrices: torch.Tensor, device='cpu') -> np.ndarray:\n",
        "    \"\"\"Compute gradient-based importance scores per node for predicted clinical score.\n",
        "\n",
        "    Returns normalized importance per node (N,).\n",
        "    \"\"\"\n",
        "    model = model.to(device)\n",
        "    model.eval()\n",
        "    node_features = node_features.to(device).requires_grad_(True)\n",
        "    adjacency_matrices = adjacency_matrices.to(device)\n",
        "    with torch.enable_grad():\n",
        "        outputs, _ = model(node_features, adjacency_matrices, rollout_steps=1, teacher_forcing_prob=1.0)\n",
        "        pred = outputs[0]['predicted_score'].squeeze(-1).mean()\n",
        "        grads = torch.autograd.grad(pred, node_features, retain_graph=False)[0]  # (B, T, N, F)\n",
        "\n",
        "    # aggregate gradients across batch/time/features -> node importance\n",
        "    grads_np = grads.detach().cpu().numpy()\n",
        "    # mean absolute gradient across batch, time and features per node\n",
        "    imp = np.mean(np.abs(grads_np), axis=(0, 1, 3))  # (N,)\n",
        "    if imp.sum() > 0:\n",
        "        imp = imp / (imp.sum() + 1e-9)\n",
        "    return imp\n",
        "\n",
        "\n",
        "# ---------------------- Network degeneration biomarkers (Option C) ----------------------\n",
        "def edge_half_life(E_seq: np.ndarray, min_fraction: float = 0.5) -> np.ndarray:\n",
        "    \"\"\"Compute half-life (in timesteps) for each edge: time until edge strength falls to min_fraction of initial.\n",
        "\n",
        "    Args:\n",
        "        E_seq: (T, N, N) adjacency sequence over time\n",
        "        min_fraction: fraction of initial strength defining 'half-life' (default 0.5)\n",
        "\n",
        "    Returns:\n",
        "        half_life: (N, N) array with half-life in timesteps (np.inf if never reaches)\n",
        "    \"\"\"\n",
        "    if isinstance(E_seq, (list, tuple)):\n",
        "        E_seq = np.stack(E_seq, axis=0)\n",
        "    T, N, _ = E_seq.shape\n",
        "    half = np.full((N, N), np.inf, dtype=np.float32)\n",
        "    init = E_seq[0]\n",
        "    for t in range(1, T):\n",
        "        mask = (E_seq[t] <= init * min_fraction)\n",
        "        newly = np.where((half == np.inf) & mask)\n",
        "        for i, j in zip(newly[0], newly[1]):\n",
        "            half[i, j] = float(t)\n",
        "    return half\n",
        "\n",
        "\n",
        "def node_vulnerability_index(H_seq: np.ndarray) -> np.ndarray:\n",
        "    \"\"\"Compute a vulnerability index per node from node-embedding trajectories.\n",
        "\n",
        "    H_seq: (T, N, D) or (N, T) strengths. If (T,N,D) compute L1 norm of temporal derivative per node.\n",
        "    Returns vulnerability: (N,) normalized to sum to 1.\n",
        "    \"\"\"\n",
        "    H = np.array(H_seq)\n",
        "    if H.ndim == 3:\n",
        "        # compute temporal derivative (finite differences) and integrate norm\n",
        "        dH = np.diff(H, axis=0)  # (T-1, N, D)\n",
        "        vuln = np.mean(np.linalg.norm(dH, axis=-1), axis=0)  # (N,)\n",
        "    elif H.ndim == 2:\n",
        "        # already (N,T) strengths\n",
        "        dH = np.diff(H, axis=1)\n",
        "        vuln = np.mean(np.abs(dH), axis=1)\n",
        "    else:\n",
        "        raise ValueError('Unsupported H_seq shape for vulnerability computation')\n",
        "    if vuln.sum() > 0:\n",
        "        vuln = vuln / (vuln.sum() + 1e-12)\n",
        "    return vuln\n",
        "\n",
        "\n",
        "def network_entropy_over_time(E_seq: np.ndarray) -> Tuple[np.ndarray, float]:\n",
        "    \"\"\"Compute network entropy at each timepoint and return mean entropy.\n",
        "\n",
        "    Entropy is computed on the normalized degree distribution at each timepoint.\n",
        "    Returns (entropies, mean_entropy).\n",
        "    \"\"\"\n",
        "    if isinstance(E_seq, (list, tuple)):\n",
        "        E_seq = np.stack(E_seq, axis=0)\n",
        "    T = E_seq.shape[0]\n",
        "    entropies = np.zeros(T, dtype=np.float32)\n",
        "    for t in range(T):\n",
        "        degrees = np.sum(E_seq[t], axis=1)\n",
        "        if degrees.sum() == 0:\n",
        "            entropies[t] = 0.0\n",
        "            continue\n",
        "        p = degrees / (degrees.sum() + 1e-12)\n",
        "        entropies[t] = -np.sum(p * np.log(p + 1e-12))\n",
        "    mean_entropy = float(np.mean(entropies))\n",
        "    return entropies, mean_entropy\n",
        "\n",
        "\n",
        "def hub_collapse_rate(E_seq: np.ndarray, top_k: int = 1) -> np.ndarray:\n",
        "    \"\"\"Compute collapse rate for top-k hubs: negative slope of their strengths over time.\n",
        "\n",
        "    Returns collapse_rates: (k,) where higher positive value means faster collapse.\n",
        "    \"\"\"\n",
        "    if isinstance(E_seq, (list, tuple)):\n",
        "        E_seq = np.stack(E_seq, axis=0)\n",
        "    T, N, _ = E_seq.shape\n",
        "    strengths = np.sum(E_seq, axis=2)  # (T, N)\n",
        "    avg_strength = strengths.mean(axis=0)\n",
        "    hubs = np.argsort(-avg_strength)[:top_k]\n",
        "    rates = []\n",
        "    for h in hubs:\n",
        "        coeffs = np.polyfit(np.arange(T), strengths[:, h], 1)\n",
        "        rates.append(-coeffs[0])\n",
        "    return np.array(rates, dtype=np.float32)\n",
        "\n",
        "\n",
        "# ---------------------- Counterfactual connectivity interventions (Option D) ----------------------\n",
        "def apply_edge_intervention(adjacency: np.ndarray, intervention_pairs: List[Tuple[int, int]], delta: float = 0.2) -> np.ndarray:\n",
        "    \"\"\"Apply additive intervention to adjacency matrix on given edge pairs.\n",
        "\n",
        "    Returns a new adjacency matrix with interventions applied (clipped to non-negative and normalized).\n",
        "    \"\"\"\n",
        "    adj = adjacency.copy()\n",
        "    for (i, j) in intervention_pairs:\n",
        "        adj[i, j] = adj[i, j] + delta\n",
        "        adj[j, i] = adj[j, i] + delta\n",
        "    adj = np.clip(adj, 0.0, None)\n",
        "    if adj.max() > 0:\n",
        "        adj = adj / (adj.max() + 1e-12)\n",
        "    return adj\n",
        "\n",
        "def counterfactual_edge_intervention(\n",
        "    model: nn.Module,\n",
        "    node_features: torch.Tensor,\n",
        "    adjacency_matrices: torch.Tensor,\n",
        "    intervention_pairs: List[Tuple[int, int]],\n",
        "    delta: float = 0.2,\n",
        "    rollout_steps: int = 5,\n",
        "    device: Optional[str] = None\n",
        ") -> Dict[str, Any]:\n",
        "    \"\"\"Simulate counterfactuals: intervene on edges at the last observed timepoint and rollout.\n",
        "\n",
        "    Args:\n",
        "        model: trained DDGModel\n",
        "        node_features: (B, T, N, F) tensor\n",
        "        adjacency_matrices: (B, T, N, N) tensor\n",
        "        intervention_pairs: list of (i,j) pairs to increase\n",
        "        delta: additive increase applied to each selected edge\n",
        "        rollout_steps: how many steps to forecast\n",
        "        device: device string or torch.device\n",
        "\n",
        "    Returns:\n",
        "        dict with 'original': predicted outputs without intervention,\n",
        "                     'intervened': predicted outputs with intervention,\n",
        "                     'modified_adjacency': the intervened adjacency tensor\n",
        "    \"\"\"\n",
        "    device = device or (torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu'))\n",
        "    model = model.to(device); model.eval()\n",
        "\n",
        "    node_features = node_features.to(device)\n",
        "    adjacency_matrices = adjacency_matrices.to(device)\n",
        "\n",
        "    # Original prediction\n",
        "    with torch.no_grad():\n",
        "        original_outputs, _ = model(node_features, adjacency_matrices, rollout_steps=rollout_steps, teacher_forcing_prob=1.0)\n",
        "\n",
        "    # Apply intervention to last observed adjacency for each batch\n",
        "    B, T, N, _ = adjacency_matrices.shape\n",
        "    modified_adj = adjacency_matrices.clone()\n",
        "    for b in range(B):\n",
        "        last = adjacency_matrices[b, -1].detach().cpu().numpy()\n",
        "        new_last = apply_edge_intervention(last, intervention_pairs, delta=delta)\n",
        "        modified_adj[b, -1] = torch.tensor(new_last, dtype=adjacency_matrices.dtype, device=device)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        intervened_outputs, _ = model(node_features, modified_adj, rollout_steps=rollout_steps, teacher_forcing_prob=1.0)\n",
        "\n",
        "    return {\n",
        "        'original': original_outputs,\n",
        "        'intervened': intervened_outputs,\n",
        "        'modified_adjacency': modified_adj\n",
        "    }\n",
        "\n",
        "# ---------------------- Quick run / demo function ----------------------\n",
        "\n",
        "# module-global early stopping counter used by quick_demo to avoid assigning attributes to the function object\n",
        "_quick_demo_patience_counter = 0\n",
        "\n",
        "def quick_demo(train_epochs=10, device=None, use_gde=False, batch_size=16, validate=True):\n",
        "    \"\"\"Complete training, validation, and analysis pipeline.\n",
        "\n",
        "    Args:\n",
        "        train_epochs: number of training epochs\n",
        "        device: torch device or auto-detect\n",
        "        use_gde: whether to use continuous-time ODE evolution\n",
        "        batch_size: training batch size\n",
        "        validate: whether to run validation checks\n",
        "    \"\"\"\n",
        "    device = device or (torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu'))\n",
        "    print(f'Starting training on device {device}')\n",
        "    print('='*80)\n",
        "\n",
        "    # Initialize model\n",
        "    model = DDGModel(in_feats=3, node_dim=32, latent_dim=12, use_edge_gru=False, use_gde=use_gde).to(device)\n",
        "    # if using VAE reconstructor, enable here\n",
        "    # re-create model with reconstructor for demo if available\n",
        "    try:\n",
        "        model = DDGModel(in_feats=3, node_dim=32, latent_dim=12, use_edge_gru=False, use_gde=use_gde, use_vae=True, use_vae_recon=True, recon_T=6).to(device)\n",
        "    except Exception:\n",
        "        pass\n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr=1e-3, weight_decay=1e-5)\n",
        "    scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=5, gamma=0.5)\n",
        "\n",
        "    # Create datasets\n",
        "    ds_train = SimulatedDDGDataset(num_subjects=120, num_regions=16, num_timepoints=6, seed=10, noise_level=0.04)\n",
        "    ds_val = SimulatedDDGDataset(num_subjects=40, num_regions=16, num_timepoints=6, seed=11, noise_level=0.04)\n",
        "    dl_train = DataLoader(ds_train, batch_size=batch_size, shuffle=True, collate_fn=collate_batch, num_workers=0)\n",
        "    dl_val = DataLoader(ds_val, batch_size=batch_size, shuffle=False, collate_fn=collate_batch, num_workers=0)\n",
        "\n",
        "    # Training loop\n",
        "    best_val_loss = float('inf')\n",
        "    train_losses = []\n",
        "    val_losses = []\n",
        "\n",
        "    for epoch in range(train_epochs):\n",
        "        # Training\n",
        "        model.train()\n",
        "        total_loss = 0.0\n",
        "        n_batches = 0\n",
        "        for batch_idx, batch in enumerate(dl_train):\n",
        "            node_features = batch['node_features'].to(device)\n",
        "            adjacency_matrices = batch['adjacency_matrices'].to(device)\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            # compute KL annealing beta (linear warmup)\n",
        "            kl_beta = float(min(1.0, (epoch + 1) / max(1.0, train_epochs * 0.5)))  # warm up over first 50% epochs\n",
        "            model.kl_beta = kl_beta\n",
        "\n",
        "            outputs, latent_sequence = model(node_features, adjacency_matrices, rollout_steps=2, teacher_forcing_prob=0.9)\n",
        "            # model.forward may return (preds, latent_seq, recon)\n",
        "            if isinstance(outputs, tuple) and len(outputs) == 3:\n",
        "                preds, latent_sequence, recon = outputs\n",
        "                loss, sublogs = compute_losses(\n",
        "                    batch,\n",
        "                    preds,\n",
        "                    latent_sequence=latent_sequence,\n",
        "                    lambda_edge=1.0,\n",
        "                    lambda_clinical=0.5,\n",
        "                    lambda_latent=0.1,\n",
        "                    kl_beta=getattr(model, 'kl_beta', 1.0)\n",
        "                )\n",
        "            else:\n",
        "                loss, sublogs = compute_losses(\n",
        "                    batch,\n",
        "                    outputs,\n",
        "                    latent_sequence=latent_sequence,\n",
        "                    lambda_edge=1.0,\n",
        "                    lambda_clinical=0.5,\n",
        "                    lambda_latent=0.1,\n",
        "                    kl_beta=getattr(model, 'kl_beta', 1.0)\n",
        "                )\n",
        "\n",
        "            loss.backward()\n",
        "            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
        "            optimizer.step()\n",
        "\n",
        "            total_loss += loss.item()\n",
        "            n_batches += 1\n",
        "\n",
        "        avg_train = total_loss / n_batches\n",
        "        train_losses.append(avg_train)\n",
        "\n",
        "        # Validation\n",
        "        if validate:\n",
        "            model.eval()\n",
        "            total_val_loss = 0.0\n",
        "            n_val = 0\n",
        "            with torch.no_grad():\n",
        "                for batch in dl_val:\n",
        "                    # Move batch to device\n",
        "                    batch = {k: v.to(device) if isinstance(v, torch.Tensor) else v for k, v in batch.items()}\n",
        "\n",
        "                    outputs, latent_sequence = model(\n",
        "                        batch['node_features'],\n",
        "                        batch['adjacency_matrices'],\n",
        "                        rollout_steps=2,\n",
        "                        teacher_forcing_prob=1.0\n",
        "                    )\n",
        "                    loss, sublogs = compute_losses(\n",
        "                        batch,\n",
        "                        outputs,\n",
        "                        latent_sequence=latent_sequence,\n",
        "                        lambda_edge=1.0,\n",
        "                        lambda_clinical=0.5,\n",
        "                        lambda_latent=0.1\n",
        "                    )\n",
        "                    total_val_loss += loss.item()\n",
        "                    n_val += 1\n",
        "\n",
        "            avg_val = total_val_loss / n_val\n",
        "            val_losses.append(avg_val)\n",
        "\n",
        "            # Early stopping\n",
        "            # use module-global counter to avoid assigning attributes to the function object\n",
        "            global _quick_demo_patience_counter\n",
        "            if avg_val < best_val_loss:\n",
        "                best_val_loss = avg_val\n",
        "                _quick_demo_patience_counter = 0\n",
        "            else:\n",
        "                _quick_demo_patience_counter = _quick_demo_patience_counter + 1\n",
        "                if _quick_demo_patience_counter >= 5:\n",
        "                    print(f'\\nEarly stopping at epoch {epoch}')\n",
        "                    break\n",
        "\n",
        "            print(f'Epoch {epoch:2d}: train_loss={avg_train:.4f} | val_loss={avg_val:.4f}')\n",
        "        else:\n",
        "            print(f'Epoch {epoch:2d}: train_loss={avg_train:.4f}')\n",
        "\n",
        "        scheduler.step()\n",
        "\n",
        "    print('='*80)\n",
        "    print('Training complete!\\n')\n",
        "\n",
        "    # Comprehensive evaluation\n",
        "    print('Running comprehensive evaluation...')\n",
        "    print('='*80)\n",
        "\n",
        "    model.eval()\n",
        "\n",
        "    # Get a sample for detailed analysis\n",
        "    sample_idx = 3\n",
        "    sample = ds_val[sample_idx]\n",
        "    node_features = torch.tensor(sample['node_features']).unsqueeze(0).to(device)\n",
        "    adjacency_matrices = torch.tensor(sample['adjacency_matrices']).unsqueeze(0).to(device)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        outputs, latent_sequence = model(node_features, adjacency_matrices, rollout_steps=5, teacher_forcing_prob=1.0)\n",
        "        predicted_adjacencies = [out['predicted_adjacency'].cpu().numpy()[0] for out in outputs]\n",
        "        attentions = [out['attention_weights'].cpu().numpy()[0] for out in outputs]\n",
        "        latent_vals = outputs[0]['latent_state'].cpu().numpy()[0]  # (latent_dim,)\n",
        "\n",
        "    true_adjacency_sequence = [sample['adjacency_matrices'][t] for t in range(sample['adjacency_matrices'].shape[0])]\n",
        "\n",
        "    # Edge interpretability\n",
        "    print('\\nEdge-Level Interpretability:')\n",
        "    print('-'*80)\n",
        "    edge_imp, top_edges = compute_edge_importance(true_adjacency_sequence)\n",
        "    print(f'  Top changing edges (i,j): {top_edges[:8]}')\n",
        "    print(f'  Edge importance range: [{edge_imp.min():.4f}, {edge_imp.max():.4f}]')\n",
        "\n",
        "    # Node degeneration analysis\n",
        "    print('\\nNode-Level Degeneration:')\n",
        "    print('-'*80)\n",
        "    node_str, degen_rate = compute_node_degeneration_rate(true_adjacency_sequence)\n",
        "    top_degen = np.argsort(-degen_rate)[:5]\n",
        "    print(f'  Top degenerating regions: {top_degen.tolist()}')\n",
        "    print(f'  Degeneration rates: {degen_rate[top_degen]}')\n",
        "    print(f'  Mean strength trajectory: {node_str.mean(axis=0)}')\n",
        "\n",
        "    # Attention flow analysis\n",
        "    print('\\nAttention Flow Analysis:')\n",
        "    print('-'*80)\n",
        "    if attentions:\n",
        "        info_flow = attention_flow_analysis(attentions, None)\n",
        "        if info_flow is not None:\n",
        "            top_info = np.argsort(-info_flow)[:5]\n",
        "            print(f'  Top information-receiving regions: {top_info.tolist()}')\n",
        "            print(f'  Information flow: {info_flow[top_info]}')\n",
        "\n",
        "    # Forecast stability\n",
        "    print('\\nForecast Stability:')\n",
        "    print('-'*80)\n",
        "    with torch.no_grad():\n",
        "        outputs_1 = model(node_features, adjacency_matrices, rollout_steps=1, teacher_forcing_prob=1.0)[0]\n",
        "        outputs_3 = model(node_features, adjacency_matrices, rollout_steps=3, teacher_forcing_prob=1.0)[0]\n",
        "        E_1 = [outputs_1[0]['predicted_adjacency'].cpu().numpy()[0]]\n",
        "        E_3 = [out['predicted_adjacency'].cpu().numpy()[0] for out in outputs_3[:1]]\n",
        "        stability = compute_forecast_stability([E_1, E_3])\n",
        "        print(f'  Forecast divergence: {stability:.4f}')\n",
        "        print(f'  (Lower = more stable predictions)')\n",
        "\n",
        "    # Latent space analysis\n",
        "    print('\\nLatent Space:')\n",
        "    print('-'*80)\n",
        "    print(f'  z vector norm: {np.linalg.norm(latent_vals):.4f}')\n",
        "    print(f'  z components: {latent_vals[:5]}...')  # first 5 components\n",
        "\n",
        "    # Clinical prediction\n",
        "    print('\\nClinical Prediction:')\n",
        "    print('-'*80)\n",
        "    predicted_score_sample = outputs[0]['predicted_score'].item()\n",
        "    true_score_sample = sample['cognitive_score']\n",
        "    print(f'  Predicted Score: {predicted_score_sample:.2f}')\n",
        "    print(f'  True Score: {true_score_sample:.2f}')\n",
        "    print(f'  Prediction error: {abs(predicted_score_sample - true_score_sample):.2f}')\n",
        "\n",
        "    # Edge trajectory plotting\n",
        "    print('\\nEdge Trajectory Analysis:')\n",
        "    print('-'*80)\n",
        "    try:\n",
        "        if not _HAS_MATPLOTLIB:\n",
        "            print('  Skipping plotting: matplotlib not available')\n",
        "        else:\n",
        "            # Import matplotlib locally to avoid static analyzers complaining about global `plt` possibly being None\n",
        "            import importlib as _local_importlib\n",
        "            plt_local = _local_importlib.import_module('matplotlib.pyplot')\n",
        "            fig, axes = plt_local.subplots(2, 2, figsize=(12, 8))\n",
        "\n",
        "            # Plot 1: Top edge trajectories\n",
        "            ax = axes[0, 0]\n",
        "            for (i, j) in top_edges[:3]:\n",
        "                true_vals = [true_adjacency_sequence[t][i, j] for t in range(len(true_adjacency_sequence))]\n",
        "                pred_vals = [predicted_adjacencies[t][i, j] for t in range(len(predicted_adjacencies))]\n",
        "                xs_true = np.arange(len(true_vals))\n",
        "                xs_pred = np.arange(len(true_adjacency_sequence)-1, len(true_adjacency_sequence)-1 + len(pred_vals))\n",
        "                ax.plot(xs_true, true_vals, '--o', label=f'true {i}-{j}', alpha=0.7)\n",
        "                ax.plot(xs_pred, pred_vals, '-x', label=f'pred {i}-{j}', alpha=0.7)\n",
        "            ax.set_xlabel('Time'); ax.set_ylabel('Edge weight')\n",
        "            ax.set_title('Top-3 Edge Trajectories')\n",
        "            ax.legend(fontsize=8); ax.grid(True, alpha=0.3)\n",
        "\n",
        "            # Plot 2: Node strength over time\n",
        "            ax = axes[0, 1]\n",
        "            for r in top_degen[:3]:\n",
        "                ax.plot(node_str[r, :], '-o', label=f'Region {r}')\n",
        "            ax.set_xlabel('Time'); ax.set_ylabel('Node strength')\n",
        "            ax.set_title('Top-3 Degenerating Regions')\n",
        "            ax.legend(); ax.grid(True, alpha=0.3)\n",
        "\n",
        "            # Plot 3: Loss curves\n",
        "            ax = axes[1, 0]\n",
        "            ax.plot(train_losses, '-o', label='train', linewidth=2)\n",
        "            if val_losses:\n",
        "                ax.plot(val_losses, '-s', label='validation', linewidth=2)\n",
        "            ax.set_xlabel('Epoch'); ax.set_ylabel('Loss')\n",
        "            ax.set_title('Training Curves')\n",
        "            ax.legend(); ax.grid(True, alpha=0.3)\n",
        "\n",
        "            # Plot 4: Adjacency matrix heatmap\n",
        "            ax = axes[1, 1]\n",
        "            E_final_pred = predicted_adjacencies[-1] if predicted_adjacencies else true_adjacency_sequence[-1]\n",
        "            im = ax.imshow(E_final_pred, cmap='viridis')\n",
        "            ax.set_title('Final Predicted Adjacency')\n",
        "            try:\n",
        "                plt_local.colorbar(im, ax=ax)\n",
        "            except Exception:\n",
        "                pass\n",
        "\n",
        "            plt_local.tight_layout()\n",
        "            plt_local.savefig('/tmp/ddg_analysis.png', dpi=100, bbox_inches='tight')\n",
        "            print('  Saved analysis plots to /tmp/ddg_analysis.png')\n",
        "            plt_local.close()\n",
        "    except Exception as e:\n",
        "        print(f'  Plotting failed: {e}')\n",
        "\n",
        "    # Animation\n",
        "    print('\\nAnimation Generation:')\n",
        "    print('-'*80)\n",
        "    try:\n",
        "        full_seq = true_adjacency_sequence + predicted_adjacencies\n",
        "        html = animate_adjacency_sequence(full_seq, title='Observed + Predicted Adj', save_path='/tmp/ddg_evolution.mp4')\n",
        "        print('  Animation saved to /tmp/ddg_evolution.mp4')\n",
        "    except Exception as e:\n",
        "        print(f'  Animation failed: {e}')\n",
        "\n",
        "    print('='*80)\n",
        "    print('Evaluation complete!')\n",
        "    print('='*80)\n",
        "\n",
        "    return model, (train_losses, val_losses)\n",
        "\n",
        "\n",
        "def demo_multimodal_synthetic(device=None):\n",
        "    \"\"\"Small demo that builds synthetic timeseries, computes multimodal components,\n",
        "    and runs a forward pass with learnable multimodal weights and importance heads.\n",
        "    \"\"\"\n",
        "    device = device or (torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu'))\n",
        "    B = 4\n",
        "    T = 6\n",
        "    N = 12\n",
        "    F = 4\n",
        "\n",
        "    # synthetic timeseries per sample (B, T, N)\n",
        "    timeseries_batch = np.random.randn(B, T, N).astype(np.float32) * 0.1\n",
        "    node_feats_batch = np.stack([node_features_from_timeseries(timeseries_batch[b]) for b in range(B)], axis=0)\n",
        "\n",
        "    # compute base adjacency components and combined empirical adjacency\n",
        "    adj_R = np.zeros((B, T, N, N), dtype=np.float32)\n",
        "    adj_MI = np.zeros_like(adj_R)\n",
        "    adj_coh = np.zeros_like(adj_R)\n",
        "    adj_comb = np.zeros_like(adj_R)\n",
        "    for b in range(B):\n",
        "        for t in range(T):\n",
        "            # use a short window around t\n",
        "            s = max(0, t - 2)\n",
        "            e = min(T, t + 1)\n",
        "            win = timeseries_batch[b, s:e]\n",
        "            R = compute_empirical_fc_from_timeseries(win)\n",
        "            I = compute_mutual_info_matrix(win)\n",
        "            C = compute_coherence_matrix(win)\n",
        "            adj_R[b, t] = R\n",
        "            adj_MI[b, t] = I\n",
        "            adj_coh[b, t] = C\n",
        "            adj_comb[b, t] = compute_multimodal_adjacency(win)\n",
        "\n",
        "    node_feats = torch.tensor(node_feats_batch).to(device)\n",
        "    adj_tensor = torch.tensor(adj_comb).to(device)\n",
        "    adj_components = {\n",
        "        'adj_R': torch.tensor(adj_R).to(device),\n",
        "        'adj_MI': torch.tensor(adj_MI).to(device),\n",
        "        'adj_coh': torch.tensor(adj_coh).to(device)\n",
        "    }\n",
        "\n",
        "    model = DDGModel(in_feats=F, node_dim=32, latent_dim=12, use_adaptive_edges=True, use_vae=True, use_vae_recon=True, recon_T=T).to(device)\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        outputs, latent_seq, recon = model(node_feats, adj_tensor, adj_components=adj_components, rollout_steps=2, teacher_forcing_prob=1.0)\n",
        "    print('Demo multimodal forward pass completed. Outputs keys:', list(outputs[0].keys()))\n",
        "    print('Recon shape:', recon.shape if recon is not None else None)\n",
        "    return model, outputs, recon\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def evaluate_models(models: Dict[str, nn.Module], dataloader: DataLoader, device=None) -> Dict[str, Dict[str, float]]:\n",
        "    \"\"\"Evaluate multiple models on a dataloader and return MSE metrics.\n",
        "\n",
        "    Args:\n",
        "        models: dict mapping name->nn.Module\n",
        "        dataloader: PyTorch DataLoader yielding batches compatible with models\n",
        "    \"\"\"\n",
        "    device = device or (torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu'))\n",
        "    results = {name: {'mse': 0.0, 'n': 0} for name in models.keys()}\n",
        "    for name, m in models.items():\n",
        "        m.to(device)\n",
        "        m.eval()\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for batch in dataloader:\n",
        "            # move tensors\n",
        "            batch = {k: v.to(device) if isinstance(v, torch.Tensor) else v for k, v in batch.items()}\n",
        "            X = batch['node_features']\n",
        "            A = batch['adjacency_matrices']\n",
        "            y = batch['cognitive_scores'].to(device)\n",
        "            for name, m in models.items():\n",
        "                try:\n",
        "                    if isinstance(m, BaselineMLPLSTM):\n",
        "                        y_hat = m(X)\n",
        "                    elif isinstance(m, StaticGNNBaseline):\n",
        "                        y_hat = m(X, A)\n",
        "                    else:\n",
        "                        # assume DDGModel-like: forward returns (preds, latent[, recon])\n",
        "                        out = m(X, A, rollout_steps=1, teacher_forcing_prob=1.0)\n",
        "                        if isinstance(out, tuple):\n",
        "                            preds = out[0]\n",
        "                        else:\n",
        "                            preds = out\n",
        "                        # take predicted_score\n",
        "                        y_hat = preds[0]['predicted_score'].squeeze(-1)\n",
        "                except Exception:\n",
        "                    # fallback: zero prediction\n",
        "                    y_hat = torch.zeros_like(y)\n",
        "\n",
        "                mse = F.mse_loss(y_hat, y, reduction='sum').item()\n",
        "                results[name]['mse'] += mse\n",
        "                results[name]['n'] += y.size(0)\n",
        "\n",
        "    # finalize\n",
        "    for name in results.keys():\n",
        "        if results[name]['n'] > 0:\n",
        "            results[name]['mse'] = results[name]['mse'] / results[name]['n']\n",
        "        else:\n",
        "            results[name]['mse'] = float('nan')\n",
        "    return results\n",
        "\n",
        "# ---------------------- PrecomputedConnectomeDataset (HDF5/NPZ) ----------------------\n",
        "\n",
        "# Note: h5py, csv, and Path already imported at top; just using them here\n",
        "class PrecomputedConnectomeDataset(Dataset):\n",
        "    \"\"\"\n",
        "    Dataset expecting one file per subject sequence OR a master CSV index.\n",
        "\n",
        "    Supported input options:\n",
        "    - Per-subject .npz with keys: 'adjacency_matrices' (T,N,N), 'node_features' (T,N,F), 'times' (T,), 'cognitive_score' float or (T,)\n",
        "    - Per-subject HDF5 with same dataset names (requires h5py).\n",
        "    - Master CSV where each row points to a file and optional metadata columns.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, paths_or_csv: str, root: Optional[str] = None, file_format: Optional[str] = None, transform=None):\n",
        "        self.transform = transform\n",
        "        self.root = Path(root) if root is not None else None\n",
        "        self.entries = []\n",
        "\n",
        "        if isinstance(paths_or_csv, (list, tuple)):\n",
        "            for p in paths_or_csv:\n",
        "                self.entries.append({'path': str(p)})\n",
        "        else:\n",
        "            p = Path(paths_or_csv)\n",
        "            if p.suffix.lower() == '.csv':\n",
        "                if not p.exists():\n",
        "                    raise FileNotFoundError(f'CSV index not found: {p}')\n",
        "                with open(p, 'r') as fh:\n",
        "                    reader = csv.DictReader(fh)\n",
        "                    for row in reader:\n",
        "                        rowpath = row.get('path')\n",
        "                        if rowpath is None:\n",
        "                            raise ValueError('CSV must have a \"path\" column')\n",
        "                        if self.root is not None and not Path(rowpath).is_absolute():\n",
        "                            rowpath = str(self.root / rowpath)\n",
        "                        row['path'] = rowpath\n",
        "                        self.entries.append(row)\n",
        "            else:\n",
        "                raise ValueError('paths_or_csv must be list or path to CSV index')\n",
        "\n",
        "    def __len__(self) -> int:\n",
        "        return len(self.entries)\n",
        "\n",
        "    def _load_file(self, path: str) -> Tuple[np.ndarray, np.ndarray, np.ndarray, Optional[float], Dict]:\n",
        "        p = Path(path)\n",
        "        if not p.exists():\n",
        "            raise FileNotFoundError(f'Data file not found: {path}')\n",
        "\n",
        "        if p.suffix == '.npz':\n",
        "            d = np.load(p, allow_pickle=True)\n",
        "            # Support legacy keys X_seq/E_seq or new keys\n",
        "            if 'adjacency_matrices' in d.files:\n",
        "                adjacency_matrices = d['adjacency_matrices']\n",
        "                node_features = d['node_features']\n",
        "            elif 'E_seq' in d.files:\n",
        "                adjacency_matrices = d['E_seq']\n",
        "                node_features = d['X_seq']\n",
        "            else:\n",
        "                raise ValueError(f'NPZ file missing required keys: {path}')\n",
        "\n",
        "            times = d['times'] if 'times' in d.files else np.arange(len(adjacency_matrices))\n",
        "\n",
        "            cognitive_score = None\n",
        "            if 'cognitive_score' in d.files:\n",
        "                cognitive_score = d['cognitive_score']\n",
        "            elif 'mmse' in d.files:\n",
        "                cognitive_score = d['mmse']\n",
        "            # coerce scalar arrays to Python float for downstream typing\n",
        "            if isinstance(cognitive_score, np.ndarray) and cognitive_score.size == 1:\n",
        "                try:\n",
        "                    cognitive_score = float(cognitive_score.item())\n",
        "                except Exception:\n",
        "                    pass\n",
        "\n",
        "            meta = dict()\n",
        "            if 'meta' in d.files:\n",
        "                try:\n",
        "                    meta = d['meta'].item()\n",
        "                except Exception:\n",
        "                    meta = dict()\n",
        "            return node_features.astype(np.float32), adjacency_matrices.astype(np.float32), times, cognitive_score, meta\n",
        "\n",
        "        elif p.suffix in ('.h5', '.hdf5'):\n",
        "            # import h5py lazily to avoid static analyzer complaints and make dependency optional\n",
        "            try:\n",
        "                import importlib as _il\n",
        "                h5py_local = _il.import_module('h5py')\n",
        "            except Exception:\n",
        "                raise RuntimeError('h5py not installed. Install with: pip install h5py')\n",
        "            with h5py_local.File(p, 'r') as fh:\n",
        "                if 'adjacency_matrices' in fh:\n",
        "                    adjacency_matrices = np.array(fh['adjacency_matrices'])\n",
        "                    node_features = np.array(fh['node_features'])\n",
        "                elif 'E_seq' in fh:\n",
        "                    adjacency_matrices = np.array(fh['E_seq'])\n",
        "                    node_features = np.array(fh['X_seq'])\n",
        "                else:\n",
        "                    raise ValueError(f'HDF5 file missing required keys: {path}')\n",
        "\n",
        "                times = np.array(fh['times']) if 'times' in fh else np.arange(len(adjacency_matrices))\n",
        "\n",
        "                cognitive_score = None\n",
        "                if 'cognitive_score' in fh:\n",
        "                    cognitive_score = np.array(fh['cognitive_score'])\n",
        "                elif 'mmse' in fh:\n",
        "                    cognitive_score = np.array(fh['mmse'])\n",
        "                if isinstance(cognitive_score, np.ndarray) and cognitive_score.size == 1:\n",
        "                    try:\n",
        "                        cognitive_score = float(cognitive_score.item())\n",
        "                    except Exception:\n",
        "                        pass\n",
        "\n",
        "                meta = dict()\n",
        "                if 'meta' in fh:\n",
        "                    try:\n",
        "                        meta = json.loads(fh['meta'][()])\n",
        "                    except Exception:\n",
        "                        meta = dict()\n",
        "                return node_features.astype(np.float32), adjacency_matrices.astype(np.float32), times, cognitive_score, meta\n",
        "        else:\n",
        "            raise ValueError('Unsupported file type: ' + p.suffix)\n",
        "\n",
        "    def __getitem__(self, idx: int) -> Dict[str, Any]:\n",
        "        entry = self.entries[idx]\n",
        "        path = entry['path']\n",
        "        node_features, adjacency_matrices, times, score, meta = self._load_file(path)\n",
        "\n",
        "        out = {\n",
        "            'node_features': torch.tensor(node_features, dtype=torch.float32),\n",
        "            'adjacency_matrices': torch.tensor(adjacency_matrices, dtype=torch.float32),\n",
        "            'times': torch.tensor(times, dtype=torch.float32),\n",
        "            'cognitive_scores': torch.tensor(float(score)) if score is not None else torch.tensor(float('nan')),\n",
        "            'metadata': meta\n",
        "        }\n",
        "        if self.transform is not None:\n",
        "            out = self.transform(out)\n",
        "        return out\n",
        "\n",
        "# ---------------------- Augmentations for Temporal Contrastive Learning ----------------------\n",
        "\n",
        "# Augmentations should be label-preserving but diverse. We provide safe defaults\n",
        "# tailored to connectome data: edge perturbation, node feature masking, gaussian noise.\n",
        "\n",
        "def augment_edge_perturbation(adjacency_matrix: np.ndarray, drop_rate: float = 0.08, noise_scale: float = 0.01) -> np.ndarray:\n",
        "    \"\"\"Randomly drops edges and adds noise to adjacency matrix.\"\"\"\n",
        "    adj = adjacency_matrix.copy()\n",
        "    num_nodes = adj.shape[0]\n",
        "\n",
        "    # Drop edges\n",
        "    mask = (np.random.rand(num_nodes, num_nodes) > drop_rate).astype(float)\n",
        "    mask = 0.5 * (mask + mask.T)  # Keep symmetry\n",
        "    adj = adj * mask\n",
        "\n",
        "    # Add noise\n",
        "    adj = adj + noise_scale * np.random.randn(num_nodes, num_nodes)\n",
        "    adj = 0.5 * (adj + adj.T)\n",
        "    adj = np.clip(adj, 0.0, None)\n",
        "\n",
        "    if adj.max() > 0:\n",
        "        adj = adj / (adj.max() + 1e-6)\n",
        "    return adj\n",
        "\n",
        "def augment_node_mask(node_features: np.ndarray, mask_prob: float = 0.1) -> np.ndarray:\n",
        "    \"\"\"Randomly masks node features.\"\"\"\n",
        "    features = node_features.copy()\n",
        "    num_nodes, num_features = features.shape\n",
        "    mask = (np.random.rand(num_nodes, num_features) > mask_prob).astype(float)\n",
        "    features = features * mask\n",
        "    return features\n",
        "\n",
        "def augment_gaussian_noise(node_features: np.ndarray, scale: float = 0.02) -> np.ndarray:\n",
        "    # Adds Gaussian noise to node features.\n",
        "    return node_features + scale * np.random.randn(*node_features.shape)\n",
        "\n",
        "def random_augment(sample: Dict[str, Any], edge_drop: float = 0.08, node_mask: float = 0.1, noise_scale: float = 0.02) -> Tuple[np.ndarray, np.ndarray]:\n",
        "    # Applies random augmentations to a sample for contrastive learning.\n",
        "    node_features = sample['node_features']\n",
        "    adjacency_matrices = sample['adjacency_matrices']\n",
        "\n",
        "    # Choose time index randomly within the subject sequence\n",
        "    num_timepoints = node_features.shape[0]\n",
        "    t = np.random.randint(0, num_timepoints)\n",
        "\n",
        "    features_t = node_features[t]\n",
        "    adjacency_t = adjacency_matrices[t]\n",
        "\n",
        "    features_aug = augment_node_mask(features_t, mask_prob=node_mask)\n",
        "    features_aug = augment_gaussian_noise(features_aug, scale=noise_scale)\n",
        "\n",
        "    adjacency_aug = augment_edge_perturbation(adjacency_t, drop_rate=edge_drop, noise_scale=noise_scale)\n",
        "\n",
        "    return features_aug, adjacency_aug\n",
        "\n",
        "# ---------------------- NT-Xent loss (vectorized, numerically stable) ----------------------\n",
        "\n",
        "class NTXentLoss(nn.Module):\n",
        "    # Normalized Temperature-scaled Cross Entropy Loss (SimCLR-style).\n",
        "    # Expects two batches of projections z_i, z_j of shape (B, D).\n",
        "    def __init__(self, temperature=0.1, eps=1e-8):\n",
        "        super().__init__()\n",
        "        self.temperature = temperature\n",
        "        self.eps = eps\n",
        "\n",
        "    def forward(self, z1, z2):\n",
        "        # z1, z2: (B, D)\n",
        "        device = z1.device\n",
        "        B = z1.shape[0]\n",
        "        z = torch.cat([z1, z2], dim=0)  # 2B x D\n",
        "        z = F.normalize(z, dim=1)\n",
        "        sim = torch.matmul(z, z.t()) / self.temperature  # 2B x 2B\n",
        "        # For numerical stability, subtract max on each row\n",
        "        sim_max, _ = torch.max(sim, dim=1, keepdim=True)\n",
        "        sim = sim - sim_max.detach()\n",
        "        exp_sim = torch.exp(sim)\n",
        "        # mask to remove self-similarity\n",
        "        mask = (~torch.eye(2*B, dtype=torch.bool, device=device)).float()\n",
        "        exp_sim = exp_sim * mask\n",
        "        # positive similarities: i with i+B and i+B with i (correctly aligned)\n",
        "        z1_norm = F.normalize(z1, dim=1)\n",
        "        z2_norm = F.normalize(z2, dim=1)\n",
        "        pos_sim_12 = torch.sum(z1_norm * z2_norm, dim=-1) / self.temperature  # (B,)\n",
        "        pos_sim_21 = torch.sum(z2_norm * z1_norm, dim=-1) / self.temperature  # (B,)\n",
        "        pos = torch.cat([torch.exp(pos_sim_12), torch.exp(pos_sim_21)], dim=0)  # (2B,)\n",
        "        # denominator: sum over row excluding self\n",
        "        denom = exp_sim.sum(dim=1)\n",
        "        loss = -torch.log(pos / (denom + self.eps))\n",
        "        return loss.mean()\n",
        "\n",
        "# ---------------------- Projection head & Pretraining Encoder wrapper ----------------------\n",
        "\n",
        "class ProjectionHead(nn.Module):\n",
        "    def __init__(self, in_dim, proj_dim=64, hidden=128):\n",
        "        super().__init__()\n",
        "        self.net = nn.Sequential(\n",
        "            nn.Linear(in_dim, hidden),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(hidden, proj_dim)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.net(x)\n",
        "\n",
        "# Pretrain wrapper takes GraphEncoder (node-level) and pools into graph-level\n",
        "class PretrainEncoder(nn.Module):\n",
        "    # Wrapper for GraphEncoder to pool node embeddings into graph embeddings.\n",
        "\n",
        "    def __init__(self, graph_encoder: GraphEncoder, pool: str = 'mean'):\n",
        "        super().__init__()\n",
        "        self.encoder = graph_encoder\n",
        "        self.pool = pool\n",
        "\n",
        "    def forward(self, node_features: torch.Tensor, adjacency_matrix: torch.Tensor) -> torch.Tensor:\n",
        "        # Handle single instance vs batch\n",
        "        is_single = False\n",
        "        if node_features.dim() == 2:\n",
        "            node_features = node_features.unsqueeze(0)\n",
        "            adjacency_matrix = adjacency_matrix.unsqueeze(0)\n",
        "            is_single = True\n",
        "        elif node_features.dim() == 3 and adjacency_matrix.dim() == 2:\n",
        "            # Batch of features but single adjacency; broadcast adjacency\n",
        "            adjacency_matrix = adjacency_matrix.unsqueeze(0).expand(node_features.shape[0], -1, -1)\n",
        "\n",
        "        node_embeddings = self.encoder(node_features, adjacency_matrix)  # (B, N, D)\n",
        "\n",
        "        if self.pool == 'mean':\n",
        "            graph_embedding = node_embeddings.mean(dim=1)  # (B, D)\n",
        "        else:\n",
        "            graph_embedding = node_embeddings.max(dim=1)[0]\n",
        "\n",
        "        if is_single:\n",
        "            return graph_embedding.squeeze(0)\n",
        "        return graph_embedding\n",
        "\n",
        "# ---------------------- Lightning pretraining module ----------------------\n",
        "\n",
        "if _HAS_PL:\n",
        "    class ContrastivePretrainModule(pl.LightningModule):\n",
        "        def __init__(self, graph_encoder: GraphEncoder, proj_dim=64, lr=1e-3, weight_decay=1e-6, temperature=0.1):\n",
        "            super().__init__()\n",
        "            self.save_hyperparameters(ignore=['graph_encoder'])\n",
        "            self.encoder = PretrainEncoder(graph_encoder)\n",
        "            self.proj = ProjectionHead(self.encoder.encoder.node_mlp[-1].out_features, proj_dim=proj_dim)\n",
        "            self.loss_fn = NTXentLoss(temperature=temperature)\n",
        "            self.lr = lr\n",
        "            self.weight_decay = weight_decay\n",
        "\n",
        "        def training_step(self, batch, batch_idx):\n",
        "            # Batch contains sequences; sample two augmented views per sequence\n",
        "            batch_size = batch['node_features'].shape[0]\n",
        "            z1_list = []\n",
        "            z2_list = []\n",
        "\n",
        "            for i in range(batch_size):\n",
        "                features_i = batch['node_features'][i].cpu().numpy()\n",
        "                adjacency_i = batch['adjacency_matrices'][i].cpu().numpy()\n",
        "\n",
        "                v1_features, v1_adj = random_augment({'node_features': features_i, 'adjacency_matrices': adjacency_i})\n",
        "                v2_features, v2_adj = random_augment({'node_features': features_i, 'adjacency_matrices': adjacency_i})\n",
        "\n",
        "                # Encode pooled graph embeddings\n",
        "                g1 = self.encoder(\n",
        "                    torch.tensor(v1_features, dtype=torch.float32).to(self.device),\n",
        "                    torch.tensor(v1_adj, dtype=torch.float32).to(self.device)\n",
        "                )\n",
        "                g2 = self.encoder(\n",
        "                    torch.tensor(v2_features, dtype=torch.float32).to(self.device),\n",
        "                    torch.tensor(v2_adj, dtype=torch.float32).to(self.device)\n",
        "                )\n",
        "                z1_list.append(g1)\n",
        "                z2_list.append(g2)\n",
        "\n",
        "            z1 = torch.stack(z1_list, dim=0)\n",
        "            z2 = torch.stack(z2_list, dim=0)\n",
        "\n",
        "            p1 = self.proj(z1)\n",
        "            p2 = self.proj(z2)\n",
        "\n",
        "            loss = self.loss_fn(p1, p2)\n",
        "            self.log('pretrain/loss', loss, prog_bar=True, on_step=True, on_epoch=True)\n",
        "            return loss\n",
        "\n",
        "        def configure_optimizers(self):\n",
        "            opt = torch.optim.Adam(self.parameters(), lr=self.lr, weight_decay=self.weight_decay)\n",
        "            return opt\n",
        "\n",
        "# ---------------------- Pretraining & Finetuning utility functions ----------------------\n",
        "\n",
        "def save_pretrained_encoder(encoder: GraphEncoder, proj: ProjectionHead, path: str):\n",
        "    dirn = os.path.dirname(path)\n",
        "    if dirn:\n",
        "        os.makedirs(dirn, exist_ok=True)\n",
        "    torch.save({'encoder_state': encoder.state_dict(), 'proj_state': proj.state_dict()}, path)\n",
        "\n",
        "def load_pretrained_encoder(encoder: GraphEncoder, proj: ProjectionHead, path: str, device: 'cpu') -> Tuple[GraphEncoder, ProjectionHead]:\n",
        "    if not os.path.exists(path):\n",
        "        raise FileNotFoundError(f'Pretrained checkpoint not found: {path}')\n",
        "    ckpt = torch.load(path, map_location=device)\n",
        "    if 'encoder_state' not in ckpt or 'proj_state' not in ckpt:\n",
        "        raise RuntimeError('Pretrained checkpoint missing encoder_state or proj_state')\n",
        "    encoder.load_state_dict(ckpt['encoder_state'])\n",
        "    proj.load_state_dict(ckpt['proj_state'])\n",
        "    return encoder, proj\n",
        "\n",
        "# Finetune helper: load pretrained encoder weights into DDGModel.encoder\n",
        "\n",
        "def inject_pretrained_into_ddg(ddg_model: DDGModel, pretrained_path: str, device: Optional[Union[str, torch.device]] = None):\n",
        "    \"\"\"Load pretrained encoder state dict into a DDGModel; `device` may be a string or torch.device or None.\"\"\"\n",
        "    if not os.path.exists(pretrained_path):\n",
        "        raise FileNotFoundError(f'Pretrained checkpoint not found: {pretrained_path}')\n",
        "    map_location = device if device is not None else 'cpu'\n",
        "    sd = torch.load(pretrained_path, map_location=map_location)\n",
        "    # we expect keys under 'encoder_state'\n",
        "    enc_state = sd.get('encoder_state', None)\n",
        "    if enc_state is None:\n",
        "        raise RuntimeError('Pretrained checkpoint missing encoder_state')\n",
        "    # load selectively (strict=False to allow shape mismatches)\n",
        "    try:\n",
        "        ddg_model.encoder.load_state_dict(enc_state, strict=False)\n",
        "    except Exception as e:\n",
        "        print(f'Warning: partial load of encoder weights due to mismatch: {e}')\n",
        "    return ddg_model\n",
        "\n",
        "# ---------------------- Finetune pipeline cells (script-friendly) ----------------------\n",
        "\n",
        "def pretrain_contrastive_main(index_csv, save_path, epochs=200, batch_size=16, gpus=0):\n",
        "    # index_csv: CSV with 'path' column pointing to per-subject .npz or .h5\n",
        "    ds = PrecomputedConnectomeDataset(index_csv)\n",
        "    dl = DataLoader(ds, batch_size=batch_size, shuffle=True, collate_fn=collate_batch)\n",
        "    # instantiate encoder\n",
        "    graph_enc = GraphEncoder(in_features=3, hidden_dim=32)\n",
        "    module = ContrastivePretrainModule(graph_enc, proj_dim=64)\n",
        "    trainer = pl.Trainer(gpus=1 if gpus>0 else 0, max_epochs=epochs)\n",
        "    trainer.fit(module, dl)\n",
        "    # save encoder + projector\n",
        "    save_pretrained_encoder(module.encoder.encoder, module.proj, save_path)\n",
        "    print('Saved pretrained encoder to', save_path)\n",
        "\n",
        "\n",
        "def finetune_ddg_main(pretrained_path, train_csv, val_csv, ckpt_save, epochs=100, batch_size=8, gpus=0):\n",
        "    # load data\n",
        "    ds_train = PrecomputedConnectomeDataset(train_csv)\n",
        "    ds_val = PrecomputedConnectomeDataset(val_csv)\n",
        "    dl_train = DataLoader(ds_train, batch_size=batch_size, shuffle=True, collate_fn=collate_batch)\n",
        "    dl_val = DataLoader(ds_val, batch_size=batch_size, shuffle=False, collate_fn=collate_batch)\n",
        "    device = torch.device('cuda' if torch.cuda.is_available() and gpus>0 else 'cpu')\n",
        "    model = DDGModel(in_feats=3, node_dim=32, latent_dim=12, use_edge_gru=False, use_gde=False)\n",
        "    model = inject_pretrained_into_ddg(model, pretrained_path, device=device)\n",
        "    if _HAS_PL:\n",
        "        lit = DDGLightning(model)\n",
        "        trainer = pl.Trainer(gpus=1 if gpus>0 else 0, max_epochs=epochs)\n",
        "        trainer.fit(lit, dl_train, dl_val)\n",
        "        # save checkpoint\n",
        "        trainer.save_checkpoint(ckpt_save)\n",
        "    else:\n",
        "        # fallback: simple training loop\n",
        "        optimizer = torch.optim.Adam(model.parameters(), lr=1e-3, weight_decay=1e-5)\n",
        "        model.to(device)\n",
        "        for epoch in range(epochs):\n",
        "            model.train()\n",
        "            train_loss = 0.0\n",
        "            for batch in dl_train:\n",
        "                node_features = batch['node_features'].to(device)\n",
        "                adjacency_matrices = batch['adjacency_matrices'].to(device)\n",
        "                true_scores = batch['cognitive_scores'].to(device)\n",
        "\n",
        "                optimizer.zero_grad()\n",
        "                outputs, _ = model(node_features, adjacency_matrices, rollout_steps=1, teacher_forcing_prob=0.8)\n",
        "                loss, sublogs = compute_losses(\n",
        "                    batch,\n",
        "                    outputs\n",
        "                )\n",
        "                loss.backward()\n",
        "                optimizer.step()\n",
        "                train_loss += loss.item()\n",
        "            print(f'Epoch {epoch}, train_loss {train_loss/len(dl_train):.4f}')\n",
        "        # save state\n",
        "        torch.save(model.state_dict(), ckpt_save)\n",
        "        print('Saved finetuned ddg model to', ckpt_save)\n",
        "\n",
        "# ---------------------- Comprehensive Evaluation Metrics & Baselines ----------------------\n",
        "\n",
        "def evaluate_clinical_trajectory(model: DDGModel, dataloader, device='cpu', forecast_horizon=5):\n",
        "    \"\"\"Evaluate multi-step clinical outcome forecasting.\n",
        "\n",
        "    Args:\n",
        "        model: trained DDGModel\n",
        "        dataloader: validation dataloader\n",
        "        forecast_horizon: how many steps ahead to predict\n",
        "\n",
        "    Returns:\n",
        "        dict with MSE at different horizons, MAE, correlation\n",
        "    \"\"\"\n",
        "    if not _HAS_SCIPY:\n",
        "        print('Warning: scipy not available, skipping Spearman correlation')\n",
        "        return {}\n",
        "    from scipy.stats import spearmanr\n",
        "\n",
        "    model.to(device)\n",
        "    model.eval()\n",
        "    y_true_all = []\n",
        "    y_pred_all = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for batch in dataloader:\n",
        "            node_features = batch['node_features'].to(device)\n",
        "            adjacency_matrices = batch['adjacency_matrices'].to(device)\n",
        "            true_scores = batch['cognitive_scores'].to(device)\n",
        "\n",
        "            outputs, latent_sequence = model(node_features, adjacency_matrices, rollout_steps=min(forecast_horizon, 3), teacher_forcing_prob=1.0)\n",
        "\n",
        "            # Average predictions across rollout\n",
        "            predicted_score = outputs[0]['predicted_score'].squeeze(-1).cpu().numpy()\n",
        "            y_true_all.append(true_scores.cpu().numpy())\n",
        "            y_pred_all.append(predicted_score)\n",
        "\n",
        "    y_true = np.concatenate(y_true_all)\n",
        "    y_pred = np.concatenate(y_pred_all)\n",
        "\n",
        "    mse = ((y_true - y_pred)**2).mean()\n",
        "    mae = np.abs(y_true - y_pred).mean()\n",
        "    res = spearmanr(y_true, y_pred)\n",
        "    # Convert result to tuple and extract numeric entries robustly\n",
        "    try:\n",
        "        res_t = tuple(res)\n",
        "        stat = float(res_t[0])\n",
        "        pval = float(res_t[1])\n",
        "    except Exception:\n",
        "        stat = float('nan')\n",
        "        pval = float('nan')\n",
        "\n",
        "    return {\n",
        "        'mse': float(mse),\n",
        "        'mae': float(mae),\n",
        "        'spearman': stat,\n",
        "        'spearman_pval': pval,\n",
        "        'n_samples': len(y_true)\n",
        "    }\n",
        "\n",
        "def evaluate_edge_forecast_auroc(model: DDGModel, dataloader, device='cpu'):\n",
        "    # Evaluate edge prediction accuracy with ROC-AUC (edge degeneration detection).\n",
        "\n",
        "    # Returns:\n",
        "    #     dict with edge MSE, AUROC, AUPR, and edge prediction accuracy at thresholds\n",
        "    if not _HAS_SKLEARN:\n",
        "        print('Warning: sklearn not available, skipping AUROC/AUPR')\n",
        "        return {}\n",
        "    from sklearn.metrics import roc_auc_score, average_precision_score, roc_curve, precision_recall_curve\n",
        "\n",
        "    model.to(device); model.eval()\n",
        "    edge_mse = []\n",
        "    y_true_bin = []  # binary: edge weakened or not\n",
        "    y_score = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for batch in dataloader:\n",
        "            node_features = batch['node_features'].to(device)\n",
        "            adjacency_matrices = batch['adjacency_matrices'].to(device)\n",
        "\n",
        "            outputs, latent_sequence = model(node_features, adjacency_matrices, rollout_steps=1, teacher_forcing_prob=1.0)\n",
        "\n",
        "            predicted_adjacency = outputs[0]['predicted_adjacency']\n",
        "            true_next_adjacency = batch['adjacency_matrices'][:, -1].to(device)\n",
        "\n",
        "            # Use previous timepoint to determine if edge weakened\n",
        "            if batch['adjacency_matrices'].shape[1] >= 2:\n",
        "                prev_adjacency = batch['adjacency_matrices'][:, -2].to(device)\n",
        "            else:\n",
        "                prev_adjacency = true_next_adjacency\n",
        "\n",
        "            edge_mse.append(((predicted_adjacency - true_next_adjacency)**2).mean().item())\n",
        "\n",
        "            # Binary labels: did edge weaken?\n",
        "            weakened = (true_next_adjacency < (prev_adjacency * 0.8)).cpu().numpy().ravel()\n",
        "            y_true_bin.append(weakened)\n",
        "            y_score.append(predicted_adjacency.cpu().numpy().ravel())\n",
        "\n",
        "    edge_mse_val = float(np.mean(edge_mse))\n",
        "    y_true_bin = np.concatenate(y_true_bin)\n",
        "    y_score = np.concatenate(y_score)\n",
        "\n",
        "    results = {'edge_mse': edge_mse_val}\n",
        "    try:\n",
        "        auroc = roc_auc_score(y_true_bin, y_score)\n",
        "        aupr = average_precision_score(y_true_bin, y_score)\n",
        "        results['auroc'] = float(auroc)\n",
        "        results['aupr'] = float(aupr)\n",
        "    except Exception:\n",
        "        pass\n",
        "\n",
        "    return results\n",
        "\n",
        "\n",
        "def _compute_midrank(x: np.ndarray) -> np.ndarray:\n",
        "    # from Sun & Xu, adapted\n",
        "    J = np.argsort(x)\n",
        "    Z = x[J]\n",
        "    N = len(x)\n",
        "    T = np.zeros(N, dtype=float)\n",
        "    i = 0\n",
        "    while i < N:\n",
        "        j = i\n",
        "        while j + 1 < N and Z[j + 1] == Z[i]:\n",
        "            j += 1\n",
        "        T[i:j + 1] = 0.5 * (i + j) + 1\n",
        "        i = j + 1\n",
        "    T2 = np.empty(N, dtype=float)\n",
        "    T2[J] = T\n",
        "    return T2\n",
        "\n",
        "\n",
        "def _fast_delong(predictions_sorted_transposed: np.ndarray, label_1_count: int):\n",
        "    m = label_1_count\n",
        "    n = predictions_sorted_transposed.shape[1] - m\n",
        "    k = predictions_sorted_transposed.shape[0]\n",
        "    tx = np.zeros((k, m), dtype=float)\n",
        "    ty = np.zeros((k, n), dtype=float)\n",
        "    aucs = np.zeros(k, dtype=float)\n",
        "    for r in range(k):\n",
        "        x = predictions_sorted_transposed[r, :]\n",
        "        tx[r, :] = _compute_midrank(x[:m])\n",
        "        ty[r, :] = _compute_midrank(x[m:])\n",
        "        aucs[r] = (np.sum(tx[r, :]) - m * (m + 1) / 2.0) / (m * n)\n",
        "    v01 = np.var(np.sum(tx, axis=0) / m)\n",
        "    v10 = np.var(np.sum(ty, axis=0) / n)\n",
        "    return aucs, v01, v10\n",
        "\n",
        "\n",
        "def delong_roc_test(y_true: np.ndarray, y_scores_a: np.ndarray, y_scores_b: np.ndarray) -> Dict[str, float]:\n",
        "    \"\"\"Perform DeLong test for two correlated ROC AUCs and return p-value and AUCs.\n",
        "\n",
        "    Returns dict with keys: auc_a, auc_b, z, pvalue\n",
        "    \"\"\"\n",
        "    # Simple bootstrap-based test for AUC difference (robust fallback)\n",
        "    try:\n",
        "        from sklearn.metrics import roc_auc_score\n",
        "    except Exception:\n",
        "        raise RuntimeError('sklearn required for delong_roc_test')\n",
        "\n",
        "    y_true = np.asarray(y_true)\n",
        "    y_scores_a = np.asarray(y_scores_a)\n",
        "    y_scores_b = np.asarray(y_scores_b)\n",
        "\n",
        "    # compute observed AUCs\n",
        "    auc_a = float(roc_auc_score(y_true, y_scores_a))\n",
        "    auc_b = float(roc_auc_score(y_true, y_scores_b))\n",
        "\n",
        "    # bootstrap difference distribution\n",
        "    rng = np.random.RandomState(0)\n",
        "    n = len(y_true)\n",
        "    n_boot = 1000\n",
        "    diffs = []\n",
        "    for _ in range(n_boot):\n",
        "        idx = rng.randint(0, n, size=n)\n",
        "        try:\n",
        "            a = roc_auc_score(y_true[idx], y_scores_a[idx])\n",
        "            b = roc_auc_score(y_true[idx], y_scores_b[idx])\n",
        "            diffs.append(a - b)\n",
        "        except Exception:\n",
        "            continue\n",
        "    diffs = np.array(diffs)\n",
        "    if diffs.size < 2:\n",
        "        z = 0.0\n",
        "        pvalue = 1.0\n",
        "    else:\n",
        "        mean = diffs.mean()\n",
        "        std = diffs.std(ddof=1)\n",
        "        z = (auc_a - auc_b) / (std + 1e-12)\n",
        "        from scipy.stats import norm\n",
        "        pvalue = 2.0 * (1.0 - norm.cdf(abs(z)))\n",
        "\n",
        "    return {'auc_a': auc_a, 'auc_b': auc_b, 'z': float(z), 'pvalue': float(pvalue)}\n",
        "\n",
        "\n",
        "def bootstrap_auc_ci(y_true: np.ndarray, y_scores: np.ndarray, n_boot: int = 1000, alpha: float = 0.95, seed: Optional[int] = 0) -> Tuple[float, float]:\n",
        "    \"\"\"Bootstrap confidence interval for AUC.\n",
        "\n",
        "    Returns (low, high) CI bounds.\n",
        "    \"\"\"\n",
        "    try:\n",
        "        from sklearn.metrics import roc_auc_score\n",
        "    except Exception:\n",
        "        raise RuntimeError('sklearn required for bootstrap AUC CI')\n",
        "\n",
        "    rng = np.random.RandomState(seed)\n",
        "    y_true = np.asarray(y_true)\n",
        "    y_scores = np.asarray(y_scores)\n",
        "    n = len(y_true)\n",
        "    aucs = []\n",
        "    for _ in range(n_boot):\n",
        "        idx = rng.randint(0, n, size=n)\n",
        "        try:\n",
        "            auc = roc_auc_score(y_true[idx], y_scores[idx])\n",
        "        except Exception:\n",
        "            auc = np.nan\n",
        "        aucs.append(auc)\n",
        "    aucs = np.array(aucs)\n",
        "    aucs = aucs[~np.isnan(aucs)]\n",
        "    if aucs.size == 0:\n",
        "        return (float('nan'), float('nan'))\n",
        "    low = np.percentile(aucs, (1.0 - alpha) / 2.0 * 100)\n",
        "    high = np.percentile(aucs, (1.0 + alpha) / 2.0 * 100)\n",
        "    return float(low), float(high)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def run_cross_site_experiment(index_csv: str, model_kwargs: Optional[Dict[str, Any]] = None, train_kwargs: Optional[Dict[str, Any]] = None, device: Union[str, torch.device] = 'cpu') -> Dict[str, Any]:\n",
        "    \"\"\"Run a simple cross-site experiment: train on sites except one, test on held-out site.\n",
        "\n",
        "    Args:\n",
        "        index_csv: CSV index understood by PrecomputedConnectomeDataset\n",
        "        model_kwargs: kwargs for DDGModel\n",
        "        train_kwargs: training params (epochs, batch_size, lr)\n",
        "    Returns: results dictionary with metrics\n",
        "    \"\"\"\n",
        "    model_kwargs = model_kwargs or {}\n",
        "    train_kwargs = train_kwargs or {}\n",
        "    epochs = int(train_kwargs.get('epochs', 10))\n",
        "    batch_size = int(train_kwargs.get('batch_size', 8))\n",
        "    lr = float(train_kwargs.get('lr', 1e-3))\n",
        "\n",
        "    ds = PrecomputedConnectomeDataset(index_csv)\n",
        "    train_idx, test_idx = cross_site_split(ds)\n",
        "    from torch.utils.data import Subset\n",
        "    ds_train = Subset(ds, train_idx)\n",
        "    ds_test = Subset(ds, test_idx)\n",
        "\n",
        "    dl_train = DataLoader(ds_train, batch_size=batch_size, shuffle=True, collate_fn=collate_batch)\n",
        "    dl_test = DataLoader(ds_test, batch_size=batch_size, shuffle=False, collate_fn=collate_batch)\n",
        "\n",
        "    device = torch.device(device)\n",
        "    model = DDGModel(**model_kwargs).to(device)\n",
        "    opt = torch.optim.Adam(model.parameters(), lr=lr)\n",
        "\n",
        "    # simple training loop\n",
        "    for epoch in range(epochs):\n",
        "        model.train()\n",
        "        for batch in dl_train:\n",
        "            node_features = batch['node_features'].to(device)\n",
        "            adjacency_matrices = batch['adjacency_matrices'].to(device)\n",
        "            opt.zero_grad()\n",
        "            outputs, latent = model(node_features, adjacency_matrices, rollout_steps=1, teacher_forcing_prob=0.9)\n",
        "            loss, _ = compute_losses(batch, outputs, latent_sequence=latent)\n",
        "            loss.backward()\n",
        "            opt.step()\n",
        "\n",
        "    # Evaluation\n",
        "    model.eval()\n",
        "    y_true_all = []\n",
        "    y_pred_all = []\n",
        "    with torch.no_grad():\n",
        "        for batch in dl_test:\n",
        "            node_features = batch['node_features'].to(device)\n",
        "            adjacency_matrices = batch['adjacency_matrices'].to(device)\n",
        "            outputs, _ = model(node_features, adjacency_matrices, rollout_steps=1, teacher_forcing_prob=1.0)\n",
        "            preds = outputs[0]['predicted_score'].squeeze(-1).cpu().numpy()\n",
        "            y_pred_all.append(preds)\n",
        "            y_true_all.append(batch['cognitive_scores'].cpu().numpy())\n",
        "\n",
        "    y_pred = np.concatenate(y_pred_all)\n",
        "    y_true = np.concatenate(y_true_all)\n",
        "    results = {'mse': float(np.mean((y_true - y_pred)**2)), 'mae': float(np.mean(np.abs(y_true - y_pred)))}\n",
        "    # if binary labels present, compute ROC/AUC and bootstrap CI\n",
        "    if set(np.unique(y_true)) <= {0, 1}:\n",
        "        try:\n",
        "            from sklearn.metrics import roc_auc_score\n",
        "            auc = roc_auc_score(y_true, y_pred)\n",
        "            low, high = bootstrap_auc_ci(y_true, y_pred)\n",
        "            results['auc'] = float(auc)\n",
        "            results['auc_ci'] = (low, high)\n",
        "        except Exception:\n",
        "            pass\n",
        "\n",
        "    return results\n",
        "\n",
        "\n",
        "def run_ablation_suite(dataset_index_csv: str, save_path: Optional[str] = None, epochs: int = 10, batch_size: int = 8):\n",
        "    \"\"\"Run ablation experiments using generate_ablation_configs and collect results.\n",
        "\n",
        "    Saves CSV to save_path (if provided) and returns list of dicts.\n",
        "    \"\"\"\n",
        "    configs = generate_ablation_configs()\n",
        "    results = []\n",
        "    for cfg in configs:\n",
        "        model_cfg = {\n",
        "            'in_feats': 3,\n",
        "            'node_dim': 32,\n",
        "            'latent_dim': 12,\n",
        "            'use_edge_gru': cfg.get('use_edge_gru', False),\n",
        "            'use_gde': cfg.get('use_gde', False),\n",
        "            'use_adaptive_edges': cfg.get('use_adaptive_edges', False)\n",
        "        }\n",
        "        res = run_cross_site_experiment(dataset_index_csv, model_kwargs=model_cfg, train_kwargs={'epochs': epochs, 'batch_size': batch_size})\n",
        "        res_record = {'config': cfg, 'metrics': res}\n",
        "        results.append(res_record)\n",
        "\n",
        "    if save_path is not None:\n",
        "        import json\n",
        "        with open(save_path, 'w') as fh:\n",
        "            json.dump(results, fh, indent=2)\n",
        "    return results\n",
        "\n",
        "class StaticGraphBaseline(nn.Module):\n",
        "    # Baseline: Apply graph neural network to each timepoint independently, then RNN.\n",
        "\n",
        "    def __init__(self, in_features: int = 3, node_dim: int = 32, out_dim: int = 1):\n",
        "        super().__init__()\n",
        "        self.gat_layers = nn.ModuleList([\n",
        "            nn.Linear(in_features, node_dim),\n",
        "            nn.Linear(node_dim, node_dim)\n",
        "        ])\n",
        "        self.rnn = nn.LSTM(node_dim, 32, batch_first=True)\n",
        "        self.clinical_head = nn.Sequential(\n",
        "            nn.Linear(32, 16),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(16, out_dim)\n",
        "        )\n",
        "\n",
        "    def forward(self, node_features_sequence: torch.Tensor, adjacency_matrices_sequence: torch.Tensor) -> torch.Tensor:\n",
        "        # node_features_sequence: (B, T, N, F), adjacency_matrices_sequence: (B, T, N, N)\n",
        "        batch_size, num_timepoints, num_nodes, _ = node_features_sequence.shape\n",
        "\n",
        "        hidden_list = []\n",
        "        for t in range(num_timepoints):\n",
        "            features_t = node_features_sequence[:, t]  # (B, N, F)\n",
        "\n",
        "            # Simple GCN-like layers (ignoring adjacency for this simple baseline or assuming implicit)\n",
        "            # Note: Original code ignored adjacency in GAT layers (just linear), so keeping it consistent.\n",
        "            h_t = self.gat_layers[0](features_t)\n",
        "            h_t = torch.relu(h_t)\n",
        "            h_t = self.gat_layers[1](h_t)\n",
        "\n",
        "            h_pool = h_t.mean(dim=1)  # (B, node_dim)\n",
        "            hidden_list.append(h_pool)\n",
        "\n",
        "        hidden_sequence = torch.stack(hidden_list, dim=1)  # (B, T, node_dim)\n",
        "        out_rnn, (h_n, c_n) = self.rnn(hidden_sequence)\n",
        "\n",
        "        predicted_score = self.clinical_head(h_n[-1])  # (B, out_dim)\n",
        "        return predicted_score\n",
        "\n",
        "class FlatRNNBaseline(nn.Module):\n",
        "    # Baseline: Flatten connectome + features, pass through LSTM.\n",
        "\n",
        "    def __init__(self, num_nodes: int = 16, in_features: int = 3, out_dim: int = 1):\n",
        "        super().__init__()\n",
        "        flat_dim = num_nodes * num_nodes + num_nodes * in_features\n",
        "        self.lstm = nn.LSTM(flat_dim, 64, batch_first=True)\n",
        "        self.head = nn.Sequential(\n",
        "            nn.Linear(64, 32),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(32, out_dim)\n",
        "        )\n",
        "\n",
        "    def forward(self, node_features_sequence: torch.Tensor, adjacency_matrices_sequence: torch.Tensor) -> torch.Tensor:\n",
        "        batch_size, num_timepoints, num_nodes, num_features = node_features_sequence.shape\n",
        "\n",
        "        flattened_sequence = []\n",
        "        for t in range(num_timepoints):\n",
        "            adj_flat = adjacency_matrices_sequence[:, t].reshape(batch_size, -1)\n",
        "            feat_flat = node_features_sequence[:, t].reshape(batch_size, -1)\n",
        "            combined = torch.cat([adj_flat, feat_flat], dim=1)\n",
        "            flattened_sequence.append(combined)\n",
        "\n",
        "        flat_seq = torch.stack(flattened_sequence, dim=1)\n",
        "        out_rnn, (h_n, c_n) = self.lstm(flat_seq)\n",
        "\n",
        "        predicted_score = self.head(h_n[-1])\n",
        "        return predicted_score\n",
        "\n",
        "# ---------------------- Ablation plan generator ----------------------\n",
        "\n",
        "def generate_ablation_configs():\n",
        "    configs = []\n",
        "    base = {'use_gde': False, 'use_edge_gru': False, 'pretrain': True}\n",
        "    # ablate z_t\n",
        "    c1 = base.copy(); c1['use_z'] = False; configs.append(c1)\n",
        "    # ablate pretraining\n",
        "    c2 = base.copy(); c2['pretrain'] = False; configs.append(c2)\n",
        "    # ablate evolution function (PerEdgeMLP vs EdgeGRU)\n",
        "    c3 = base.copy(); c3['use_edge_gru'] = True; configs.append(c3)\n",
        "    # test GDE\n",
        "    c4 = base.copy(); c4['use_gde'] = True; configs.append(c4)\n",
        "    return configs\n",
        "\n",
        "# Add at the end before main\n",
        "\n",
        "def comprehensive_validation():\n",
        "    # Run full validation suite to ensure everything works correctly.\n",
        "    print('Running Comprehensive Validation Suite')\n",
        "    print('='*80)\n",
        "\n",
        "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "    checks_passed = 0\n",
        "    checks_total = 0\n",
        "    model = None\n",
        "\n",
        "    # Check 1: Data loading\n",
        "    print('\\nCheck 1: Data Loading')\n",
        "    checks_total += 1\n",
        "    try:\n",
        "        ds = SimulatedDDGDataset(num_subjects=10, num_regions=16, num_timepoints=6, seed=42)\n",
        "        dl = DataLoader(ds, batch_size=4, collate_fn=collate_batch)\n",
        "        batch = next(iter(dl))\n",
        "        assert batch['node_features'].shape == (4, 6, 16, 3), f\"Features shape mismatch: {batch['node_features'].shape}\"\n",
        "        assert batch['adjacency_matrices'].shape == (4, 6, 16, 16), f\"Adjacency shape mismatch: {batch['adjacency_matrices'].shape}\"\n",
        "        print('  Data loading: PASS')\n",
        "        checks_passed += 1\n",
        "    except Exception as e:\n",
        "        print(f'  Data loading: FAIL - {e}')\n",
        "\n",
        "    # Check 2: Model instantiation\n",
        "    print('\\nCheck 2: Model Instantiation')\n",
        "    checks_total += 1\n",
        "    try:\n",
        "        model = DDGModel(in_feats=3, node_dim=32, latent_dim=12).to(device)\n",
        "        print(f'  Model parameters: {sum(p.numel() for p in model.parameters()):,}')\n",
        "        print('  Model instantiation: PASS')\n",
        "        checks_passed += 1\n",
        "    except Exception as e:\n",
        "        print(f'  Model instantiation: FAIL - {e}')\n",
        "\n",
        "    # Check 3: Forward pass\n",
        "    print('\\nCheck 3: Forward Pass')\n",
        "    checks_total += 1\n",
        "    try:\n",
        "        if model is None:\n",
        "            raise RuntimeError('Model instantiation failed earlier')\n",
        "        model.eval()\n",
        "        with torch.no_grad():\n",
        "            node_features = torch.randn(4, 6, 16, 3).to(device)\n",
        "            adjacency_matrices = torch.randn(4, 6, 16, 16).to(device)\n",
        "            adjacency_matrices = F.softplus(adjacency_matrices)  # ensure non-negative\n",
        "            outputs, latent_sequence = model(node_features, adjacency_matrices, rollout_steps=2)\n",
        "        assert len(outputs) == 2, f\"Expected 2 outputs, got {len(outputs)}\"\n",
        "        assert latent_sequence.shape == (4, 6, 12), f\"latent_sequence shape: {latent_sequence.shape}\"\n",
        "        print('  Forward pass: PASS')\n",
        "        checks_passed += 1\n",
        "    except Exception as e:\n",
        "        print(f'  Forward pass: FAIL - {e}')\n",
        "\n",
        "    # Check 4: Loss computation\n",
        "    print('\\nCheck 4: Loss Computation')\n",
        "    checks_total += 1\n",
        "    try:\n",
        "        batch_dict = {\n",
        "            'adjacency_matrices': F.softplus(torch.randn(4, 6, 16, 16)).to(device),\n",
        "            'cognitive_scores': torch.rand(4).to(device) * 30\n",
        "        }\n",
        "        loss, sublogs = compute_losses(batch_dict, outputs, latent_sequence=latent_sequence)\n",
        "        assert loss.item() > 0, f\"Loss not positive: {loss.item()}\"\n",
        "        print(f'  Loss breakdown: {sublogs}')\n",
        "        print('  Loss computation: PASS')\n",
        "        checks_passed += 1\n",
        "    except Exception as e:\n",
        "        print(f'  Loss computation: FAIL - {e}')\n",
        "\n",
        "    # Check 5: Interpretability functions\n",
        "    print('\\nCheck 5: Interpretability Functions')\n",
        "    checks_total += 1\n",
        "    try:\n",
        "        adj_seq = [np.random.rand(16, 16) for _ in range(6)]\n",
        "        edge_imp, top_edges = compute_edge_importance(adj_seq, adj_seq)\n",
        "        assert edge_imp.shape == (16, 16), f\"edge_imp shape: {edge_imp.shape}\"\n",
        "        assert len(top_edges) > 0, \"No top edges found\"\n",
        "\n",
        "        node_str, degen_rate = compute_node_degeneration_rate(adj_seq)\n",
        "        assert node_str.shape == (16, 6), f\"node_str shape: {node_str.shape}\"\n",
        "        assert degen_rate.shape == (16,), f\"degen_rate shape: {degen_rate.shape}\"\n",
        "        print('  Interpretability: PASS')\n",
        "        checks_passed += 1\n",
        "    except Exception as e:\n",
        "        print(f'  Interpretability: FAIL - {e}')\n",
        "\n",
        "    # Check 6: Diagonal masking\n",
        "    print('\\nCheck 6: Diagonal Masking')\n",
        "    checks_total += 1\n",
        "    try:\n",
        "        model_masked = DDGModel(in_feats=3, node_dim=32, latent_dim=12).to(device)\n",
        "        model_masked.eval()\n",
        "        with torch.no_grad():\n",
        "            node_features = torch.randn(2, 6, 16, 3).to(device)\n",
        "            adjacency_matrices = F.softplus(torch.randn(2, 6, 16, 16)).to(device)\n",
        "            outputs, latent_sequence = model_masked(node_features, adjacency_matrices, rollout_steps=1)\n",
        "            adj_out = outputs[0]['predicted_adjacency']  # shape: (B, N, N)\n",
        "            # robust diagonal extraction that works for both batched and unbatched tensors\n",
        "            diag_vals = torch.diagonal(adj_out, dim1=-2, dim2=-1)  # shape: (B, N)\n",
        "            # pick first batch element for check\n",
        "            diag_first = diag_vals[0] if diag_vals.dim() == 2 else diag_vals\n",
        "            assert torch.allclose(diag_first, torch.zeros_like(diag_first), atol=1e-5), \"Diagonal not masked!\"\n",
        "        print('  Diagonal masking: PASS')\n",
        "        checks_passed += 1\n",
        "    except Exception as e:\n",
        "        print(f'  Diagonal masking: FAIL - {e}')\n",
        "\n",
        "    print('\\n' + '='*80)\n",
        "    print(f'Validation Summary: {checks_passed}/{checks_total} checks passed')\n",
        "    if checks_passed == checks_total:\n",
        "        print('All validation checks PASSED!')\n",
        "    else:\n",
        "        print(f'{checks_total - checks_passed} checks FAILED')\n",
        "    print('='*80)\n",
        "\n",
        "    return checks_passed == checks_total\n",
        "\n",
        "# ---------------------- Neural ODE longitudinal GNN & utilities ----------------------\n",
        "\n",
        "class ADNILongitudinalDataset(torch.utils.data.Dataset):\n",
        "    \"\"\"Subject-level longitudinal dataset for ADNI-derived ROI features.\n",
        "\n",
        "    Each subject folder should contain:\n",
        "      - times.npy             (T,) timestamps (days or years)\n",
        "      - node_features.npy     (T, N, F)\n",
        "      - edge_index.npy        (2, E) static edge_index OR per-time (T, N, N) adjacency\n",
        "      - label.npy             (1,) scalar label (e.g., diagnosis 0/1)\n",
        "\n",
        "    The dataset returns dicts with keys: 'x' (T,N,F), 'times' (T,), 'edge_index' or 'adj', 'y' (scalar)\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, root: str, subjects=None, time_unit: str = 'years', zscore: bool = True):\n",
        "        self.root = root\n",
        "        self.subjects = subjects if subjects is not None else sorted([d for d in os.listdir(root) if os.path.isdir(os.path.join(root, d))])\n",
        "        self.time_unit = time_unit\n",
        "        self.zscore = zscore\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.subjects)\n",
        "\n",
        "    def _load_subject(self, subject_dir: str):\n",
        "        d = os.path.join(self.root, subject_dir)\n",
        "        times = np.load(os.path.join(d, 'times.npy'))\n",
        "        x = np.load(os.path.join(d, 'node_features.npy'))  # T x N x F\n",
        "        edge_path = os.path.join(d, 'edge_index.npy')\n",
        "        adj_path = os.path.join(d, 'adjacency.npy')\n",
        "        if os.path.exists(edge_path):\n",
        "            edge_index = np.load(edge_path)\n",
        "            # ensure int dtype\n",
        "            edge_index = edge_index.astype(np.int64)\n",
        "            edge_index = torch.tensor(edge_index, dtype=torch.long)\n",
        "            adj = None\n",
        "        elif os.path.exists(adj_path):\n",
        "            adj = np.load(adj_path)\n",
        "            edge_index = None\n",
        "        else:\n",
        "            raise FileNotFoundError(f'No edge_index.npy or adjacency.npy in {d}')\n",
        "\n",
        "        label = np.load(os.path.join(d, 'label.npy'))\n",
        "        # unit conversion\n",
        "        if self.time_unit == 'days':\n",
        "            times = times / 365.25\n",
        "\n",
        "        if self.zscore:\n",
        "            mean = x.mean(axis=(0, 1), keepdims=True)\n",
        "            std = x.std(axis=(0, 1), keepdims=True) + 1e-6\n",
        "            x = (x - mean) / std\n",
        "\n",
        "        return torch.tensor(x, dtype=torch.float32), torch.tensor(times, dtype=torch.float32), edge_index, (torch.tensor(adj, dtype=torch.float32) if adj is not None else None), torch.tensor(label, dtype=torch.float32)\n",
        "\n",
        "    def __getitem__(self, idx: int):\n",
        "        sub = self.subjects[idx]\n",
        "        x, times, edge_index, adj, y = self._load_subject(sub)\n",
        "        return { 'x': x, 'times': times, 'edge_index': edge_index, 'adj': adj, 'y': y }\n",
        "\n",
        "\n",
        "def collate_longitudinal(batch):\n",
        "    \"\"\"Collate list of subject dicts into a batch dict; keeps variable-length times as lists.\"\"\"\n",
        "    xs = [b['x'] for b in batch]\n",
        "    times = [b['times'] for b in batch]\n",
        "    edges = [b['edge_index'] for b in batch]\n",
        "    adjs = [b['adj'] for b in batch]\n",
        "    ys = torch.stack([b['y'] for b in batch]).squeeze()\n",
        "    return { 'x': xs, 'times': times, 'edge_index': edges, 'adj': adjs, 'y': ys }\n",
        "\n",
        "\n",
        "# Lightweight guarded imports for graph layers and ODE integration\n",
        "_try_tg = True\n",
        "_try_torchdiffeq = True\n",
        "_try_captum = True\n",
        "try:\n",
        "    from torch_geometric.nn import GCNConv, global_mean_pool\n",
        "except Exception:\n",
        "    _try_tg = False\n",
        "    # define a fallback GCN-like layer (dense adjacency matmul)\n",
        "    class GCNConv(torch.nn.Module):\n",
        "        def __init__(self, in_channels, out_channels):\n",
        "            super().__init__()\n",
        "            self.lin = torch.nn.Linear(in_channels, out_channels)\n",
        "        def forward(self, x, edge_index=None, adj=None):\n",
        "            # x: (N, F) if adj provided: (N,N)\n",
        "            if adj is not None:\n",
        "                agg = adj @ x\n",
        "            else:\n",
        "                agg = x\n",
        "            return self.lin(agg)\n",
        "    def global_mean_pool(x, batch=None):\n",
        "        return x.mean(dim=0, keepdim=True)\n",
        "\n",
        "# Prefer torchdiffeq when available, otherwise provide a lightweight RK4 integrator fallback\n",
        "try:\n",
        "    from torchdiffeq import odeint  # type: ignore\n",
        "    _try_torchdiffeq = True\n",
        "except Exception:\n",
        "    _try_torchdiffeq = False\n",
        "\n",
        "    def odeint(func, y0, t, method: str = 'rk4', rtol: float = 1e-5, atol: float = 1e-6, **kwargs):\n",
        "        \"\"\"Fallback RK4-style integrator for small tests and debugging.\n",
        "\n",
        "        Note: This is not a production-quality ODE solver. For real training use\n",
        "        `torchdiffeq.odeint` which is more accurate and efficient.\n",
        "        \"\"\"\n",
        "        # ensure times is 1D numpy\n",
        "        if isinstance(t, torch.Tensor):\n",
        "            t_np = t.detach().cpu().numpy()\n",
        "        else:\n",
        "            t_np = np.asarray(t)\n",
        "        device = y0.device if isinstance(y0, torch.Tensor) else None\n",
        "        ys = []\n",
        "        y = y0\n",
        "        ys.append(y)\n",
        "        for i in range(len(t_np) - 1):\n",
        "            t0 = float(t_np[i])\n",
        "            t1 = float(t_np[i + 1])\n",
        "            dt = t1 - t0\n",
        "            # simple RK4 single-step on interval [t0, t1]\n",
        "            k1 = func(t0, y)\n",
        "            k2 = func(t0 + dt / 2.0, y + 0.5 * dt * k1)\n",
        "            k3 = func(t0 + dt / 2.0, y + 0.5 * dt * k2)\n",
        "            k4 = func(t0 + dt, y + dt * k3)\n",
        "            y = y + (dt / 6.0) * (k1 + 2 * k2 + 2 * k3 + k4)\n",
        "            ys.append(y)\n",
        "        # convert list to tensor returning (T, N, H)\n",
        "        return torch.stack([yy if isinstance(yy, torch.Tensor) else torch.tensor(yy, dtype=torch.float32, device=device) for yy in ys], dim=0)\n",
        "        out = torch.stack(ys, dim=0)\n",
        "        return out\n",
        "\n",
        "try:\n",
        "    from captum.attr import IntegratedGradients\n",
        "except Exception:\n",
        "    _try_captum = False\n",
        "\n",
        "\n",
        "class GraphODEFunc(torch.nn.Module):\n",
        "    \"\"\"Graph-parameterized ODE function f(h, A, t) that returns dh/dt.\n",
        "\n",
        "    This module explicitly models the instantaneous dynamics of node embeddings\n",
        "    using graph convolutions (or a dense matmul fallback). It accepts time `t`\n",
        "    as an argument so time-dependent dynamics can be added easily.\n",
        "\n",
        "    dh/dt = f(h, A, t)\n",
        "    \"\"\"\n",
        "    def __init__(self, hidden_dim: int, edge_index=None, adj=None, dropout: float = 0.0, use_batchnorm: bool = False, time_embed: bool = False):\n",
        "        super().__init__()\n",
        "        self.gc1 = GCNConv(hidden_dim, hidden_dim)\n",
        "        self.gc2 = GCNConv(hidden_dim, hidden_dim)\n",
        "        self.act = torch.nn.ReLU()\n",
        "        self.edge_index = edge_index\n",
        "        self.adj = adj\n",
        "        self.dropout = torch.nn.Dropout(dropout) if dropout > 0 else None\n",
        "        self.use_batchnorm = use_batchnorm\n",
        "        self.time_embed = time_embed\n",
        "        if use_batchnorm:\n",
        "            try:\n",
        "                # torch_geometric / dense fallbacks both use torch.nn.BatchNorm1d\n",
        "                self.bn1 = torch.nn.BatchNorm1d(hidden_dim)\n",
        "                self.bn2 = torch.nn.BatchNorm1d(hidden_dim)\n",
        "            except Exception:\n",
        "                self.bn1 = None\n",
        "                self.bn2 = None\n",
        "        if time_embed:\n",
        "            # small time embedding MLP: scalar t -> vector\n",
        "            self.t2 = torch.nn.Sequential(torch.nn.Linear(1, hidden_dim), torch.nn.Tanh())\n",
        "\n",
        "    def forward(self, t, h):\n",
        "        # h: (N, H)\n",
        "        # incorporate optional time embedding by adding it to node embeddings\n",
        "        if self.time_embed:\n",
        "            te = self.t2(torch.tensor([[float(t)]], device=h.device).float()).squeeze(0)  # (H,)\n",
        "            h = h + te\n",
        "        if _try_tg:\n",
        "            out = self.gc1(h, self.edge_index)\n",
        "            if self.use_batchnorm and self.bn1 is not None:\n",
        "                out = self.bn1(out)\n",
        "            out = self.act(out)\n",
        "            if self.dropout is not None:\n",
        "                out = self.dropout(out)\n",
        "            out = self.gc2(out, self.edge_index)\n",
        "            if self.use_batchnorm and self.bn2 is not None:\n",
        "                out = self.bn2(out)\n",
        "        else:\n",
        "            out = self.gc1(h, edge_index=None, adj=self.adj)\n",
        "            if self.use_batchnorm and self.bn1 is not None:\n",
        "                out = self.bn1(out)\n",
        "            out = self.act(out)\n",
        "            if self.dropout is not None:\n",
        "                out = self.dropout(out)\n",
        "            out = self.gc2(out, edge_index=None, adj=self.adj)\n",
        "            if self.use_batchnorm and self.bn2 is not None:\n",
        "                out = self.bn2(out)\n",
        "        return out\n",
        "\n",
        "\n",
        "class ODEBlock(torch.nn.Module):\n",
        "    def __init__(self, odefunc: GraphODEFunc, method: str = 'rk4', rtol: float = 1e-5, atol: float = 1e-6, require_torchdiffeq: bool = False):\n",
        "        \"\"\"Integrate a GraphODEFunc over specified time points.\n",
        "\n",
        "        Args:\n",
        "            odefunc: instance of GraphODEFunc or compatible callable\n",
        "            method: integration method (e.g., 'rk4' or method supported by torchdiffeq)\n",
        "            rtol, atol: relative and absolute tolerances forwarded to torchdiffeq.odeint when available\n",
        "            require_torchdiffeq: if True, raise ImportError when torchdiffeq is not installed\n",
        "        \"\"\"\n",
        "        super().__init__()\n",
        "        self.odefunc = odefunc\n",
        "        self.method = method\n",
        "        self.rtol = rtol\n",
        "        self.atol = atol\n",
        "        self.require_torchdiffeq = require_torchdiffeq\n",
        "\n",
        "    def forward(self, h0: torch.Tensor, times: torch.Tensor):\n",
        "        # times: (T,) increasing\n",
        "        if self.require_torchdiffeq and (not _try_torchdiffeq):\n",
        "            raise RuntimeError('torchdiffeq is required for require_torchdiffeq=True')\n",
        "        # Forward integration; pass tolerances for solvers that accept them\n",
        "        out = odeint(self.odefunc, h0, times, method=self.method, rtol=self.rtol, atol=self.atol)\n",
        "        return out  # (T, N, H)\n",
        "\n",
        "\n",
        "class NeuralODEGNN(torch.nn.Module):\n",
        "    \"\"\"Combines spatial GNN with continuous-time Neural ODE for longitudinal modeling.\n",
        "\n",
        "    Includes dropout and optional batch normalization for better generalization.\n",
        "    \"\"\"\n",
        "    def __init__(self, in_channels: int, hidden_dim: int = 64, num_classes: int = 1, dropout: float = 0.0, use_batchnorm: bool = False):\n",
        "        super().__init__()\n",
        "        self.input_proj = torch.nn.Linear(in_channels, hidden_dim)\n",
        "        self.input_bn = torch.nn.BatchNorm1d(hidden_dim) if use_batchnorm else None\n",
        "        self.dropout = torch.nn.Dropout(dropout) if dropout > 0 else None\n",
        "        self.odefunc = None\n",
        "        self.odeblock = None\n",
        "        # classifier with dropout\n",
        "        self.classifier = torch.nn.Sequential(\n",
        "            torch.nn.Linear(hidden_dim, max(hidden_dim // 2, 8)),\n",
        "            torch.nn.ReLU(),\n",
        "            torch.nn.Dropout(dropout) if dropout > 0 else torch.nn.Identity(),\n",
        "            torch.nn.Linear(max(hidden_dim // 2, 8), num_classes)\n",
        "        )\n",
        "        self._dropout_rate = dropout\n",
        "        self._use_batchnorm = use_batchnorm\n",
        "\n",
        "    def set_edge_index(self, edge_index=None, adj=None, time_embed: bool = False, require_torchdiffeq: bool = False, ode_method: str = 'rk4'):\n",
        "        # allows static topologies or precomputed dense adjacency\n",
        "        self.odefunc = GraphODEFunc(self.input_proj.out_features, edge_index=edge_index, adj=adj, dropout=self._dropout_rate, use_batchnorm=self._use_batchnorm, time_embed=time_embed)\n",
        "        self.odeblock = ODEBlock(self.odefunc, method=ode_method, require_torchdiffeq=require_torchdiffeq)\n",
        "\n",
        "    def forward(self, x_time: torch.Tensor, times: torch.Tensor, batch=None):\n",
        "        # x_time: (T, N, F)  times: (T,)\n",
        "        T, N, F = x_time.shape\n",
        "        x0 = x_time[0]\n",
        "        h0 = self.input_proj(x0)  # (N, H)\n",
        "        if self.input_bn is not None:\n",
        "            # BatchNorm1d expects shape (N, H)\n",
        "            h0 = self.input_bn(h0)\n",
        "        if self.dropout is not None:\n",
        "            h0 = self.dropout(h0)\n",
        "        if self.odeblock is None:\n",
        "            raise RuntimeError('Edge index or adjacency must be set via set_edge_index before forward')\n",
        "        h_ts = self.odeblock(h0, times)\n",
        "        h_last = h_ts[-1]\n",
        "        pooled = global_mean_pool(h_last, batch=batch)\n",
        "        out = self.classifier(pooled)\n",
        "        return out.squeeze()\n",
        "\n",
        "    def predict_proba(self, x_time: torch.Tensor, times: torch.Tensor, batch=None):\n",
        "        self.eval()\n",
        "        with torch.no_grad():\n",
        "            logits = self.forward(x_time, times, batch=batch)\n",
        "            return torch.sigmoid(logits)\n",
        "\n",
        "\n",
        "def augment_time_series(x_time: torch.Tensor, noise_std: float = 0.01, time_jitter: float = 0.0):\n",
        "    \"\"\"Simple augmentation: additive gaussian noise and optional small timepoint jitter (interpolation not implemented here).\n",
        "\n",
        "    This keeps augmentation lightweight and safe for continuous-time modeling.\n",
        "    \"\"\"\n",
        "    out = x_time.clone()\n",
        "    if noise_std > 0:\n",
        "        out = out + torch.randn_like(out) * noise_std\n",
        "    # time_jitter could be implemented by resampling/interpolation; keep as placeholder for extensibility\n",
        "    return out\n",
        "\n",
        "\n",
        "\n",
        "# ---------------------- Encoder / Decoder / Connectivity utilities ----------------------\n",
        "\n",
        "class RNNEncoder(torch.nn.Module):\n",
        "    \"\"\"Per-node RNN encoder that maps (T, N, F) -> (N, hidden_dim*). Produces mu and logvar per node.\"\"\"\n",
        "    def __init__(self, in_features: int, hidden_dim: int = 64, rnn_hidden: int = 64, num_layers: int = 1, bidirectional: bool = False):\n",
        "        super().__init__()\n",
        "        self.in_features = in_features\n",
        "        self.hidden_dim = hidden_dim\n",
        "        self.rnn_hidden = rnn_hidden\n",
        "        self.num_layers = num_layers\n",
        "        self.bidirectional = bidirectional\n",
        "        self.gru = torch.nn.GRU(in_features, rnn_hidden, num_layers=num_layers, batch_first=True, bidirectional=bidirectional)\n",
        "        rnn_out = rnn_hidden * (2 if bidirectional else 1)\n",
        "        self.mu = torch.nn.Linear(rnn_out, hidden_dim)\n",
        "        self.logvar = torch.nn.Linear(rnn_out, hidden_dim)\n",
        "\n",
        "    def forward(self, x_time: torch.Tensor):\n",
        "        # x_time: (T, N, F)\n",
        "        T, N, F = x_time.shape\n",
        "        # process per-node sequence: make (N, T, F)\n",
        "        x = x_time.permute(1, 0, 2).contiguous()\n",
        "        out, h_n = self.gru(x)  # out: (N, T, rnn_out)\n",
        "        last = out[:, -1, :]\n",
        "        mu = self.mu(last)\n",
        "        logvar = self.logvar(last)\n",
        "        return mu, logvar\n",
        "\n",
        "\n",
        "class Decoder(torch.nn.Module):\n",
        "    \"\"\"Decoder mapping latent node embeddings over time to reconstructed node features.\n",
        "\n",
        "    Applies a shared MLP to node embeddings at each timepoint.\n",
        "    \"\"\"\n",
        "    def __init__(self, hidden_dim: int, out_features: int, hidden_layers: int = 1):\n",
        "        super().__init__()\n",
        "        layers = []\n",
        "        cur = hidden_dim\n",
        "        for i in range(hidden_layers):\n",
        "            layers.append(torch.nn.Linear(cur, cur))\n",
        "            layers.append(torch.nn.ReLU())\n",
        "        layers.append(torch.nn.Linear(cur, out_features))\n",
        "        self.net = torch.nn.Sequential(*layers)\n",
        "\n",
        "    def forward(self, H_ts: torch.Tensor):\n",
        "        # H_ts: (T, N, H) -> output: (T, N, F)\n",
        "        T, N, H = H_ts.shape\n",
        "        x = H_ts.view(T * N, H)\n",
        "        out = self.net(x)\n",
        "        return out.view(T, N, -1)\n",
        "\n",
        "\n",
        "class ConnectivityCombiner(torch.nn.Module):\n",
        "    \"\"\"Combine multiple connectivity matrices with learnable positive weights.\n",
        "\n",
        "    Accepts a list of adjacency matrices (dense NxN). Returns weighted combination.\n",
        "    \"\"\"\n",
        "    def __init__(self, n_measures: int = 3):\n",
        "        super().__init__()\n",
        "        self.logits = torch.nn.Parameter(torch.zeros(n_measures))\n",
        "\n",
        "    def forward(self, adjs: list):\n",
        "        # adjs: list of (N, N) arrays/tensors\n",
        "        weights = torch.softmax(self.logits, dim=0)\n",
        "        device = adjs[0].device if isinstance(adjs[0], torch.Tensor) else None\n",
        "        out = 0.0\n",
        "        for w, a in zip(weights, adjs):\n",
        "            a_t = a if isinstance(a, torch.Tensor) else torch.tensor(a, dtype=torch.float32, device=device)\n",
        "            out = out + w * a_t\n",
        "        return out\n",
        "\n",
        "\n",
        "class EdgeAttention(torch.nn.Module):\n",
        "    \"\"\"Compute probabilistic edge importance PA from node embeddings.\n",
        "\n",
        "    Produces values in (0,1) for each potential edge (dense NxN).\n",
        "    \"\"\"\n",
        "    def __init__(self, node_dim: int, hidden: int = 32):\n",
        "        super().__init__()\n",
        "        self.fc = torch.nn.Sequential(torch.nn.Linear(2 * node_dim, hidden), torch.nn.ReLU(), torch.nn.Linear(hidden, 1))\n",
        "\n",
        "    def forward(self, H: torch.Tensor):\n",
        "        # H: (N, D) -> PA: (N, N)\n",
        "        N, D = H.shape\n",
        "        # compute pairwise concatenations efficiently\n",
        "        H_i = H.unsqueeze(1).repeat(1, N, 1)  # (N, N, D)\n",
        "        H_j = H.unsqueeze(0).repeat(N, 1, 1)  # (N, N, D)\n",
        "        pair = torch.cat([H_i, H_j], dim=-1)  # (N, N, 2D)\n",
        "        out = self.fc(pair.view(N * N, 2 * D)).view(N, N)\n",
        "        PA = torch.sigmoid(out)\n",
        "        return PA\n",
        "\n",
        "\n",
        "class MINE(torch.nn.Module):\n",
        "    \"\"\"A small MINE-like mutual information estimator for masks and labels.\n",
        "\n",
        "    Usage: estimate = mine.estimate(mask_flat, labels)\n",
        "    \"\"\"\n",
        "    def __init__(self, input_dim: int, hidden: int = 128):\n",
        "        super().__init__()\n",
        "        self.net = torch.nn.Sequential(torch.nn.Linear(input_dim + 1, hidden), torch.nn.ReLU(), torch.nn.Linear(hidden, 1))\n",
        "\n",
        "    def forward(self, mask, labels):\n",
        "        # mask: (B, D), labels: (B,1) or (B,)\n",
        "        if labels.dim() == 1:\n",
        "            labels = labels.unsqueeze(1)\n",
        "        x = torch.cat([mask, labels], dim=1)\n",
        "        return self.net(x).squeeze(-1)\n",
        "\n",
        "    def estimate(self, mask, labels):\n",
        "        # simple Donsker-Varadhan lower bound estimate (biased):\n",
        "        T = self.forward(mask, labels)\n",
        "        # shuffle labels for marginal\n",
        "        idx = torch.randperm(labels.size(0))\n",
        "        T_m = self.forward(mask, labels[idx])\n",
        "        mi = T.mean() - torch.log(torch.exp(T_m).mean() + 1e-8)\n",
        "        return mi\n",
        "\n",
        "\n",
        "class NodePX(torch.nn.Module):\n",
        "    \"\"\"Per-node-per-feature importance module PX: maps node embeddings -> (N, F) mask.\n",
        "\n",
        "    Produces probabilities in (0,1). Also provides entropy and sparsity computations.\n",
        "    \"\"\"\n",
        "    def __init__(self, node_dim: int, out_features: int):\n",
        "        super().__init__()\n",
        "        self.lin = torch.nn.Linear(node_dim, out_features)\n",
        "\n",
        "    def forward(self, H: torch.Tensor):\n",
        "        # H: (N, D) -> PX: (N, F)\n",
        "        out = self.lin(H)  # (N, F)\n",
        "        PX = torch.sigmoid(out)\n",
        "        return PX\n",
        "\n",
        "    @staticmethod\n",
        "    def entropy(PX: torch.Tensor, eps: float = 1e-8):\n",
        "        # binary entropy per entry: -p log p - (1-p) log(1-p)\n",
        "        e = -PX * torch.log(PX + eps) - (1.0 - PX) * torch.log(1.0 - PX + eps)\n",
        "        return e.mean()\n",
        "\n",
        "    @staticmethod\n",
        "    def sparsity(PX: torch.Tensor):\n",
        "        # L1 mean encourages sparsity across entries\n",
        "        return PX.abs().mean()\n",
        "\n",
        "\n",
        "# ---------------------- VAE-style Neural ODE longitudinal model ----------------------\n",
        "\n",
        "class NeuralODEVAEGNN(torch.nn.Module):\n",
        "    \"\"\"VAE-style Neural ODE GNN that encodes sequences into initial latent H0 and integrates.\n",
        "\n",
        "    - Encoder: RNN per node -> q(H0)=N(mu, sigma)\n",
        "    - ODE: integrates H0 to H(t)\n",
        "    - Decoder: reconstructs node features x_hat(t) from H(t)\n",
        "    - Attention & importance masks: PA (edges) and PX (node-feature masks)\n",
        "    \"\"\"\n",
        "    def __init__(self, in_features: int, hidden_dim: int = 64, decoder_hidden_layers: int = 1, dropout: float = 0.1, use_batchnorm: bool = True, use_mine: bool = False):\n",
        "        super().__init__()\n",
        "        self.encoder = RNNEncoder(in_features, hidden_dim)\n",
        "        self.decoder = Decoder(hidden_dim, in_features, hidden_layers=decoder_hidden_layers)\n",
        "        self.hidden_dim = hidden_dim\n",
        "        self.in_features = in_features\n",
        "        self.combiner = ConnectivityCombiner(n_measures=3)\n",
        "        self.edge_attention = EdgeAttention(node_dim=hidden_dim)\n",
        "        # per-node-per-feature importance module\n",
        "        self.px_module = NodePX(node_dim=hidden_dim, out_features=in_features)\n",
        "        self.classifier = torch.nn.Sequential(torch.nn.Linear(hidden_dim, hidden_dim // 2), torch.nn.ReLU(), torch.nn.Linear(hidden_dim // 2, 1))\n",
        "        self.use_mine = use_mine\n",
        "        if use_mine:\n",
        "            self.mine = MINE(input_dim=None if in_features is None else (1 * in_features), hidden=128)\n",
        "\n",
        "    def reparameterize(self, mu, logvar):\n",
        "        std = torch.exp(0.5 * logvar)\n",
        "        eps = torch.randn_like(std)\n",
        "        return mu + eps * std\n",
        "\n",
        "    def _make_adj_func(self, times_tensor: torch.Tensor, adjs_list: list, PA=None):\n",
        "        # adjs_list: list of T x N x N adjacency tensors or (T, N, N) np arrays\n",
        "        # build nearest-neighbor function\n",
        "        times = times_tensor.cpu().numpy()\n",
        "        adjs_np = [ (a if isinstance(a, np.ndarray) else a.detach().cpu().numpy()) for a in adjs_list ]\n",
        "        def adj_func(t):\n",
        "            # pick nearest time index\n",
        "            idx = int(np.argmin(np.abs(times - float(t))))\n",
        "            A = torch.tensor(adjs_np[idx], dtype=torch.float32)\n",
        "            if PA is not None:\n",
        "                A = A * PA\n",
        "            return A\n",
        "        return adj_func\n",
        "\n",
        "    def forward(self, x_time: torch.Tensor, times: torch.Tensor, adjs_list: list = None, labels: torch.Tensor = None, return_dict: bool = True, beta: float = 1.0):\n",
        "        # x_time: (T, N, F)\n",
        "        T, N, F = x_time.shape\n",
        "        mu, logvar = self.encoder(x_time)\n",
        "        # mu/logvar: (N, H)\n",
        "        H0 = self.reparameterize(mu, logvar)\n",
        "        # set ODEFunc adj func\n",
        "        if adjs_list is None:\n",
        "            # fall back to using static adjacency from last observed time (dense)\n",
        "            adjs_list = [torch.eye(N, dtype=torch.float32) for _ in range(T)]\n",
        "        # create PA from H0\n",
        "        PA = self.edge_attention(H0)  # (N, N)\n",
        "        # create PX per-node-per-feature\n",
        "        PX = self.px_module(H0)  # (N, F)\n",
        "        # attach graph-parameterized ODE function (ensures dh/dt = f(h, A, t))\n",
        "        gfunc = GraphODEFunc(self.hidden_dim, edge_index=None, adj=None, dropout=0.0, use_batchnorm=False, time_embed=False)\n",
        "        gfunc.adj_func = self._make_adj_func(times, adjs_list, PA=PA)\n",
        "        odeblock = ODEBlock(gfunc)\n",
        "        H_ts = odeblock(H0, times)  # (T, N, H)\n",
        "        # reconstruct\n",
        "        X_hat = self.decoder(H_ts)  # (T, N, F)\n",
        "        # apply PX to emphasize important features\n",
        "        X_hat_masked = X_hat * PX.unsqueeze(0)  # broadcast over time\n",
        "        # compute losses: recon MSE and KL\n",
        "        recon_loss = torch.nn.functional.mse_loss(X_hat_masked, x_time * PX.unsqueeze(0))\n",
        "        kl = -0.5 * torch.mean(1 + logvar - mu.pow(2) - logvar.exp())\n",
        "        # classifier on last time\n",
        "        h_last = H_ts[-1]\n",
        "        pooled = h_last.mean(dim=0, keepdim=True)\n",
        "        logits = self.classifier(pooled).squeeze()\n",
        "\n",
        "        # regularizers for PX\n",
        "        sparsity_loss = NodePX.sparsity(PX)\n",
        "        entropy_loss = NodePX.entropy(PX)\n",
        "\n",
        "        # optional mutual information estimate between flattened PX and labels\n",
        "        mi_est = None\n",
        "        if self.use_mine and labels is not None:\n",
        "            # flatten PX to (N*F,) then tile to batch if needed; for single-subject case create (1, D)\n",
        "            mask_flat = PX.view(1, -1)\n",
        "            lab = labels.view(-1) if labels is not None else torch.tensor([0.0])\n",
        "            if lab.dim() == 0:\n",
        "                lab = lab.unsqueeze(0)\n",
        "            mi_est = self.mine.estimate(mask_flat, lab)\n",
        "\n",
        "        if return_dict:\n",
        "            return {\n",
        "                'logits': logits,\n",
        "                'recon': recon_loss,\n",
        "                'kl': kl,\n",
        "                'PA': PA.detach().cpu().numpy(),\n",
        "                'PX': PX.detach().cpu().numpy(),\n",
        "                'sparsity_loss': sparsity_loss.detach().cpu().item(),\n",
        "                'entropy_loss': entropy_loss.detach().cpu().item(),\n",
        "                'mi_est': None if mi_est is None else float(mi_est.detach().cpu().item()),\n",
        "                'mu': mu,\n",
        "                'logvar': logvar\n",
        "            }\n",
        "        return logits\n",
        "\n",
        "# ---------------------- VAE training helper ----------------------\n",
        "\n",
        "def train_longitudinal_vae(model: NeuralODEVAEGNN, dataloader, optimizer, device='cpu', beta: float = 1.0, px_sparsity_weight: float = 0.0, px_entropy_weight: float = 0.0, mine_weight: float = 0.0):\n",
        "    \"\"\"Train a single epoch for the VAE-style Neural ODE model.\n",
        "\n",
        "    Each batch is expected to be a list of subject dicts with keys: 'x', 'times', 'adjs'(optional), 'y'.\n",
        "    Regularizers weights control the contribution of PX sparsity and entropy, and optional MINE MI loss.\n",
        "    \"\"\"\n",
        "    model.train()\n",
        "    total = {'recon': 0.0, 'kl': 0.0, 'cls': 0.0, 'sparsity': 0.0, 'entropy': 0.0, 'mi': 0.0}\n",
        "    loss_fn = torch.nn.BCEWithLogitsLoss()\n",
        "    n = 0\n",
        "    for batch in dataloader:\n",
        "        subjects = batch  # if dataloader yields lists\n",
        "        optimizer.zero_grad()\n",
        "        batch_loss = 0.0\n",
        "        for s in subjects:\n",
        "            x = s['x'].to(device)  # (T,N,F)\n",
        "            times = s['times'].to(device)\n",
        "            adjs = s.get('adjs', None)\n",
        "            y = s['y'].to(device)\n",
        "            res = model(x, times, adjs, labels=y, return_dict=True, beta=beta)\n",
        "            recon = res['recon']\n",
        "            kl = res['kl']\n",
        "            logits = res['logits']\n",
        "            sparsity = res['sparsity_loss']\n",
        "            entropy = res['entropy_loss']\n",
        "            mi_est = res.get('mi_est', None)\n",
        "\n",
        "            cls_loss = loss_fn(logits.unsqueeze(0), y.unsqueeze(0))\n",
        "            reg = px_sparsity_weight * sparsity + px_entropy_weight * entropy\n",
        "            if mine_weight > 0 and mi_est is not None:\n",
        "                reg = reg - mine_weight * mi_est  # maximize MI (subtract to maximize)\n",
        "\n",
        "            loss = recon + beta * kl + cls_loss + reg\n",
        "            loss.backward()\n",
        "            batch_loss += loss.item()\n",
        "            total['recon'] += float(recon.detach().cpu().item())\n",
        "            total['kl'] += float(kl.detach().cpu().item())\n",
        "            total['cls'] += float(cls_loss.detach().cpu().item())\n",
        "            total['sparsity'] += float(sparsity)\n",
        "            total['entropy'] += float(entropy)\n",
        "            total['mi'] += float(mi_est) if mi_est is not None else 0.0\n",
        "            n += 1\n",
        "        optimizer.step()\n",
        "    for k in total:\n",
        "        total[k] /= max(1, n)\n",
        "    return total\n",
        "\n",
        "# -----------------------------------------------------------------------------\n",
        "# Interpretability utilities (kept after these new modules)\n",
        "# -----------------------------------------------------------------------------\n",
        "\n",
        "# Interpretability utilities\n",
        "\n",
        "def integrated_gradients_over_time(model: torch.nn.Module, x_time: torch.Tensor, times: torch.Tensor, target: int = 0, baselines=None, n_steps: int = 50):\n",
        "    \"\"\"Compute Integrated Gradients across timepoints for node features.\n",
        "\n",
        "    Returns numpy array (T, N, F)\n",
        "    \"\"\"\n",
        "    if not _try_captum:\n",
        "        raise RuntimeError('captum is required for Integrated Gradients (pip install captum)')\n",
        "    if baselines is None:\n",
        "        baselines = torch.zeros_like(x_time)\n",
        "\n",
        "    T, N, F = x_time.shape\n",
        "    ig = IntegratedGradients(lambda inp: model(inp, times))\n",
        "    attributions = []\n",
        "    for t_idx in range(T):\n",
        "        def wrapper(x_slice):\n",
        "            xt = x_time.clone()\n",
        "            xt[t_idx] = x_slice\n",
        "            return model(xt, times)\n",
        "        ig_local = IntegratedGradients(wrapper)\n",
        "        at = ig_local.attribute(x_time[t_idx].unsqueeze(0), baselines=baselines[t_idx].unsqueeze(0), target=target, n_steps=n_steps)\n",
        "        attributions.append(at.squeeze(0).detach().cpu().numpy())\n",
        "    return np.stack(attributions, axis=0)\n",
        "\n",
        "\n",
        "def explain_snapshot_with_gnnexplainer(model: torch.nn.Module, snapshot_x: torch.Tensor, edge_index, epochs: int = 100):\n",
        "    \"\"\"Use torch-geometric's GNNExplainer for a single snapshot to get node/edge masks.\"\"\"\n",
        "    if not _try_tg:\n",
        "        raise RuntimeError('torch-geometric is required for GNNExplainer usage')\n",
        "    try:\n",
        "        from torch_geometric.nn import GNNExplainer\n",
        "    except Exception:\n",
        "        raise RuntimeError('GNNExplainer import failed')\n",
        "    explainer = GNNExplainer(model, epochs=epochs)\n",
        "    node_feat_mask, edge_mask = explainer.explain_graph(snapshot_x, edge_index)\n",
        "    return node_feat_mask.detach().cpu().numpy(), edge_mask.detach().cpu().numpy()\n",
        "\n",
        "\n",
        "# --- Interpretability helpers: explicit APIs for node/edge importance, saliency & simulation ---\n",
        "\n",
        "\n",
        "def node_importance_scores(model: torch.nn.Module, x_time: torch.Tensor, times: torch.Tensor, method: str = 'px', target: int = 0):\n",
        "    \"\"\"Compute per-node importance scores.\n",
        "\n",
        "    Methods:\n",
        "      - 'px': use per-node-per-feature PX if model exposes it (VAE returns 'PX') and aggregate over features\n",
        "      - 'gradient': compute abs gradient of scalar target w.r.t. input and aggregate over time+features\n",
        "      - 'ig': integrated gradients over time (requires captum)\n",
        "\n",
        "    Returns:\n",
        "      node_scores: numpy array (N,) with non-negative importance scores\n",
        "    \"\"\"\n",
        "    model.eval()\n",
        "    device = x_time.device\n",
        "    if method == 'px':\n",
        "        # try to get PX from a forward if supported\n",
        "        try:\n",
        "            out = model(x_time, times, return_dict=True)\n",
        "            PX = out.get('PX', None)\n",
        "            if PX is not None:\n",
        "                # PX is (N, F)\n",
        "                return np.asarray(PX.mean(axis=1))\n",
        "        except Exception:\n",
        "            pass\n",
        "        # fallback to gradient\n",
        "        method = 'gradient'\n",
        "\n",
        "    if method == 'ig':\n",
        "        if not _try_captum:\n",
        "            raise RuntimeError('captum required for integrated gradients')\n",
        "        at = integrated_gradients_over_time(model, x_time.detach(), times.detach(), target=target)\n",
        "        # at: (T, N, F) -> aggregate across time and features\n",
        "        node_scores = np.sum(np.abs(at), axis=(0, 2))\n",
        "        return node_scores\n",
        "\n",
        "    # gradient-based: scalarize and compute gradients\n",
        "    x = x_time.clone().detach().requires_grad_(True)\n",
        "    # forward\n",
        "    out = model(x, times)\n",
        "    if isinstance(out, dict):\n",
        "        logits = out.get('logits', None)\n",
        "        if logits is None:\n",
        "            # try recon loss as scalar\n",
        "            logits = out.get('recon', torch.tensor(0.0, device=device))\n",
        "    else:\n",
        "        logits = out\n",
        "    # scalar target\n",
        "    if torch.is_tensor(logits):\n",
        "        scalar = logits if logits.dim() == 0 else logits.sum()\n",
        "    else:\n",
        "        scalar = torch.tensor(float(logits), device=device)\n",
        "    grads = torch.autograd.grad(scalar, x, allow_unused=True)[0]\n",
        "    if grads is None:\n",
        "        # fallback: zeros\n",
        "        return np.zeros(x_time.shape[1], dtype=float)\n",
        "    # aggregate across time and features\n",
        "    node_scores = grads.abs().sum(dim=(0, 2)).detach().cpu().numpy()\n",
        "    return node_scores\n",
        "\n",
        "\n",
        "def edge_attribution_via_pa(model: NeuralODEVAEGNN, x_time: torch.Tensor, times: torch.Tensor, adjs_list: list = None, use_abs: bool = True):\n",
        "    \"\"\"Compute edge attribution scores by computing gradient of model logit wrt learned PA.\n",
        "\n",
        "    This currently supports `NeuralODEVAEGNN` which exposes `encoder` and `edge_attention`.\n",
        "    Returns numpy array (N, N) with importance scores.\n",
        "    \"\"\"\n",
        "    if not isinstance(model, NeuralODEVAEGNN):\n",
        "        # best-effort: try GNNExplainer if torch_geometric is available\n",
        "        if _try_tg:\n",
        "            try:\n",
        "                node_mask, edge_mask = explain_snapshot_with_gnnexplainer(model, x_time[-1], None)\n",
        "                # edge_mask is 1D of length num_edges; return None-shaped placeholder\n",
        "                return edge_mask\n",
        "            except Exception:\n",
        "                return None\n",
        "        return None\n",
        "\n",
        "    model.eval()\n",
        "    with torch.enable_grad():\n",
        "        mu, logvar = model.encoder(x_time)\n",
        "        H0 = mu  # deterministic for attribution\n",
        "        PA = model.edge_attention(H0)\n",
        "        PA = PA.clone().detach().requires_grad_(True)\n",
        "        PX = model.px_module(H0)  # (N, F) but we don't need grads for it here\n",
        "        # build adj func\n",
        "        if adjs_list is None:\n",
        "            adjs_list = [torch.eye(x_time.shape[1], dtype=torch.float32) for _ in range(x_time.shape[0])]\n",
        "        odefunc = ODEFunc(model.hidden_dim, edge_index=None, adj=None)\n",
        "        odefunc.adj_func = model._make_adj_func(times, adjs_list, PA=PA)\n",
        "        odeblock = ODEBlock(odefunc)\n",
        "        H_ts = odeblock(H0, times)\n",
        "        X_hat = model.decoder(H_ts) * PX.unsqueeze(0)\n",
        "        h_last = H_ts[-1]\n",
        "        pooled = h_last.mean(dim=0, keepdim=True)\n",
        "        logits = model.classifier(pooled).squeeze()\n",
        "        scalar = logits if logits.dim() == 0 else logits.sum()\n",
        "        grads = torch.autograd.grad(scalar, PA, retain_graph=False, allow_unused=True)[0]\n",
        "        if grads is None:\n",
        "            return None\n",
        "        scores = grads.abs().detach().cpu().numpy() if use_abs else grads.detach().cpu().numpy()\n",
        "        return scores\n",
        "\n",
        "\n",
        "def roi_saliency_over_time(model: torch.nn.Module, x_time: torch.Tensor, times: torch.Tensor, target: int = 0):\n",
        "    \"\"\"Return saliency map over ROIs across time: shape (T, N).\n",
        "\n",
        "    Uses gradient of scalar target w.r.t. input and aggregates abs grads across features.\n",
        "    \"\"\"\n",
        "    model.eval()\n",
        "    x = x_time.clone().detach().requires_grad_(True)\n",
        "    out = model(x, times)\n",
        "    logits = out['logits'] if isinstance(out, dict) and 'logits' in out else out\n",
        "    scalar = logits if torch.is_tensor(logits) and logits.dim() == 0 else (logits.sum() if torch.is_tensor(logits) else torch.tensor(float(logits)))\n",
        "    grads = torch.autograd.grad(scalar, x, allow_unused=True)[0]\n",
        "    if grads is None:\n",
        "        return np.zeros((x_time.shape[0], x_time.shape[1]), dtype=float)\n",
        "    saliency = grads.abs().sum(dim=-1).detach().cpu().numpy()\n",
        "    return saliency\n",
        "\n",
        "\n",
        "def simulate_time_evolution(model: NeuralODEVAEGNN, x_time: torch.Tensor, times: torch.Tensor, sim_times: Union[np.ndarray, List[float]], adjs_list: list = None, deterministic: bool = True):\n",
        "    \"\"\"Simulate time evolution (reconstructions + logits) at arbitrary `sim_times`.\n",
        "\n",
        "    Works for `NeuralODEVAEGNN` by using encoder->H0->ODEBlock->decoder flow. Returns dict with keys:\n",
        "      - 'X_hat': (T_sim, N, F)\n",
        "      - 'H_ts': (T_sim, N, H)\n",
        "      - 'logits': scalar or array depending on model\n",
        "      - 'PA': (N, N)\n",
        "      - 'PX': (N, F)\n",
        "    \"\"\"\n",
        "    if not isinstance(model, NeuralODEVAEGNN):\n",
        "        raise RuntimeError('simulate_time_evolution currently supports NeuralODEVAEGNN')\n",
        "    model.eval()\n",
        "    device = x_time.device\n",
        "    sim_times = np.array(sim_times, dtype=float)\n",
        "    with torch.no_grad():\n",
        "        mu, logvar = model.encoder(x_time)\n",
        "        H0 = mu if deterministic else model.reparameterize(mu, logvar)\n",
        "        if adjs_list is None:\n",
        "            adjs_list = [torch.eye(x_time.shape[1], dtype=torch.float32) for _ in range(x_time.shape[0])]\n",
        "        PA = model.edge_attention(H0)\n",
        "        PX = model.px_module(H0)\n",
        "        odefunc = ODEFunc(model.hidden_dim, edge_index=None, adj=None)\n",
        "        odefunc.adj_func = model._make_adj_func(torch.tensor(sim_times, dtype=torch.float32), adjs_list, PA=PA)\n",
        "        odeblock = ODEBlock(odefunc)\n",
        "        H_ts = odeblock(H0, torch.tensor(sim_times, dtype=torch.float32))\n",
        "        X_hat = model.decoder(H_ts) * PX.unsqueeze(0)\n",
        "        h_last = H_ts[-1]\n",
        "        pooled = h_last.mean(dim=0, keepdim=True)\n",
        "        logits = model.classifier(pooled).squeeze()\n",
        "    return {\n",
        "        'X_hat': X_hat.detach().cpu().numpy(),\n",
        "        'H_ts': H_ts.detach().cpu().numpy(),\n",
        "        'logits': logits.detach().cpu().item() if torch.is_tensor(logits) and logits.dim() == 0 else (logits.detach().cpu().numpy() if torch.is_tensor(logits) else logits),\n",
        "        'PA': PA.detach().cpu().numpy(),\n",
        "        'PX': PX.detach().cpu().numpy()\n",
        "    }\n",
        "\n",
        "\n",
        "def simulate_subject_trajectories(model: NeuralODEVAEGNN, x_time: torch.Tensor, times: torch.Tensor, n_future: int = 3, dt: float = 1.0, deterministic: bool = True):\n",
        "    \"\"\"Simulate simple subject-level trajectories extending `n_future` timepoints beyond observed `times`.\n",
        "\n",
        "    Returns same dict as `simulate_time_evolution` with sim_times that include observed times followed by future times.\n",
        "    \"\"\"\n",
        "    last = float(times[-1].detach().cpu().numpy())\n",
        "    sim_times = list(times.detach().cpu().numpy()) + list(last + (np.arange(1, n_future + 1) * float(dt)))\n",
        "    return simulate_time_evolution(model, x_time, times, sim_times, adjs_list=None, deterministic=deterministic)\n",
        "\n",
        "\n",
        "# --- Longitudinal interpretability helpers: extract latents & time-varying attributions ---\n",
        "\n",
        "def extract_latent_trajectories(model: torch.nn.Module, x_time: torch.Tensor, times: torch.Tensor, deterministic: bool = True):\n",
        "    \"\"\"Return latent trajectories H_ts (T, N, H) for supported models.\n",
        "\n",
        "    - NeuralODEVAEGNN: uses encoder -> H0 (mu if deterministic) -> ODEBlock\n",
        "    - NeuralODEGNN: uses input projection -> ODEBlock (requires model.set_edge_index called beforehand)\n",
        "    Returns H_ts torch.Tensor.\n",
        "    \"\"\"\n",
        "    model.eval()\n",
        "    if isinstance(model, NeuralODEVAEGNN):\n",
        "        mu, logvar = model.encoder(x_time)\n",
        "        H0 = mu if deterministic else model.reparameterize(mu, logvar)\n",
        "        # build odefunc/adjs similar to forward\n",
        "        adjs_list = None\n",
        "        odefunc = ODEFunc(model.hidden_dim, edge_index=None, adj=None)\n",
        "        odefunc.adj_func = model._make_adj_func(times, adjs_list or [torch.eye(x_time.shape[1]) for _ in range(x_time.shape[0])], PA=model.edge_attention(H0))\n",
        "        odeblock = ODEBlock(odefunc)\n",
        "        H_ts = odeblock(H0, times)\n",
        "        return H_ts\n",
        "    elif isinstance(model, NeuralODEGNN):\n",
        "        # input proj + use existing odefunc/odeblock (must be set via set_edge_index)\n",
        "        if model.odeblock is None:\n",
        "            raise RuntimeError('Model ODE block (edge_index/adj) must be set via set_edge_index')\n",
        "        h0 = model.input_proj(x_time[0])\n",
        "        if model.input_bn is not None:\n",
        "            h0 = model.input_bn(h0)\n",
        "        if model.dropout is not None and model.training:\n",
        "            h0 = model.dropout(h0)\n",
        "        H_ts = model.odeblock(h0, times)\n",
        "        return H_ts\n",
        "    else:\n",
        "        raise RuntimeError('Unsupported model for latent extraction')\n",
        "\n",
        "\n",
        "def compute_time_varying_edge_attention(model: torch.nn.Module, x_time: torch.Tensor, times: torch.Tensor, deterministic: bool = True):\n",
        "    \"\"\"Compute PA for each timepoint by applying EdgeAttention to latent H(t).\n",
        "\n",
        "    Returns: PA_ts as numpy array of shape (T, N, N)\n",
        "    \"\"\"\n",
        "    H_ts = extract_latent_trajectories(model, x_time, times, deterministic=deterministic)\n",
        "    T, N, H = H_ts.shape\n",
        "    pa_list = []\n",
        "    attn = None\n",
        "    # if model exposes an EdgeAttention instance use it; otherwise construct one\n",
        "    if hasattr(model, 'edge_attention') and isinstance(getattr(model, 'edge_attention'), EdgeAttention):\n",
        "        attn = model.edge_attention\n",
        "    else:\n",
        "        attn = EdgeAttention(H)\n",
        "    for t in range(T):\n",
        "        Ht = H_ts[t]\n",
        "        PA = attn(Ht)\n",
        "        pa_list.append(PA.detach().cpu().numpy())\n",
        "    return np.stack(pa_list, axis=0)\n",
        "\n",
        "\n",
        "def compute_longitudinal_importances(model: torch.nn.Module, subject_item: Dict[str, Any], method: str = 'px'):\n",
        "    \"\"\"Aggregate node and edge importances across sessions for a subject item from `ADNISubjectDataset`.\n",
        "\n",
        "    Returns a dict with keys:\n",
        "      - 'node_scores': (S, N) numpy array\n",
        "      - 'edge_scores': (S, N, N) numpy array\n",
        "      - 'session_ids': list of session ids corresponding to rows\n",
        "    \"\"\"\n",
        "    sessions = subject_item['sessions']\n",
        "    S = len(sessions)\n",
        "    node_scores = []\n",
        "    edge_scores = []\n",
        "    session_ids = subject_item.get('session_ids', [sess['session_id'] for sess in sessions])\n",
        "    for sess in sessions:\n",
        "        x_time = torch.tensor(sess['node_features'], dtype=torch.float32)\n",
        "        times = torch.tensor(np.arange(x_time.shape[0]), dtype=torch.float32)\n",
        "        ns = node_importance_scores(model, x_time, times, method=method)\n",
        "        es = None\n",
        "        # try PA-based edge attribution if VAE available\n",
        "        if isinstance(model, NeuralODEVAEGNN):\n",
        "            try:\n",
        "                es = edge_attribution_via_pa(model, x_time, times)\n",
        "            except Exception:\n",
        "                es = None\n",
        "        if es is None:\n",
        "            # fall back to time-varying attention computed from latents\n",
        "            try:\n",
        "                pa_ts = compute_time_varying_edge_attention(model, x_time, times)\n",
        "                # simple summary: average absolute PA over time\n",
        "                es = np.mean(np.abs(pa_ts), axis=0)\n",
        "            except Exception:\n",
        "                # final fallback: zero matrix\n",
        "                N = x_time.shape[1]\n",
        "                es = np.zeros((N, N), dtype=float)\n",
        "        node_scores.append(ns)\n",
        "        edge_scores.append(es)\n",
        "    return {\n",
        "        'node_scores': np.stack(node_scores, axis=0),\n",
        "        'edge_scores': np.stack(edge_scores, axis=0),\n",
        "        'session_ids': session_ids\n",
        "    }\n",
        "\n",
        "\n",
        "# --- Visualization helpers (node/edge saliency & subject trajectories) ---\n",
        "\n",
        "def plot_node_saliency_over_time(saliency: np.ndarray, times: Optional[Union[np.ndarray, List[float]]] = None, roi_names: Optional[List[str]] = None, ax=None, cmap: str = 'viridis', title: Optional[str] = None):\n",
        "    \"\"\"Plot a heatmap of saliency over time and ROIs.\n",
        "\n",
        "    Args:\n",
        "        saliency: (T, N) array of saliency values (time x ROI)\n",
        "        times: optional (T,) array for x-axis labels\n",
        "        roi_names: optional list of length N for y-axis labels\n",
        "        ax: optional matplotlib Axes to draw into\n",
        "\n",
        "    Returns:\n",
        "        (fig, ax)\n",
        "    \"\"\"\n",
        "    try:\n",
        "        import matplotlib.pyplot as plt\n",
        "    except Exception:\n",
        "        raise RuntimeError('matplotlib is required for plotting (pip install matplotlib)')\n",
        "\n",
        "    T, N = saliency.shape\n",
        "    if times is None:\n",
        "        times = np.arange(T)\n",
        "    if ax is None:\n",
        "        fig, ax = plt.subplots(figsize=(6, 4))\n",
        "    else:\n",
        "        fig = ax.get_figure()\n",
        "\n",
        "    im = ax.imshow(saliency.T, aspect='auto', origin='lower', cmap=cmap)\n",
        "    ax.set_xlabel('Time')\n",
        "    ax.set_xticks(np.arange(T))\n",
        "    ax.set_xticklabels([f\"{t:.2f}\" for t in times], rotation=45)\n",
        "    ax.set_ylabel('ROI')\n",
        "    if roi_names is not None:\n",
        "        ax.set_yticks(np.arange(N))\n",
        "        ax.set_yticklabels(roi_names)\n",
        "    cbar = fig.colorbar(im, ax=ax)\n",
        "    cbar.set_label('Saliency')\n",
        "    if title:\n",
        "        ax.set_title(title)\n",
        "    return fig, ax\n",
        "\n",
        "\n",
        "def plot_edge_attribution(edge_scores: np.ndarray, labels: Optional[List[str]] = None, ax=None, cmap: str = 'coolwarm', title: Optional[str] = 'Edge attribution'):\n",
        "    \"\"\"Plot edge attribution as a heatmap (N x N).\n",
        "\n",
        "    Returns (fig, ax).\n",
        "    \"\"\"\n",
        "    try:\n",
        "        import matplotlib.pyplot as plt\n",
        "    except Exception:\n",
        "        raise RuntimeError('matplotlib is required for plotting (pip install matplotlib)')\n",
        "\n",
        "    if edge_scores.ndim != 2 or edge_scores.shape[0] != edge_scores.shape[1]:\n",
        "        raise ValueError('edge_scores must be a square matrix (N,N)')\n",
        "    N = edge_scores.shape[0]\n",
        "    if ax is None:\n",
        "        fig, ax = plt.subplots(figsize=(5, 5))\n",
        "    else:\n",
        "        fig = ax.get_figure()\n",
        "    im = ax.imshow(edge_scores, cmap=cmap, vmin=None, vmax=None)\n",
        "    ax.set_xlabel('ROI')\n",
        "    ax.set_ylabel('ROI')\n",
        "    if labels is not None:\n",
        "        ax.set_xticks(np.arange(N)); ax.set_xticklabels(labels, rotation=90)\n",
        "        ax.set_yticks(np.arange(N)); ax.set_yticklabels(labels)\n",
        "    fig.colorbar(im, ax=ax)\n",
        "    if title:\n",
        "        ax.set_title(title)\n",
        "    return fig, ax\n",
        "\n",
        "\n",
        "def plot_subject_trajectories(sim: dict, times: Optional[Union[np.ndarray, List[float]]] = None, nodes: Optional[List[int]] = None, feature: Optional[int] = None, ax=None, show_legend: bool = True, title: Optional[str] = 'Subject trajectories'):\n",
        "    \"\"\"Plot simulated subject trajectories (X_hat) for selected nodes and a single feature.\n",
        "\n",
        "    Args:\n",
        "        sim: dict returned by `simulate_time_evolution`/`simulate_subject_trajectories` with 'X_hat'\n",
        "        times: optional times (length T_sim)\n",
        "        nodes: list of node indices to plot (default: first 4 or all if <=4)\n",
        "        feature: which feature index to plot (default: mean across features)\n",
        "\n",
        "    Returns (fig, ax)\n",
        "    \"\"\"\n",
        "    try:\n",
        "        import matplotlib.pyplot as plt\n",
        "    except Exception:\n",
        "        raise RuntimeError('matplotlib is required for plotting (pip install matplotlib)')\n",
        "\n",
        "    X_hat = sim.get('X_hat')\n",
        "    if X_hat is None:\n",
        "        raise ValueError('sim dict must contain X_hat')\n",
        "    T_sim, N, F = X_hat.shape\n",
        "    if times is None:\n",
        "        times = np.arange(T_sim)\n",
        "    if nodes is None:\n",
        "        nodes = list(range(min(4, N)))\n",
        "    if feature is None:\n",
        "        # average over features\n",
        "        Y = X_hat.mean(axis=-1)\n",
        "    else:\n",
        "        Y = X_hat[:, :, feature]\n",
        "\n",
        "    if ax is None:\n",
        "        fig, ax = plt.subplots(figsize=(6, 4))\n",
        "    else:\n",
        "        fig = ax.get_figure()\n",
        "\n",
        "    for n in nodes:\n",
        "        ax.plot(times, Y[:, n], label=f'ROI {n}')\n",
        "    ax.set_xlabel('Time')\n",
        "    ax.set_ylabel('Signal')\n",
        "    if show_legend:\n",
        "        ax.legend()\n",
        "    if title:\n",
        "        ax.set_title(title)\n",
        "    return fig, ax\n",
        "\n",
        "\n",
        "# Training helpers (lightweight)\n",
        "\n",
        "def train_longitudinal(model: torch.nn.Module, dataloader, optimizer, device='cpu', augment: dict = None):\n",
        "    \"\"\"Single-epoch training with optional augmentation.\"\"\"\n",
        "    model.train()\n",
        "    loss_fn = torch.nn.BCEWithLogitsLoss()\n",
        "    total = 0.0\n",
        "    for batch in dataloader:\n",
        "        xs = batch['x']\n",
        "        times = batch['times']\n",
        "        ys = batch['y'].to(device)\n",
        "        optimizer.zero_grad()\n",
        "        batch_loss = 0.0\n",
        "        for i in range(len(xs)):\n",
        "            x_time = xs[i].to(device)\n",
        "            t = times[i].to(device)\n",
        "            if augment is not None and augment.get('noise_std', 0.0) > 0:\n",
        "                x_time = augment_time_series(x_time, noise_std=augment.get('noise_std', 0.01))\n",
        "            edge_index = batch['edge_index'][i]\n",
        "            adj = batch['adj'][i]\n",
        "            if edge_index is not None:\n",
        "                model.set_edge_index(edge_index=edge_index.to(device), adj=None)\n",
        "            else:\n",
        "                model.set_edge_index(edge_index=None, adj=adj.to(device))\n",
        "            out = model(x_time, t)\n",
        "            loss = loss_fn(out.unsqueeze(0), ys[i].unsqueeze(0))\n",
        "            loss.backward()\n",
        "            batch_loss += loss.item()\n",
        "        optimizer.step()\n",
        "        total += batch_loss\n",
        "    return total / len(dataloader)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def run_kfold_cv(dataset, k: int = 5, random_seed: int = 42, **train_kwargs):\n",
        "    \"\"\"Run k-fold cross validation to assess generalization and reduce overfitting.\n",
        "\n",
        "    train_kwargs are forwarded to `universal_train_with_early_stopping` (e.g., lr, weight_decay, max_epochs, patience)\n",
        "    Returns list of histories and validation scores.\n",
        "    \"\"\"\n",
        "    from sklearn.model_selection import KFold\n",
        "    kf = KFold(n_splits=k, shuffle=True, random_state=random_seed)\n",
        "    idx = np.arange(len(dataset))\n",
        "    histories = []\n",
        "    val_scores = []\n",
        "    for fold, (tr, va) in enumerate(kf.split(idx)):\n",
        "        print(f\"Fold {fold+1}/{k}\")\n",
        "        ds_tr = torch.utils.data.Subset(dataset, tr)\n",
        "        ds_va = torch.utils.data.Subset(dataset, va)\n",
        "        dl_tr = torch.utils.data.DataLoader(ds_tr, batch_size=train_kwargs.get('batch_size', 8), shuffle=True, collate_fn=collate_longitudinal)\n",
        "        dl_va = torch.utils.data.DataLoader(ds_va, batch_size=train_kwargs.get('batch_size', 8), shuffle=False, collate_fn=collate_longitudinal)\n",
        "        device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "        sample = dataset[0]\n",
        "        T, N, F = sample['x'].shape\n",
        "        model = NeuralODEGNN(in_channels=F, hidden_dim=train_kwargs.get('hidden_dim', 64), num_classes=1, dropout=train_kwargs.get('dropout', 0.0), use_batchnorm=train_kwargs.get('use_batchnorm', False)).to(device)\n",
        "        # define batch forward function for longitudinal batches\n",
        "        def batch_forward_longitudinal(m, batch, device):\n",
        "            xs = batch['x']\n",
        "            times = batch['times']\n",
        "            ys = batch['y']\n",
        "            preds = []\n",
        "            targets = []\n",
        "            for i in range(len(xs)):\n",
        "                x_time = xs[i].to(device)\n",
        "                t = times[i].to(device)\n",
        "                edge_index = batch['edge_index'][i]\n",
        "                adj = batch['adj'][i]\n",
        "                if edge_index is not None:\n",
        "                    m.set_edge_index(edge_index=edge_index.to(device), adj=None)\n",
        "                else:\n",
        "                    m.set_edge_index(edge_index=None, adj=adj.to(device))\n",
        "                out = m(x_time, t)\n",
        "                preds.append(out.unsqueeze(0))\n",
        "                targets.append(ys[i].unsqueeze(0).to(device))\n",
        "            preds = torch.cat(preds, dim=0)\n",
        "            targets = torch.cat(targets, dim=0)\n",
        "            return preds, targets\n",
        "\n",
        "        trained_model, history = universal_train_with_early_stopping(model, dl_tr, dl_va, batch_forward_fn=batch_forward_longitudinal, loss_fn=torch.nn.BCEWithLogitsLoss(), device=device, **train_kwargs)\n",
        "        histories.append(history)\n",
        "        auc, acc = eval_longitudinal(trained_model, dl_va, device=device)\n",
        "        val_scores.append({'auc': auc, 'acc': acc})\n",
        "        print(f\"Fold {fold+1} val_auc={auc:.4f}, val_acc={acc:.4f}\")\n",
        "    return histories, val_scores\n",
        "\n",
        "def eval_longitudinal(model: torch.nn.Module, dataloader, device='cpu'):\n",
        "    model.eval()\n",
        "    preds = []\n",
        "    trues = []\n",
        "    with torch.no_grad():\n",
        "        for batch in dataloader:\n",
        "            xs = batch['x']\n",
        "            times = batch['times']\n",
        "            ys = batch['y'].numpy().tolist()\n",
        "            for i in range(len(xs)):\n",
        "                x_time = xs[i].to(device)\n",
        "                t = times[i].to(device)\n",
        "                edge_index = batch['edge_index'][i]\n",
        "                adj = batch['adj'][i]\n",
        "                if edge_index is not None:\n",
        "                    model.set_edge_index(edge_index=edge_index.to(device), adj=None)\n",
        "                else:\n",
        "                    model.set_edge_index(edge_index=None, adj=adj.to(device))\n",
        "                out = torch.sigmoid(model(x_time, t))\n",
        "                preds.append(out.item())\n",
        "            trues.extend(ys)\n",
        "    from sklearn.metrics import roc_auc_score, accuracy_score\n",
        "    auc = roc_auc_score(trues, preds) if len(set(trues)) > 1 else float('nan')\n",
        "    acc = accuracy_score([int(p > 0.5) for p in preds], [int(t) for t in trues])\n",
        "    return auc, acc\n",
        "\n",
        "\n",
        "# -----------------------------------------------------------------------------\n",
        "# Generic batch-based training utilities (work with arbitrary batch formats)\n",
        "# -----------------------------------------------------------------------------\n",
        "\n",
        "def universal_train_epoch(model: torch.nn.Module, dataloader, optimizer, device: str = 'cpu', loss_fn=torch.nn.MSELoss(), batch_forward_fn=None, augment: dict = None):\n",
        "    \"\"\"Train for one epoch using a generic batch forward function.\n",
        "\n",
        "    - batch_forward_fn(model, batch, device) should return (preds, targets)\n",
        "    - dataloader is any iterable of 'batch' objects\n",
        "    - loss_fn is a PyTorch loss (e.g., MSELoss, BCEWithLogitsLoss)\n",
        "    \"\"\"\n",
        "    model.train()\n",
        "    total_loss = 0.0\n",
        "    n_batches = 0\n",
        "    for batch in dataloader:\n",
        "        if augment is not None and isinstance(augment, dict) and augment.get('noise_std', 0.0) > 0:\n",
        "            # caller is responsible for applying augmentation inside batch_forward_fn if needed\n",
        "            pass\n",
        "        optimizer.zero_grad()\n",
        "        preds, targets = batch_forward_fn(model, batch, device)\n",
        "        loss = loss_fn(preds, targets)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        total_loss += float(loss.detach().cpu().item())\n",
        "        n_batches += 1\n",
        "    return total_loss / max(1, n_batches)\n",
        "\n",
        "\n",
        "def universal_eval(model: torch.nn.Module, dataloader, device: str = 'cpu', batch_forward_fn=None):\n",
        "    \"\"\"Evaluate model over dataloader using batch_forward_fn; returns dict with preds and trues and computed metrics when possible.\"\"\"\n",
        "    model.eval()\n",
        "    preds_all = []\n",
        "    trues_all = []\n",
        "    with torch.no_grad():\n",
        "        for batch in dataloader:\n",
        "            preds, targets = batch_forward_fn(model, batch, device)\n",
        "            # allow preds/targets to be tensors or numpy\n",
        "            preds_np = preds.detach().cpu().numpy() if isinstance(preds, torch.Tensor) else np.array(preds)\n",
        "            targets_np = targets.detach().cpu().numpy() if isinstance(targets, torch.Tensor) else np.array(targets)\n",
        "            # flatten\n",
        "            preds_all.extend(np.asarray(preds_np).ravel().tolist())\n",
        "            trues_all.extend(np.asarray(targets_np).ravel().tolist())\n",
        "    res = {'preds': np.array(preds_all), 'trues': np.array(trues_all)}\n",
        "    # try to compute sensible metrics\n",
        "    try:\n",
        "        from sklearn.metrics import roc_auc_score, accuracy_score, mean_squared_error\n",
        "        # classification\n",
        "        if set(np.unique(res['trues'])) <= {0, 1}:\n",
        "            if len(set(res['trues'])) > 1:\n",
        "                res['auc'] = roc_auc_score(res['trues'], res['preds'])\n",
        "            else:\n",
        "                res['auc'] = float('nan')\n",
        "            res['acc'] = accuracy_score((res['preds'] > 0.5).astype(int), res['trues'].astype(int))\n",
        "        else:\n",
        "            # regression\n",
        "            res['mse'] = mean_squared_error(res['trues'], res['preds'])\n",
        "    except Exception:\n",
        "        pass\n",
        "    return res\n",
        "\n",
        "\n",
        "def universal_train_with_early_stopping(model: torch.nn.Module, dl_train, dl_val, batch_forward_fn, loss_fn=torch.nn.MSELoss(), device: str = 'cpu', lr: float = 1e-3, weight_decay: float = 1e-4, max_epochs: int = 100, patience: int = 10, monitor: str = 'auc', augment: dict = None):\n",
        "    \"\"\"Generic training loop with early stopping and LR scheduling.\n",
        "\n",
        "    - monitor: 'auc' or 'val_loss' (for regression)\n",
        "    - batch_forward_fn: function(model, batch, device) -> (preds, targets)\n",
        "    - dl_train/dl_val: iterables of batches\n",
        "    \"\"\"\n",
        "    import copy\n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr=lr, weight_decay=weight_decay)\n",
        "    # choose scheduler mode\n",
        "    mode = 'max' if monitor == 'auc' else 'min'\n",
        "    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode=mode, factor=0.5, patience=max(1, patience // 3), verbose=True)\n",
        "\n",
        "    best_metric = -float('inf') if mode == 'max' else float('inf')\n",
        "    best_state = None\n",
        "    no_improve = 0\n",
        "    history = {'train_loss': [], 'val_metric': []}\n",
        "\n",
        "    for epoch in range(max_epochs):\n",
        "        train_loss = universal_train_epoch(model, dl_train, optimizer, device=device, loss_fn=loss_fn, batch_forward_fn=batch_forward_fn, augment=augment)\n",
        "        val_res = universal_eval(model, dl_val, device=device, batch_forward_fn=batch_forward_fn)\n",
        "\n",
        "        if monitor == 'auc':\n",
        "            metric = val_res.get('auc', float('nan'))\n",
        "        else:\n",
        "            metric = val_res.get('mse', val_res.get('val_loss', float('nan')))\n",
        "\n",
        "        history['train_loss'].append(train_loss)\n",
        "        history['val_metric'].append(metric)\n",
        "\n",
        "        # scheduler step\n",
        "        if not np.isnan(metric):\n",
        "            scheduler.step(metric)\n",
        "\n",
        "        improved = False\n",
        "        if (mode == 'max' and metric > best_metric + 1e-6) or (mode == 'min' and metric < best_metric - 1e-6):\n",
        "            best_metric = metric\n",
        "            best_state = copy.deepcopy(model.state_dict())\n",
        "            no_improve = 0\n",
        "            improved = True\n",
        "        else:\n",
        "            no_improve += 1\n",
        "\n",
        "        print(f\"Epoch {epoch}: train_loss={train_loss:.4f}, val_metric={metric}\")\n",
        "\n",
        "        if no_improve >= patience:\n",
        "            print(f\"Early stopping after {epoch+1} epochs (no improvement for {patience} epochs)\")\n",
        "            break\n",
        "\n",
        "    if best_state is not None:\n",
        "        model.load_state_dict(best_state)\n",
        "    return model, history\n",
        "\n",
        "# Extend CLI to include longitudinal training/demo\n",
        "if __name__ == '__main__':\n",
        "    parser = argparse.ArgumentParser(description='GNN ADNI utilities')\n",
        "    parser.add_argument('--preview-manifest', type=str, help='Path to ADNI manifest CSV to preview')\n",
        "    parser.add_argument('--adni-root', type=str, default=None, help='Root dir for timeseries files')\n",
        "    parser.add_argument('--n', type=int, default=5, help='Number of samples to preview')\n",
        "    parser.add_argument('--run-validation', action='store_true', help='Run full internal validation and demo')\n",
        "    parser.add_argument('--train-longitudinal', action='store_true', help='Train Neural ODE longitudinal GNN')\n",
        "    parser.add_argument('--data-dir', type=str, help='Directory of per-subject folders for longitudinal training')\n",
        "    parser.add_argument('--epochs', type=int, default=20)\n",
        "    parser.add_argument('--batch-size', type=int, default=8)\n",
        "    parser.add_argument('--lr', type=float, default=1e-3)\n",
        "    parser.add_argument('--demo-longitudinal', action='store_true', help='Run a short randomized demo training for the longitudinal model')\n",
        "    args = parser.parse_args()\n",
        "\n",
        "    if args.preview_manifest:\n",
        "        _cli_preview(args.preview_manifest, adni_root=args.adni_root, n=args.n)\n",
        "        sys.exit(0)\n",
        "\n",
        "    # If user requested validation or no args provided, run internal checks + demo\n",
        "    if args.run_validation or (not any([args.train_longitudinal, args.demo_longitudinal]) and len(sys.argv) == 1):\n",
        "        print('='*80)\n",
        "        print('Dynamic Disease Graph (DDG) - Research-Grade Pipeline')\n",
        "        print('='*80)\n",
        "        print()\n",
        "\n",
        "        # Run validation\n",
        "        validation_ok = comprehensive_validation()\n",
        "\n",
        "        if validation_ok:\n",
        "            print('\\nRunning demo...\\n')\n",
        "            model, losses = quick_demo(train_epochs=10, use_gde=False, batch_size=16)\n",
        "        else:\n",
        "            print('\\nValidation failed. Skipping demo.')\n",
        "\n",
        "    if args.demo_longitudinal:\n",
        "        # create tiny synthetic per-subject folders and run training for a few epochs\n",
        "        import tempfile\n",
        "        import shutil\n",
        "        print('Creating tiny synthetic longitudinal dataset...')\n",
        "        with tempfile.TemporaryDirectory() as tmp:\n",
        "            # create 10 subjects\n",
        "            for i in range(10):\n",
        "                sub = f\"sub_{i:03d}\"\n",
        "                d = os.path.join(tmp, sub)\n",
        "                os.makedirs(d, exist_ok=True)\n",
        "                T = 3\n",
        "                N = 12\n",
        "                F = 6\n",
        "                times = np.linspace(0.0, 2.0, T)\n",
        "                x = np.random.randn(T, N, F).astype(np.float32)\n",
        "                # ring edges\n",
        "                edges = np.array([[j for j in range(N)], [(j+1)%N for j in range(N)]], dtype=np.int64)\n",
        "                label = np.array([float(i%2)], dtype=np.float32)\n",
        "                np.save(os.path.join(d, 'times.npy'), times)\n",
        "                np.save(os.path.join(d, 'node_features.npy'), x)\n",
        "                np.save(os.path.join(d, 'edge_index.npy'), edges)\n",
        "                np.save(os.path.join(d, 'label.npy'), label)\n",
        "            ds = ADNILongitudinalDataset(tmp)\n",
        "            ds_train = torch.utils.data.Subset(ds, list(range(8)))\n",
        "            ds_val = torch.utils.data.Subset(ds, list(range(8, 10)))\n",
        "            dl_train = torch.utils.data.DataLoader(ds_train, batch_size=args.batch_size, shuffle=True, collate_fn=collate_longitudinal)\n",
        "            dl_val = torch.utils.data.DataLoader(ds_val, batch_size=args.batch_size, shuffle=False, collate_fn=collate_longitudinal)\n",
        "            device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "            sample = ds[0]\n",
        "            T, N, F = sample['x'].shape\n",
        "            model = NeuralODEGNN(in_channels=F, hidden_dim=32, num_classes=1).to(device)\n",
        "            opt = torch.optim.Adam(model.parameters(), lr=args.lr)\n",
        "            print('Training demo model...')\n",
        "            for ep in range(3):\n",
        "                loss = train_longitudinal(model, dl_train, opt, device=device)\n",
        "                auc, acc = eval_longitudinal(model, dl_val, device=device)\n",
        "                print(f'Epoch {ep}: loss={loss:.4f}, val_auc={auc:.4f}, val_acc={acc:.4f}')\n",
        "        print('Demo finished.')\n",
        "\n",
        "    if args.train_longitudinal:\n",
        "        if not args.data_dir:\n",
        "            raise ValueError('--data-dir is required for --train-longitudinal')\n",
        "        ds = ADNILongitudinalDataset(args.data_dir)\n",
        "        n = len(ds)\n",
        "        train_idx = list(range(int(0.8*n)))\n",
        "        val_idx = list(range(int(0.8*n), n))\n",
        "        dl_train = torch.utils.data.DataLoader(torch.utils.data.Subset(ds, train_idx), batch_size=args.batch_size, shuffle=True, collate_fn=collate_longitudinal)\n",
        "        dl_val = torch.utils.data.DataLoader(torch.utils.data.Subset(ds, val_idx), batch_size=args.batch_size, shuffle=False, collate_fn=collate_longitudinal)\n",
        "        device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "        sample = ds[0]\n",
        "        T, N, F = sample['x'].shape\n",
        "        model = NeuralODEGNN(in_channels=F, hidden_dim=64, num_classes=1).to(device)\n",
        "        opt = torch.optim.Adam(model.parameters(), lr=args.lr)\n",
        "        best_auc = 0.0\n",
        "        for ep in range(args.epochs):\n",
        "            loss = train_longitudinal(model, dl_train, opt, device=device)\n",
        "            auc, acc = eval_longitudinal(model, dl_val, device=device)\n",
        "            print(f'Epoch {ep}: loss={loss:.4f}, val_auc={auc:.4f}, val_acc={acc:.4f}')\n",
        "            if not np.isnan(auc) and auc > best_auc:\n",
        "                best_auc = auc\n",
        "                torch.save(model.state_dict(), 'best_longitudinal_ode_gnn.pt')\n"
      ],
      "metadata": {
        "id": "RsEEHe2QS7z9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================\n",
        "# ADNI DATASET UPLOAD & PREPARATION BLOCK (FOR GNN.py)\n",
        "# ============================================================\n",
        "#\n",
        "# WHAT THIS DATASET IS:\n",
        "# ---------------------\n",
        "# Alzheimerâ€™s Disease Neuroimaging Initiative (ADNI) resting-state fMRI data.\n",
        "# Each subject has a 4D rs-fMRI scan that is converted into ROI-level time series.\n",
        "# Each ROI is treated as a graph node; functional connectivity defines graph edges.\n",
        "#\n",
        "# The GNN model performs classification:\n",
        "#   CN (Cognitively Normal)\n",
        "#   MCI (Mild Cognitive Impairment)\n",
        "#   AD (Alzheimerâ€™s Disease)\n",
        "#\n",
        "# WHAT FORMAT THE DATA MUST BE IN:\n",
        "# --------------------------------\n",
        "# After this cell runs, the dataset will be organized as:\n",
        "#\n",
        "# data/\n",
        "# â”œâ”€â”€ timeseries/\n",
        "# â”‚   â”œâ”€â”€ sub-001.npy     # shape: (T, N_ROI)\n",
        "# â”‚   â”œâ”€â”€ sub-002.npy\n",
        "# â”œâ”€â”€ labels.csv\n",
        "#\n",
        "# labels.csv format:\n",
        "# subject_id,diagnosis\n",
        "# sub-001,CN\n",
        "# sub-002,MCI\n",
        "# sub-003,AD\n",
        "#\n",
        "# Each .npy file contains ROI time series extracted from rs-fMRI using a brain atlas.\n",
        "# ============================================================\n",
        "\n",
        "import os\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from nilearn.input_data import NiftiLabelsMasker\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "# ----------------------------\n",
        "# USER CONFIGURATION\n",
        "# ----------------------------\n",
        "RAW_FMRI_DIR = \"/path/to/adni_rs_fmri_nifti\"   # folder with ADNI rs-fMRI NIfTI files\n",
        "ATLAS_PATH  = \"/path/to/atlas.nii.gz\"          # e.g., AAL / Schaefer atlas\n",
        "LABELS_CSV  = \"/path/to/adni_labels.csv\"       # subject_id, diagnosis\n",
        "\n",
        "OUTPUT_DIR  = \"data\"\n",
        "TS_DIR      = os.path.join(OUTPUT_DIR, \"timeseries\")\n",
        "\n",
        "TR = 2.0                 # repetition time (seconds)\n",
        "MIN_TIMEPOINTS = 100     # discard very short scans\n",
        "\n",
        "os.makedirs(TS_DIR, exist_ok=True)\n",
        "\n",
        "# ----------------------------\n",
        "# LOAD SUBJECT LABELS\n",
        "# ----------------------------\n",
        "labels_df = pd.read_csv(LABELS_CSV)\n",
        "subjects = labels_df[\"subject_id\"].values\n",
        "\n",
        "# ----------------------------\n",
        "# ROI TIME-SERIES EXTRACTOR\n",
        "# ----------------------------\n",
        "masker = NiftiLabelsMasker(\n",
        "    labels_img=ATLAS_PATH,\n",
        "    standardize=True,\n",
        "    detrend=True,\n",
        "    t_r=TR\n",
        ")\n",
        "\n",
        "valid_subjects = []\n",
        "\n",
        "# ----------------------------\n",
        "# EXTRACT ROI TIME SERIES\n",
        "# ----------------------------\n",
        "for sid in subjects:\n",
        "    fmri_file = os.path.join(RAW_FMRI_DIR, f\"{sid}_bold.nii.gz\")\n",
        "\n",
        "    if not os.path.exists(fmri_file):\n",
        "        print(f\"[SKIP] Missing file for {sid}\")\n",
        "        continue\n",
        "\n",
        "    try:\n",
        "        ts = masker.fit_transform(fmri_file)   # (T, N_ROI)\n",
        "\n",
        "        if ts.shape[0] < MIN_TIMEPOINTS:\n",
        "            print(f\"[SKIP] Too few timepoints: {sid}\")\n",
        "            continue\n",
        "\n",
        "        # Final normalization (safe for GNN training)\n",
        "        ts = StandardScaler().fit_transform(ts)\n",
        "\n",
        "        np.save(os.path.join(TS_DIR, f\"{sid}.npy\"), ts)\n",
        "        valid_subjects.append(sid)\n",
        "\n",
        "        print(f\"[OK] {sid} â†’ {ts.shape}\")\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"[ERROR] {sid}: {e}\")\n",
        "\n",
        "# ----------------------------\n",
        "# SAVE FILTERED LABELS\n",
        "# ----------------------------\n",
        "labels_df = labels_df[labels_df.subject_id.isin(valid_subjects)]\n",
        "labels_df.to_csv(os.path.join(OUTPUT_DIR, \"labels.csv\"), index=False)\n",
        "\n",
        "print(\"\\nDataset ready for GNN.py\")\n",
        "print(f\"Subjects processed: {len(valid_subjects)}\")\n",
        "print(\"Data directory:\", OUTPUT_DIR)\n"
      ],
      "metadata": {
        "id": "McWiwV2tC7So"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "384f162a",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1767296209801,
          "user_tz": 360,
          "elapsed": 3823,
          "user": {
            "displayName": "Siva Ram",
            "userId": "03856573893135331823"
          }
        },
        "outputId": "758d0b36-6cc5-45fd-b847-52c1cfdf6a77"
      },
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "# ============================================================\n",
        "# CONFIGURATION (MATCHES YOUR PIPELINE)\n",
        "# ============================================================\n",
        "\n",
        "OUTPUT_DIR = \"data\"\n",
        "TS_DIR = os.path.join(OUTPUT_DIR, \"timeseries\")\n",
        "LABELS_PATH = os.path.join(OUTPUT_DIR, \"labels.csv\")\n",
        "\n",
        "N_SUBJECTS = 240\n",
        "N_ROI = 90                  # AAL-like\n",
        "MIN_TIMEPOINTS = 120\n",
        "MAX_TIMEPOINTS = 200\n",
        "\n",
        "RANDOM_SEED = 42\n",
        "np.random.seed(RANDOM_SEED)\n",
        "\n",
        "# Class proportions similar to ADNI\n",
        "CLASS_DISTRIBUTION = {\n",
        "    \"CN\": 0.4,\n",
        "    \"MCI\": 0.35,\n",
        "    \"AD\": 0.25\n",
        "}\n",
        "\n",
        "# Disease effects are intentionally subtle\n",
        "DISEASE_EFFECT_SCALE = {\n",
        "    \"CN\": 0.00,\n",
        "    \"MCI\": 0.05,\n",
        "    \"AD\": 0.10\n",
        "}\n",
        "\n",
        "NOISE_LEVEL = 0.25\n",
        "BASE_CONNECTIVITY_STRENGTH = 0.6\n",
        "\n",
        "\n",
        "# ============================================================\n",
        "# UTILITY FUNCTIONS\n",
        "# ============================================================\n",
        "\n",
        "def generate_spd_matrix(n, strength):\n",
        "    \"\"\"Generate a realistic covariance matrix shared across all subjects\"\"\"\n",
        "    A = np.random.randn(n, n)\n",
        "    cov = A @ A.T\n",
        "    D = np.sqrt(np.diag(cov))\n",
        "    corr = cov / np.outer(D, D)\n",
        "    corr = strength * corr + (1 - strength) * np.eye(n)\n",
        "    return corr\n",
        "\n",
        "\n",
        "def generate_timeseries(base_cov, T, subject_variation):\n",
        "    \"\"\"Generate subject-specific fMRI time series\"\"\"\n",
        "    cov = base_cov + subject_variation\n",
        "    ts = np.random.multivariate_normal(\n",
        "        mean=np.zeros(cov.shape[0]),\n",
        "        cov=cov,\n",
        "        size=T\n",
        "    )\n",
        "    ts += NOISE_LEVEL * np.random.randn(*ts.shape)\n",
        "    return ts\n",
        "\n",
        "\n",
        "# ============================================================\n",
        "# DATASET GENERATION\n",
        "# ============================================================\n",
        "\n",
        "def generate_dataset():\n",
        "    os.makedirs(TS_DIR, exist_ok=True)\n",
        "\n",
        "    # Shared population-level covariance (CRITICAL: prevents leakage)\n",
        "    base_cov = generate_spd_matrix(N_ROI, BASE_CONNECTIVITY_STRENGTH)\n",
        "\n",
        "    labels = []\n",
        "    subject_idx = 1\n",
        "\n",
        "    for diagnosis, frac in CLASS_DISTRIBUTION.items():\n",
        "        n_class = int(N_SUBJECTS * frac)\n",
        "\n",
        "        for _ in range(n_class):\n",
        "            sid = f\"sub-{subject_idx:03d}\"\n",
        "            subject_idx += 1\n",
        "\n",
        "            T = np.random.randint(MIN_TIMEPOINTS, MAX_TIMEPOINTS)\n",
        "\n",
        "            # Subject-specific variability dominates diagnosis effect\n",
        "            subject_variation = (\n",
        "                np.random.randn(N_ROI, N_ROI) * 0.02\n",
        "            )\n",
        "            subject_variation = (\n",
        "                subject_variation + subject_variation.T\n",
        "            ) / 2\n",
        "\n",
        "            # Subtle disease modulation (NO direct label encoding)\n",
        "            disease_noise = (\n",
        "                DISEASE_EFFECT_SCALE[diagnosis]\n",
        "                * np.random.randn(N_ROI, N_ROI)\n",
        "            )\n",
        "            disease_noise = (\n",
        "                disease_noise + disease_noise.T\n",
        "            ) / 2\n",
        "\n",
        "            ts = generate_timeseries(\n",
        "                base_cov + disease_noise,\n",
        "                T,\n",
        "                subject_variation\n",
        "            )\n",
        "\n",
        "            # EXACT SAME normalization as real pipeline\n",
        "            ts = StandardScaler().fit_transform(ts)\n",
        "\n",
        "            np.save(os.path.join(TS_DIR, f\"{sid}.npy\"), ts)\n",
        "            labels.append((sid, diagnosis))\n",
        "\n",
        "    # Save labels\n",
        "    labels_df = pd.DataFrame(labels, columns=[\"subject_id\", \"diagnosis\"])\n",
        "    labels_df.to_csv(LABELS_PATH, index=False)\n",
        "\n",
        "    print(\"Synthetic ADNI-style dataset created\")\n",
        "    print(f\"Subjects: {len(labels_df)}\")\n",
        "    print(f\"ROIs: {N_ROI}\")\n",
        "    print(f\"Saved to: {OUTPUT_DIR}\")\n",
        "\n",
        "\n",
        "# ============================================================\n",
        "# ENTRY POINT\n",
        "# ============================================================\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    generate_dataset()"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-1232474952.py:57: RuntimeWarning: covariance is not symmetric positive-semidefinite.\n",
            "  ts = np.random.multivariate_normal(\n",
            "/tmp/ipython-input-1232474952.py:57: RuntimeWarning: covariance is not symmetric positive-semidefinite.\n",
            "  ts = np.random.multivariate_normal(\n",
            "/tmp/ipython-input-1232474952.py:57: RuntimeWarning: covariance is not symmetric positive-semidefinite.\n",
            "  ts = np.random.multivariate_normal(\n",
            "/tmp/ipython-input-1232474952.py:57: RuntimeWarning: covariance is not symmetric positive-semidefinite.\n",
            "  ts = np.random.multivariate_normal(\n",
            "/tmp/ipython-input-1232474952.py:57: RuntimeWarning: covariance is not symmetric positive-semidefinite.\n",
            "  ts = np.random.multivariate_normal(\n",
            "/tmp/ipython-input-1232474952.py:57: RuntimeWarning: covariance is not symmetric positive-semidefinite.\n",
            "  ts = np.random.multivariate_normal(\n",
            "/tmp/ipython-input-1232474952.py:57: RuntimeWarning: covariance is not symmetric positive-semidefinite.\n",
            "  ts = np.random.multivariate_normal(\n",
            "/tmp/ipython-input-1232474952.py:57: RuntimeWarning: covariance is not symmetric positive-semidefinite.\n",
            "  ts = np.random.multivariate_normal(\n",
            "/tmp/ipython-input-1232474952.py:57: RuntimeWarning: covariance is not symmetric positive-semidefinite.\n",
            "  ts = np.random.multivariate_normal(\n",
            "/tmp/ipython-input-1232474952.py:57: RuntimeWarning: covariance is not symmetric positive-semidefinite.\n",
            "  ts = np.random.multivariate_normal(\n",
            "/tmp/ipython-input-1232474952.py:57: RuntimeWarning: covariance is not symmetric positive-semidefinite.\n",
            "  ts = np.random.multivariate_normal(\n",
            "/tmp/ipython-input-1232474952.py:57: RuntimeWarning: covariance is not symmetric positive-semidefinite.\n",
            "  ts = np.random.multivariate_normal(\n",
            "/tmp/ipython-input-1232474952.py:57: RuntimeWarning: covariance is not symmetric positive-semidefinite.\n",
            "  ts = np.random.multivariate_normal(\n",
            "/tmp/ipython-input-1232474952.py:57: RuntimeWarning: covariance is not symmetric positive-semidefinite.\n",
            "  ts = np.random.multivariate_normal(\n",
            "/tmp/ipython-input-1232474952.py:57: RuntimeWarning: covariance is not symmetric positive-semidefinite.\n",
            "  ts = np.random.multivariate_normal(\n",
            "/tmp/ipython-input-1232474952.py:57: RuntimeWarning: covariance is not symmetric positive-semidefinite.\n",
            "  ts = np.random.multivariate_normal(\n",
            "/tmp/ipython-input-1232474952.py:57: RuntimeWarning: covariance is not symmetric positive-semidefinite.\n",
            "  ts = np.random.multivariate_normal(\n",
            "/tmp/ipython-input-1232474952.py:57: RuntimeWarning: covariance is not symmetric positive-semidefinite.\n",
            "  ts = np.random.multivariate_normal(\n",
            "/tmp/ipython-input-1232474952.py:57: RuntimeWarning: covariance is not symmetric positive-semidefinite.\n",
            "  ts = np.random.multivariate_normal(\n",
            "/tmp/ipython-input-1232474952.py:57: RuntimeWarning: covariance is not symmetric positive-semidefinite.\n",
            "  ts = np.random.multivariate_normal(\n",
            "/tmp/ipython-input-1232474952.py:57: RuntimeWarning: covariance is not symmetric positive-semidefinite.\n",
            "  ts = np.random.multivariate_normal(\n",
            "/tmp/ipython-input-1232474952.py:57: RuntimeWarning: covariance is not symmetric positive-semidefinite.\n",
            "  ts = np.random.multivariate_normal(\n",
            "/tmp/ipython-input-1232474952.py:57: RuntimeWarning: covariance is not symmetric positive-semidefinite.\n",
            "  ts = np.random.multivariate_normal(\n",
            "/tmp/ipython-input-1232474952.py:57: RuntimeWarning: covariance is not symmetric positive-semidefinite.\n",
            "  ts = np.random.multivariate_normal(\n",
            "/tmp/ipython-input-1232474952.py:57: RuntimeWarning: covariance is not symmetric positive-semidefinite.\n",
            "  ts = np.random.multivariate_normal(\n",
            "/tmp/ipython-input-1232474952.py:57: RuntimeWarning: covariance is not symmetric positive-semidefinite.\n",
            "  ts = np.random.multivariate_normal(\n",
            "/tmp/ipython-input-1232474952.py:57: RuntimeWarning: covariance is not symmetric positive-semidefinite.\n",
            "  ts = np.random.multivariate_normal(\n",
            "/tmp/ipython-input-1232474952.py:57: RuntimeWarning: covariance is not symmetric positive-semidefinite.\n",
            "  ts = np.random.multivariate_normal(\n",
            "/tmp/ipython-input-1232474952.py:57: RuntimeWarning: covariance is not symmetric positive-semidefinite.\n",
            "  ts = np.random.multivariate_normal(\n",
            "/tmp/ipython-input-1232474952.py:57: RuntimeWarning: covariance is not symmetric positive-semidefinite.\n",
            "  ts = np.random.multivariate_normal(\n",
            "/tmp/ipython-input-1232474952.py:57: RuntimeWarning: covariance is not symmetric positive-semidefinite.\n",
            "  ts = np.random.multivariate_normal(\n",
            "/tmp/ipython-input-1232474952.py:57: RuntimeWarning: covariance is not symmetric positive-semidefinite.\n",
            "  ts = np.random.multivariate_normal(\n",
            "/tmp/ipython-input-1232474952.py:57: RuntimeWarning: covariance is not symmetric positive-semidefinite.\n",
            "  ts = np.random.multivariate_normal(\n",
            "/tmp/ipython-input-1232474952.py:57: RuntimeWarning: covariance is not symmetric positive-semidefinite.\n",
            "  ts = np.random.multivariate_normal(\n",
            "/tmp/ipython-input-1232474952.py:57: RuntimeWarning: covariance is not symmetric positive-semidefinite.\n",
            "  ts = np.random.multivariate_normal(\n",
            "/tmp/ipython-input-1232474952.py:57: RuntimeWarning: covariance is not symmetric positive-semidefinite.\n",
            "  ts = np.random.multivariate_normal(\n",
            "/tmp/ipython-input-1232474952.py:57: RuntimeWarning: covariance is not symmetric positive-semidefinite.\n",
            "  ts = np.random.multivariate_normal(\n",
            "/tmp/ipython-input-1232474952.py:57: RuntimeWarning: covariance is not symmetric positive-semidefinite.\n",
            "  ts = np.random.multivariate_normal(\n",
            "/tmp/ipython-input-1232474952.py:57: RuntimeWarning: covariance is not symmetric positive-semidefinite.\n",
            "  ts = np.random.multivariate_normal(\n",
            "/tmp/ipython-input-1232474952.py:57: RuntimeWarning: covariance is not symmetric positive-semidefinite.\n",
            "  ts = np.random.multivariate_normal(\n",
            "/tmp/ipython-input-1232474952.py:57: RuntimeWarning: covariance is not symmetric positive-semidefinite.\n",
            "  ts = np.random.multivariate_normal(\n",
            "/tmp/ipython-input-1232474952.py:57: RuntimeWarning: covariance is not symmetric positive-semidefinite.\n",
            "  ts = np.random.multivariate_normal(\n",
            "/tmp/ipython-input-1232474952.py:57: RuntimeWarning: covariance is not symmetric positive-semidefinite.\n",
            "  ts = np.random.multivariate_normal(\n",
            "/tmp/ipython-input-1232474952.py:57: RuntimeWarning: covariance is not symmetric positive-semidefinite.\n",
            "  ts = np.random.multivariate_normal(\n",
            "/tmp/ipython-input-1232474952.py:57: RuntimeWarning: covariance is not symmetric positive-semidefinite.\n",
            "  ts = np.random.multivariate_normal(\n",
            "/tmp/ipython-input-1232474952.py:57: RuntimeWarning: covariance is not symmetric positive-semidefinite.\n",
            "  ts = np.random.multivariate_normal(\n",
            "/tmp/ipython-input-1232474952.py:57: RuntimeWarning: covariance is not symmetric positive-semidefinite.\n",
            "  ts = np.random.multivariate_normal(\n",
            "/tmp/ipython-input-1232474952.py:57: RuntimeWarning: covariance is not symmetric positive-semidefinite.\n",
            "  ts = np.random.multivariate_normal(\n",
            "/tmp/ipython-input-1232474952.py:57: RuntimeWarning: covariance is not symmetric positive-semidefinite.\n",
            "  ts = np.random.multivariate_normal(\n",
            "/tmp/ipython-input-1232474952.py:57: RuntimeWarning: covariance is not symmetric positive-semidefinite.\n",
            "  ts = np.random.multivariate_normal(\n",
            "/tmp/ipython-input-1232474952.py:57: RuntimeWarning: covariance is not symmetric positive-semidefinite.\n",
            "  ts = np.random.multivariate_normal(\n",
            "/tmp/ipython-input-1232474952.py:57: RuntimeWarning: covariance is not symmetric positive-semidefinite.\n",
            "  ts = np.random.multivariate_normal(\n",
            "/tmp/ipython-input-1232474952.py:57: RuntimeWarning: covariance is not symmetric positive-semidefinite.\n",
            "  ts = np.random.multivariate_normal(\n",
            "/tmp/ipython-input-1232474952.py:57: RuntimeWarning: covariance is not symmetric positive-semidefinite.\n",
            "  ts = np.random.multivariate_normal(\n",
            "/tmp/ipython-input-1232474952.py:57: RuntimeWarning: covariance is not symmetric positive-semidefinite.\n",
            "  ts = np.random.multivariate_normal(\n",
            "/tmp/ipython-input-1232474952.py:57: RuntimeWarning: covariance is not symmetric positive-semidefinite.\n",
            "  ts = np.random.multivariate_normal(\n",
            "/tmp/ipython-input-1232474952.py:57: RuntimeWarning: covariance is not symmetric positive-semidefinite.\n",
            "  ts = np.random.multivariate_normal(\n",
            "/tmp/ipython-input-1232474952.py:57: RuntimeWarning: covariance is not symmetric positive-semidefinite.\n",
            "  ts = np.random.multivariate_normal(\n",
            "/tmp/ipython-input-1232474952.py:57: RuntimeWarning: covariance is not symmetric positive-semidefinite.\n",
            "  ts = np.random.multivariate_normal(\n",
            "/tmp/ipython-input-1232474952.py:57: RuntimeWarning: covariance is not symmetric positive-semidefinite.\n",
            "  ts = np.random.multivariate_normal(\n",
            "/tmp/ipython-input-1232474952.py:57: RuntimeWarning: covariance is not symmetric positive-semidefinite.\n",
            "  ts = np.random.multivariate_normal(\n",
            "/tmp/ipython-input-1232474952.py:57: RuntimeWarning: covariance is not symmetric positive-semidefinite.\n",
            "  ts = np.random.multivariate_normal(\n",
            "/tmp/ipython-input-1232474952.py:57: RuntimeWarning: covariance is not symmetric positive-semidefinite.\n",
            "  ts = np.random.multivariate_normal(\n",
            "/tmp/ipython-input-1232474952.py:57: RuntimeWarning: covariance is not symmetric positive-semidefinite.\n",
            "  ts = np.random.multivariate_normal(\n",
            "/tmp/ipython-input-1232474952.py:57: RuntimeWarning: covariance is not symmetric positive-semidefinite.\n",
            "  ts = np.random.multivariate_normal(\n",
            "/tmp/ipython-input-1232474952.py:57: RuntimeWarning: covariance is not symmetric positive-semidefinite.\n",
            "  ts = np.random.multivariate_normal(\n",
            "/tmp/ipython-input-1232474952.py:57: RuntimeWarning: covariance is not symmetric positive-semidefinite.\n",
            "  ts = np.random.multivariate_normal(\n",
            "/tmp/ipython-input-1232474952.py:57: RuntimeWarning: covariance is not symmetric positive-semidefinite.\n",
            "  ts = np.random.multivariate_normal(\n",
            "/tmp/ipython-input-1232474952.py:57: RuntimeWarning: covariance is not symmetric positive-semidefinite.\n",
            "  ts = np.random.multivariate_normal(\n",
            "/tmp/ipython-input-1232474952.py:57: RuntimeWarning: covariance is not symmetric positive-semidefinite.\n",
            "  ts = np.random.multivariate_normal(\n",
            "/tmp/ipython-input-1232474952.py:57: RuntimeWarning: covariance is not symmetric positive-semidefinite.\n",
            "  ts = np.random.multivariate_normal(\n",
            "/tmp/ipython-input-1232474952.py:57: RuntimeWarning: covariance is not symmetric positive-semidefinite.\n",
            "  ts = np.random.multivariate_normal(\n",
            "/tmp/ipython-input-1232474952.py:57: RuntimeWarning: covariance is not symmetric positive-semidefinite.\n",
            "  ts = np.random.multivariate_normal(\n",
            "/tmp/ipython-input-1232474952.py:57: RuntimeWarning: covariance is not symmetric positive-semidefinite.\n",
            "  ts = np.random.multivariate_normal(\n",
            "/tmp/ipython-input-1232474952.py:57: RuntimeWarning: covariance is not symmetric positive-semidefinite.\n",
            "  ts = np.random.multivariate_normal(\n",
            "/tmp/ipython-input-1232474952.py:57: RuntimeWarning: covariance is not symmetric positive-semidefinite.\n",
            "  ts = np.random.multivariate_normal(\n",
            "/tmp/ipython-input-1232474952.py:57: RuntimeWarning: covariance is not symmetric positive-semidefinite.\n",
            "  ts = np.random.multivariate_normal(\n",
            "/tmp/ipython-input-1232474952.py:57: RuntimeWarning: covariance is not symmetric positive-semidefinite.\n",
            "  ts = np.random.multivariate_normal(\n",
            "/tmp/ipython-input-1232474952.py:57: RuntimeWarning: covariance is not symmetric positive-semidefinite.\n",
            "  ts = np.random.multivariate_normal(\n",
            "/tmp/ipython-input-1232474952.py:57: RuntimeWarning: covariance is not symmetric positive-semidefinite.\n",
            "  ts = np.random.multivariate_normal(\n",
            "/tmp/ipython-input-1232474952.py:57: RuntimeWarning: covariance is not symmetric positive-semidefinite.\n",
            "  ts = np.random.multivariate_normal(\n",
            "/tmp/ipython-input-1232474952.py:57: RuntimeWarning: covariance is not symmetric positive-semidefinite.\n",
            "  ts = np.random.multivariate_normal(\n",
            "/tmp/ipython-input-1232474952.py:57: RuntimeWarning: covariance is not symmetric positive-semidefinite.\n",
            "  ts = np.random.multivariate_normal(\n",
            "/tmp/ipython-input-1232474952.py:57: RuntimeWarning: covariance is not symmetric positive-semidefinite.\n",
            "  ts = np.random.multivariate_normal(\n",
            "/tmp/ipython-input-1232474952.py:57: RuntimeWarning: covariance is not symmetric positive-semidefinite.\n",
            "  ts = np.random.multivariate_normal(\n",
            "/tmp/ipython-input-1232474952.py:57: RuntimeWarning: covariance is not symmetric positive-semidefinite.\n",
            "  ts = np.random.multivariate_normal(\n",
            "/tmp/ipython-input-1232474952.py:57: RuntimeWarning: covariance is not symmetric positive-semidefinite.\n",
            "  ts = np.random.multivariate_normal(\n",
            "/tmp/ipython-input-1232474952.py:57: RuntimeWarning: covariance is not symmetric positive-semidefinite.\n",
            "  ts = np.random.multivariate_normal(\n",
            "/tmp/ipython-input-1232474952.py:57: RuntimeWarning: covariance is not symmetric positive-semidefinite.\n",
            "  ts = np.random.multivariate_normal(\n",
            "/tmp/ipython-input-1232474952.py:57: RuntimeWarning: covariance is not symmetric positive-semidefinite.\n",
            "  ts = np.random.multivariate_normal(\n",
            "/tmp/ipython-input-1232474952.py:57: RuntimeWarning: covariance is not symmetric positive-semidefinite.\n",
            "  ts = np.random.multivariate_normal(\n",
            "/tmp/ipython-input-1232474952.py:57: RuntimeWarning: covariance is not symmetric positive-semidefinite.\n",
            "  ts = np.random.multivariate_normal(\n",
            "/tmp/ipython-input-1232474952.py:57: RuntimeWarning: covariance is not symmetric positive-semidefinite.\n",
            "  ts = np.random.multivariate_normal(\n",
            "/tmp/ipython-input-1232474952.py:57: RuntimeWarning: covariance is not symmetric positive-semidefinite.\n",
            "  ts = np.random.multivariate_normal(\n",
            "/tmp/ipython-input-1232474952.py:57: RuntimeWarning: covariance is not symmetric positive-semidefinite.\n",
            "  ts = np.random.multivariate_normal(\n",
            "/tmp/ipython-input-1232474952.py:57: RuntimeWarning: covariance is not symmetric positive-semidefinite.\n",
            "  ts = np.random.multivariate_normal(\n",
            "/tmp/ipython-input-1232474952.py:57: RuntimeWarning: covariance is not symmetric positive-semidefinite.\n",
            "  ts = np.random.multivariate_normal(\n",
            "/tmp/ipython-input-1232474952.py:57: RuntimeWarning: covariance is not symmetric positive-semidefinite.\n",
            "  ts = np.random.multivariate_normal(\n",
            "/tmp/ipython-input-1232474952.py:57: RuntimeWarning: covariance is not symmetric positive-semidefinite.\n",
            "  ts = np.random.multivariate_normal(\n",
            "/tmp/ipython-input-1232474952.py:57: RuntimeWarning: covariance is not symmetric positive-semidefinite.\n",
            "  ts = np.random.multivariate_normal(\n",
            "/tmp/ipython-input-1232474952.py:57: RuntimeWarning: covariance is not symmetric positive-semidefinite.\n",
            "  ts = np.random.multivariate_normal(\n",
            "/tmp/ipython-input-1232474952.py:57: RuntimeWarning: covariance is not symmetric positive-semidefinite.\n",
            "  ts = np.random.multivariate_normal(\n",
            "/tmp/ipython-input-1232474952.py:57: RuntimeWarning: covariance is not symmetric positive-semidefinite.\n",
            "  ts = np.random.multivariate_normal(\n",
            "/tmp/ipython-input-1232474952.py:57: RuntimeWarning: covariance is not symmetric positive-semidefinite.\n",
            "  ts = np.random.multivariate_normal(\n",
            "/tmp/ipython-input-1232474952.py:57: RuntimeWarning: covariance is not symmetric positive-semidefinite.\n",
            "  ts = np.random.multivariate_normal(\n",
            "/tmp/ipython-input-1232474952.py:57: RuntimeWarning: covariance is not symmetric positive-semidefinite.\n",
            "  ts = np.random.multivariate_normal(\n",
            "/tmp/ipython-input-1232474952.py:57: RuntimeWarning: covariance is not symmetric positive-semidefinite.\n",
            "  ts = np.random.multivariate_normal(\n",
            "/tmp/ipython-input-1232474952.py:57: RuntimeWarning: covariance is not symmetric positive-semidefinite.\n",
            "  ts = np.random.multivariate_normal(\n",
            "/tmp/ipython-input-1232474952.py:57: RuntimeWarning: covariance is not symmetric positive-semidefinite.\n",
            "  ts = np.random.multivariate_normal(\n",
            "/tmp/ipython-input-1232474952.py:57: RuntimeWarning: covariance is not symmetric positive-semidefinite.\n",
            "  ts = np.random.multivariate_normal(\n",
            "/tmp/ipython-input-1232474952.py:57: RuntimeWarning: covariance is not symmetric positive-semidefinite.\n",
            "  ts = np.random.multivariate_normal(\n",
            "/tmp/ipython-input-1232474952.py:57: RuntimeWarning: covariance is not symmetric positive-semidefinite.\n",
            "  ts = np.random.multivariate_normal(\n",
            "/tmp/ipython-input-1232474952.py:57: RuntimeWarning: covariance is not symmetric positive-semidefinite.\n",
            "  ts = np.random.multivariate_normal(\n",
            "/tmp/ipython-input-1232474952.py:57: RuntimeWarning: covariance is not symmetric positive-semidefinite.\n",
            "  ts = np.random.multivariate_normal(\n",
            "/tmp/ipython-input-1232474952.py:57: RuntimeWarning: covariance is not symmetric positive-semidefinite.\n",
            "  ts = np.random.multivariate_normal(\n",
            "/tmp/ipython-input-1232474952.py:57: RuntimeWarning: covariance is not symmetric positive-semidefinite.\n",
            "  ts = np.random.multivariate_normal(\n",
            "/tmp/ipython-input-1232474952.py:57: RuntimeWarning: covariance is not symmetric positive-semidefinite.\n",
            "  ts = np.random.multivariate_normal(\n",
            "/tmp/ipython-input-1232474952.py:57: RuntimeWarning: covariance is not symmetric positive-semidefinite.\n",
            "  ts = np.random.multivariate_normal(\n",
            "/tmp/ipython-input-1232474952.py:57: RuntimeWarning: covariance is not symmetric positive-semidefinite.\n",
            "  ts = np.random.multivariate_normal(\n",
            "/tmp/ipython-input-1232474952.py:57: RuntimeWarning: covariance is not symmetric positive-semidefinite.\n",
            "  ts = np.random.multivariate_normal(\n",
            "/tmp/ipython-input-1232474952.py:57: RuntimeWarning: covariance is not symmetric positive-semidefinite.\n",
            "  ts = np.random.multivariate_normal(\n",
            "/tmp/ipython-input-1232474952.py:57: RuntimeWarning: covariance is not symmetric positive-semidefinite.\n",
            "  ts = np.random.multivariate_normal(\n",
            "/tmp/ipython-input-1232474952.py:57: RuntimeWarning: covariance is not symmetric positive-semidefinite.\n",
            "  ts = np.random.multivariate_normal(\n",
            "/tmp/ipython-input-1232474952.py:57: RuntimeWarning: covariance is not symmetric positive-semidefinite.\n",
            "  ts = np.random.multivariate_normal(\n",
            "/tmp/ipython-input-1232474952.py:57: RuntimeWarning: covariance is not symmetric positive-semidefinite.\n",
            "  ts = np.random.multivariate_normal(\n",
            "/tmp/ipython-input-1232474952.py:57: RuntimeWarning: covariance is not symmetric positive-semidefinite.\n",
            "  ts = np.random.multivariate_normal(\n",
            "/tmp/ipython-input-1232474952.py:57: RuntimeWarning: covariance is not symmetric positive-semidefinite.\n",
            "  ts = np.random.multivariate_normal(\n",
            "/tmp/ipython-input-1232474952.py:57: RuntimeWarning: covariance is not symmetric positive-semidefinite.\n",
            "  ts = np.random.multivariate_normal(\n",
            "/tmp/ipython-input-1232474952.py:57: RuntimeWarning: covariance is not symmetric positive-semidefinite.\n",
            "  ts = np.random.multivariate_normal(\n",
            "/tmp/ipython-input-1232474952.py:57: RuntimeWarning: covariance is not symmetric positive-semidefinite.\n",
            "  ts = np.random.multivariate_normal(\n",
            "/tmp/ipython-input-1232474952.py:57: RuntimeWarning: covariance is not symmetric positive-semidefinite.\n",
            "  ts = np.random.multivariate_normal(\n",
            "/tmp/ipython-input-1232474952.py:57: RuntimeWarning: covariance is not symmetric positive-semidefinite.\n",
            "  ts = np.random.multivariate_normal(\n",
            "/tmp/ipython-input-1232474952.py:57: RuntimeWarning: covariance is not symmetric positive-semidefinite.\n",
            "  ts = np.random.multivariate_normal(\n",
            "/tmp/ipython-input-1232474952.py:57: RuntimeWarning: covariance is not symmetric positive-semidefinite.\n",
            "  ts = np.random.multivariate_normal(\n",
            "/tmp/ipython-input-1232474952.py:57: RuntimeWarning: covariance is not symmetric positive-semidefinite.\n",
            "  ts = np.random.multivariate_normal(\n",
            "/tmp/ipython-input-1232474952.py:57: RuntimeWarning: covariance is not symmetric positive-semidefinite.\n",
            "  ts = np.random.multivariate_normal(\n",
            "/tmp/ipython-input-1232474952.py:57: RuntimeWarning: covariance is not symmetric positive-semidefinite.\n",
            "  ts = np.random.multivariate_normal(\n",
            "/tmp/ipython-input-1232474952.py:57: RuntimeWarning: covariance is not symmetric positive-semidefinite.\n",
            "  ts = np.random.multivariate_normal(\n",
            "/tmp/ipython-input-1232474952.py:57: RuntimeWarning: covariance is not symmetric positive-semidefinite.\n",
            "  ts = np.random.multivariate_normal(\n",
            "/tmp/ipython-input-1232474952.py:57: RuntimeWarning: covariance is not symmetric positive-semidefinite.\n",
            "  ts = np.random.multivariate_normal(\n",
            "/tmp/ipython-input-1232474952.py:57: RuntimeWarning: covariance is not symmetric positive-semidefinite.\n",
            "  ts = np.random.multivariate_normal(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Synthetic ADNI-style dataset created\n",
            "Subjects: 240\n",
            "ROIs: 90\n",
            "Saved to: data\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-1232474952.py:57: RuntimeWarning: covariance is not symmetric positive-semidefinite.\n",
            "  ts = np.random.multivariate_normal(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================\n",
        "# SYNTHETIC ADNI DATASET LOADER (FOR GNN.py)\n",
        "# ============================================================\n",
        "#\n",
        "# This block assumes synthetic data has already been generated as:\n",
        "#\n",
        "# data/\n",
        "# â”œâ”€â”€ timeseries/\n",
        "# â”‚   â”œâ”€â”€ sub-001.npy   # shape: (T, N_ROI)\n",
        "# â”‚   â”œâ”€â”€ sub-002.npy\n",
        "# â””â”€â”€ labels.csv\n",
        "#\n",
        "# NO NIFTI FILES\n",
        "# NO ATLAS\n",
        "# NO NILEARN\n",
        "# ============================================================\n",
        "\n",
        "import os\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "# ----------------------------\n",
        "# DATA LOCATION\n",
        "# ----------------------------\n",
        "DATA_DIR = \"data\"\n",
        "TS_DIR = os.path.join(DATA_DIR, \"timeseries\")\n",
        "LABELS_CSV = os.path.join(DATA_DIR, \"labels.csv\")\n",
        "\n",
        "# ----------------------------\n",
        "# SANITY CHECKS (IMPORTANT)\n",
        "# ----------------------------\n",
        "assert os.path.exists(TS_DIR), \"âŒ timeseries directory not found\"\n",
        "assert os.path.exists(LABELS_CSV), \"âŒ labels.csv not found\"\n",
        "\n",
        "labels_df = pd.read_csv(LABELS_CSV)\n",
        "subjects = labels_df[\"subject_id\"].values\n",
        "\n",
        "print(f\"[OK] Found {len(subjects)} subjects\")\n",
        "\n",
        "# ----------------------------\n",
        "# QUICK DATA VALIDATION\n",
        "# ----------------------------\n",
        "example_ts = np.load(os.path.join(TS_DIR, f\"{subjects[0]}.npy\"))\n",
        "\n",
        "print(f\"[CHECK] Example timeseries shape: {example_ts.shape}\")\n",
        "print(f\"[CHECK] Mean: {example_ts.mean():.2e}, Std: {example_ts.std():.2f}\")\n",
        "\n",
        "print(\"\\nSynthetic dataset ready for GNN.py\")\n"
      ],
      "metadata": {
        "id": "i421c4vLFg7T"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a2483634",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1767317618223,
          "user_tz": 360,
          "elapsed": 573738,
          "user": {
            "displayName": "Siva Ram",
            "userId": "03856573893135331823"
          }
        },
        "outputId": "89285fae-31e6-428e-944b-9f5bb640be18"
      },
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from typing import Optional, Union, Dict, Tuple, List, Any\n",
        "\n",
        "# NOTE: DDGModel and its sub-components, as well as compute_losses,\n",
        "# are now expected to be available from the first cell's execution scope.\n",
        "# The definitions are removed from this cell to avoid duplication.\n",
        "\n",
        "# ============================================================\n",
        "# SYNTHETIC DATASET CONFIGURATION (COPIED FROM CELL 384f162a)\n",
        "# ============================================================\n",
        "\n",
        "# OUTPUT_DIR is now local to this script\n",
        "OUTPUT_DIR_SYNTH = \"data\"\n",
        "TS_DIR_SYNTH = os.path.join(OUTPUT_DIR_SYNTH, \"timeseries\")\n",
        "LABELS_PATH_SYNTH = os.path.join(OUTPUT_DIR_SYNTH, \"labels.csv\")\n",
        "\n",
        "N_SUBJECTS = 240\n",
        "N_ROI = 90                  # AAL-like\n",
        "MIN_TIMEPOINTS = 120\n",
        "MAX_TIMEPOINTS = 200\n",
        "\n",
        "RANDOM_SEED_SYNTH = 42\n",
        "np.random.seed(RANDOM_SEED_SYNTH)\n",
        "\n",
        "# Class proportions similar to ADNI\n",
        "CLASS_DISTRIBUTION = {\n",
        "    \"CN\": 0.4,\n",
        "    \"MCI\": 0.35,\n",
        "    \"AD\": 0.25\n",
        "}\n",
        "\n",
        "# Disease effects are intentionally subtle\n",
        "DISEASE_EFFECT_SCALE = {\n",
        "    \"CN\": 0.00,\n",
        "    \"MCI\": 0.05,\n",
        "    \"AD\": 0.10\n",
        "}\n",
        "\n",
        "NOISE_LEVEL = 0.25\n",
        "BASE_CONNECTIVITY_STRENGTH = 0.6\n",
        "\n",
        "\n",
        "# ============================================================\n",
        "# SYNTHETIC DATA UTILITY FUNCTIONS (COPIED FROM CELL 384f162a)\n",
        "# ============================================================\n",
        "\n",
        "def generate_spd_matrix(n, strength):\n",
        "    \"\"\"Generate a realistic covariance matrix shared across all subjects\"\"\"\n",
        "    A = np.random.randn(n, n)\n",
        "    cov = A @ A.T\n",
        "    D = np.sqrt(np.diag(cov))\n",
        "    corr = cov / np.outer(D, D)\n",
        "    corr = strength * corr + (1 - strength) * np.eye(n)\n",
        "    return corr\n",
        "\n",
        "def generate_timeseries(base_cov, T, subject_variation):\n",
        "    \"\"\"Generate subject-specific fMRI time series\"\"\"\n",
        "    cov = base_cov + subject_variation\n",
        "    ts = np.random.multivariate_normal(\n",
        "        mean=np.zeros(cov.shape[0]),\n",
        "        cov=cov,\n",
        "        size=T\n",
        "    )\n",
        "    ts += NOISE_LEVEL * np.random.randn(*ts.shape)\n",
        "    return ts\n",
        "\n",
        "# ============================================================\n",
        "# SYNTHETIC DATASET GENERATION FUNCTION (COPIED FROM CELL 384f162a)\n",
        "# ============================================================\n",
        "\n",
        "def generate_dataset():\n",
        "    os.makedirs(TS_DIR_SYNTH, exist_ok=True)\n",
        "\n",
        "    # Shared population-level covariance (CRITICAL: prevents leakage)\n",
        "    base_cov = generate_spd_matrix(N_ROI, BASE_CONNECTIVITY_STRENGTH)\n",
        "\n",
        "    labels = []\n",
        "    subject_idx = 1\n",
        "\n",
        "    for diagnosis, frac in CLASS_DISTRIBUTION.items():\n",
        "        n_class = int(N_SUBJECTS * frac)\n",
        "\n",
        "        for _ in range(n_class):\n",
        "            sid = f\"sub-{subject_idx:03d}\"\n",
        "            subject_idx += 1\n",
        "\n",
        "            T = np.random.randint(MIN_TIMEPOINTS, MAX_TIMEPOINTS)\n",
        "\n",
        "            # Subject-specific variability dominates diagnosis effect\n",
        "            subject_variation = (\n",
        "                np.random.randn(N_ROI, N_ROI) * 0.02\n",
        "            )\n",
        "            subject_variation = (\n",
        "                subject_variation + subject_variation.T\n",
        "            ) / 2\n",
        "\n",
        "            # Subtle disease modulation (NO direct label encoding)\n",
        "            disease_noise = (\n",
        "                DISEASE_EFFECT_SCALE[diagnosis]\n",
        "                * np.random.randn(N_ROI, N_ROI)\n",
        "            )\n",
        "            disease_noise = (\n",
        "                disease_noise + disease_noise.T\n",
        "            ) / 2\n",
        "\n",
        "            ts = generate_timeseries(\n",
        "                base_cov + disease_noise,\n",
        "                T,\n",
        "                subject_variation\n",
        "            )\n",
        "\n",
        "            # EXACT SAME normalization as real pipeline\n",
        "            ts = StandardScaler().fit_transform(ts)\n",
        "\n",
        "            np.save(os.path.join(TS_DIR_SYNTH, f\"{sid}.npy\"), ts)\n",
        "            labels.append((sid, diagnosis))\n",
        "\n",
        "    # Save labels\n",
        "    labels_df = pd.DataFrame(labels, columns=[\"subject_id\", \"diagnosis\"])\n",
        "    labels_df.to_csv(LABELS_PATH_SYNTH, index=False)\n",
        "\n",
        "    print(\"Synthetic ADNI-style dataset created\")\n",
        "    print(f\"Subjects: {len(labels_df)}\")\n",
        "    print(f\"ROIs: {N_ROI}\")\n",
        "    print(f\"Saved to: {OUTPUT_DIR_SYNTH}\")\n",
        "\n",
        "\n",
        "# --- Custom collate_fn for DataLoader ---\n",
        "def collate_batch_for_ddg(batch: list):\n",
        "    # Assumes each item in batch already has 'node_features' and 'adjacency_matrices' with T_longitudinal=1\n",
        "    # Shape: (1, N, F) for node_features and (1, N, N) for adjacency_matrices\n",
        "\n",
        "    node_features_batch = []\n",
        "    adjacency_matrices_batch = []\n",
        "    cognitive_scores_batch = []\n",
        "\n",
        "    for item in batch:\n",
        "        # item['node_features'] is already (1, N, F)\n",
        "        node_features_batch.append(item['node_features'])\n",
        "        # item['adjacency_matrices'] is already (1, N, N)\n",
        "        adjacency_matrices_batch.append(item['adjacency_matrices'])\n",
        "        cognitive_scores_batch.append(item['cognitive_scores'])\n",
        "\n",
        "    # Stack along a new batch dimension, resulting in (B, 1, N, F) and (B, 1, N, N)\n",
        "    node_features_tensor = torch.stack(node_features_batch, dim=0)\n",
        "    adjacency_matrices_tensor = torch.stack(adjacency_matrices_batch, dim=0)\n",
        "    cognitive_scores_tensor = torch.tensor(cognitive_scores_batch, dtype=torch.float32)\n",
        "\n",
        "    return {\n",
        "        'node_features': node_features_tensor,\n",
        "        'adjacency_matrices': adjacency_matrices_tensor,\n",
        "        'cognitive_scores': cognitive_scores_tensor\n",
        "    }\n",
        "\n",
        "# --- Custom Dataset for Synthetic Data ---\n",
        "class CustomSyntheticDDGDataset(Dataset):\n",
        "    def __init__(self, ts_dir, labels_csv):\n",
        "        self.ts_dir = ts_dir\n",
        "        self.labels_df = pd.read_csv(labels_csv)\n",
        "        self.subject_ids = self.labels_df[\"subject_id\"].values\n",
        "        self.label_map = {\"CN\": 0, \"MCI\": 1, \"AD\": 2}\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.subject_ids)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        sid = self.subject_ids[idx]\n",
        "        diagnosis_str = self.labels_df.iloc[idx][\"diagnosis\"]\n",
        "\n",
        "        ts_path = os.path.join(self.ts_dir, f\"{sid}.npy\")\n",
        "        raw_timeseries = np.load(ts_path)\n",
        "\n",
        "        # These functions are now expected to be available from the first cell\n",
        "        node_feats_intra_session = node_features_from_timeseries(raw_timeseries)\n",
        "        adj_intra_session = compute_empirical_fc_from_timeseries(raw_timeseries)\n",
        "\n",
        "        # For DDGModel, we treat each synthetic sample as a single longitudinal time point (T_longitudinal=1)\n",
        "        # So we take the features at index 0, and ensure it's (1, N, F) and (1, N, N)\n",
        "        final_node_features = torch.tensor(node_feats_intra_session[0], dtype=torch.float32).unsqueeze(0)\n",
        "        final_adjacency_matrices = torch.tensor(adj_intra_session, dtype=torch.float32).unsqueeze(0)\n",
        "\n",
        "        if diagnosis_str == \"CN\":\n",
        "            cognitive_score = 28.0 + np.random.rand() * 2.0\n",
        "        elif diagnosis_str == \"MCI\":\n",
        "            cognitive_score = 18.0 + np.random.rand() * 7.0\n",
        "        else:\n",
        "            cognitive_score = 5.0 + np.random.rand() * 5.0\n",
        "        cognitive_score = float(cognitive_score)\n",
        "\n",
        "        return {\n",
        "            'node_features': final_node_features, # (1, N, F)\n",
        "            'adjacency_matrices': final_adjacency_matrices, # (1, N, N)\n",
        "            'cognitive_scores': cognitive_score\n",
        "        }\n",
        "\n",
        "# --- Main execution function to train DDGModel on synthetic data ---\n",
        "def run_ddg_on_synthetic_data(epochs=30, batch_size=16, lr=1e-3, device=None):\n",
        "    device = device or (torch.device(\"cuda\") if torch.cuda.is_available() else \"cpu\")\n",
        "    print(f\"\\nRunning DDGModel on synthetic data on device {device}\")\n",
        "    print(\"=\"*80)\n",
        "\n",
        "    TS_DIR = TS_DIR_SYNTH\n",
        "    LABELS_CSV = LABELS_PATH_SYNTH\n",
        "\n",
        "    generate_dataset()\n",
        "\n",
        "    full_dataset = CustomSyntheticDDGDataset(TS_DIR, LABELS_CSV)\n",
        "\n",
        "    total_subjects = len(full_dataset)\n",
        "    train_size = int(0.8 * total_subjects)\n",
        "    val_size = total_subjects - train_size\n",
        "    torch.manual_seed(42)\n",
        "    train_dataset, val_dataset = torch.utils.data.random_split(full_dataset, [train_size, val_size])\n",
        "\n",
        "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, collate_fn=collate_batch_for_ddg)\n",
        "    val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False, collate_fn=collate_batch_for_ddg)\n",
        "\n",
        "    sample_item = full_dataset[0]\n",
        "    # node_features from dataset item will be (1, N, F) -> shape[1] for N_ROI, shape[2] for F_node_feats\n",
        "    _, N_ROI, F_node_feats = sample_item['node_features'].shape\n",
        "    print(f\"Detected N_ROI: {N_ROI}, F_node_feats: {F_node_feats}\")\n",
        "\n",
        "    # DDGModel is now imported implicitly from the first cell's scope\n",
        "    model = DDGModel(in_feats=F_node_feats, node_dim=32, latent_dim=12, use_vae=True, use_vae_recon=True, recon_T=1).to(device)\n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
        "\n",
        "    best_val_loss = float('inf')\n",
        "    early_stopping_patience = 10\n",
        "    patience_counter = 0\n",
        "\n",
        "    print(\"\\nStarting training...\")\n",
        "    for epoch in range(epochs):\n",
        "        model.train()\n",
        "        total_train_loss = 0.0\n",
        "        for batch_idx, batch in enumerate(train_loader):\n",
        "            node_features = batch['node_features'].to(device)\n",
        "            adjacency_matrices = batch['adjacency_matrices'].to(device)\n",
        "            cognitive_scores = batch['cognitive_scores'].to(device)\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            # DDGModel.forward from the first cell is called\n",
        "            outputs_tuple = model(node_features, adjacency_matrices, rollout_steps=1, teacher_forcing_prob=0.9)\n",
        "\n",
        "            # compute_losses from the first cell is called\n",
        "            loss, sublogs = compute_losses(\n",
        "                {'node_features': node_features, 'adjacency_matrices': adjacency_matrices, 'cognitive_scores': cognitive_scores},\n",
        "                outputs_tuple,\n",
        "                latent_sequence=outputs_tuple[1],\n",
        "                lambda_edge=1.0,\n",
        "                lambda_clinical=0.5,\n",
        "                lambda_latent=0.1\n",
        "            )\n",
        "\n",
        "            loss.backward()\n",
        "            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
        "            optimizer.step()\n",
        "            total_train_loss += loss.item()\n",
        "\n",
        "        avg_train_loss = total_train_loss / len(train_loader)\n",
        "\n",
        "        model.eval()\n",
        "        total_val_loss = 0.0\n",
        "        with torch.no_grad():\n",
        "            for batch in val_loader:\n",
        "                node_features = batch['node_features'].to(device)\n",
        "                adjacency_matrices = batch['adjacency_matrices'].to(device)\n",
        "                cognitive_scores = batch['cognitive_scores'].to(device)\n",
        "\n",
        "                outputs_tuple = model(node_features, adjacency_matrices, rollout_steps=1, teacher_forcing_prob=1.0)\n",
        "                loss, _ = compute_losses(\n",
        "                    {'node_features': node_features, 'adjacency_matrices': adjacency_matrices, 'cognitive_scores': cognitive_scores},\n",
        "                    outputs_tuple,\n",
        "                    latent_sequence=outputs_tuple[1],\n",
        "                    lambda_edge=1.0,\n",
        "                    lambda_clinical=0.5,\n",
        "                    lambda_latent=0.1\n",
        "                )\n",
        "                total_val_loss += loss.item()\n",
        "        avg_val_loss = total_val_loss / len(val_loader)\n",
        "\n",
        "        print(f\"Epoch {epoch+1:02d}: Train Loss={avg_train_loss:.4f} | Val Loss={avg_val_loss:.4f}\")\n",
        "\n",
        "        if avg_val_loss < best_val_loss:\n",
        "            best_val_loss = avg_val_loss\n",
        "            patience_counter = 0\n",
        "        else:\n",
        "            patience_counter += 1\n",
        "            if patience_counter >= early_stopping_patience:\n",
        "                print(f\"Early stopping triggered at epoch {epoch+1}\")\n",
        "                break\n",
        "\n",
        "    print(\"\\nTraining complete!\")\n",
        "    print(f\"Best validation loss: {best_val_loss:.4f}\")\n",
        "\n",
        "    model.eval()\n",
        "    y_true_val = []\n",
        "    y_pred_val = []\n",
        "    with torch.no_grad():\n",
        "        for batch in val_loader:\n",
        "            node_features = batch['node_features'].to(device)\n",
        "            adjacency_matrices = batch['adjacency_matrices'].to(device)\n",
        "            cognitive_scores = batch['cognitive_scores'].to(device)\n",
        "            outputs_tuple = model(node_features, adjacency_matrices, rollout_steps=1, teacher_forcing_prob=1.0)\n",
        "            predicted_score = outputs_tuple[0][0]['predicted_score'].squeeze(-1)\n",
        "            y_true_val.extend(cognitive_scores.cpu().numpy())\n",
        "            y_pred_val.extend(predicted_score.cpu().numpy())\n",
        "\n",
        "    y_true_val = np.array(y_true_val)\n",
        "    y_pred_val = np.array(y_pred_val)\n",
        "    val_mse = np.mean((y_true_val - y_pred_val)**2)\n",
        "    val_mae = np.mean(np.abs(y_true_val - y_pred_val))\n",
        "    print(f\"Validation Clinical MSE: {val_mse:.4f}\")\n",
        "    print(f\"Validation Clinical MAE: {val_mae:.4f}\")\n",
        "\n",
        "    return model\n",
        "\n",
        "# --- Call the main function to run the pipeline ---\n",
        "run_ddg_on_synthetic_data(epochs=30, batch_size=16, lr=1e-3)\n"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Running DDGModel on synthetic data on device cuda\n",
            "================================================================================\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-3033003688.py:66: RuntimeWarning: covariance is not symmetric positive-semidefinite.\n",
            "  ts = np.random.multivariate_normal(\n",
            "/tmp/ipython-input-3033003688.py:66: RuntimeWarning: covariance is not symmetric positive-semidefinite.\n",
            "  ts = np.random.multivariate_normal(\n",
            "/tmp/ipython-input-3033003688.py:66: RuntimeWarning: covariance is not symmetric positive-semidefinite.\n",
            "  ts = np.random.multivariate_normal(\n",
            "/tmp/ipython-input-3033003688.py:66: RuntimeWarning: covariance is not symmetric positive-semidefinite.\n",
            "  ts = np.random.multivariate_normal(\n",
            "/tmp/ipython-input-3033003688.py:66: RuntimeWarning: covariance is not symmetric positive-semidefinite.\n",
            "  ts = np.random.multivariate_normal(\n",
            "/tmp/ipython-input-3033003688.py:66: RuntimeWarning: covariance is not symmetric positive-semidefinite.\n",
            "  ts = np.random.multivariate_normal(\n",
            "/tmp/ipython-input-3033003688.py:66: RuntimeWarning: covariance is not symmetric positive-semidefinite.\n",
            "  ts = np.random.multivariate_normal(\n",
            "/tmp/ipython-input-3033003688.py:66: RuntimeWarning: covariance is not symmetric positive-semidefinite.\n",
            "  ts = np.random.multivariate_normal(\n",
            "/tmp/ipython-input-3033003688.py:66: RuntimeWarning: covariance is not symmetric positive-semidefinite.\n",
            "  ts = np.random.multivariate_normal(\n",
            "/tmp/ipython-input-3033003688.py:66: RuntimeWarning: covariance is not symmetric positive-semidefinite.\n",
            "  ts = np.random.multivariate_normal(\n",
            "/tmp/ipython-input-3033003688.py:66: RuntimeWarning: covariance is not symmetric positive-semidefinite.\n",
            "  ts = np.random.multivariate_normal(\n",
            "/tmp/ipython-input-3033003688.py:66: RuntimeWarning: covariance is not symmetric positive-semidefinite.\n",
            "  ts = np.random.multivariate_normal(\n",
            "/tmp/ipython-input-3033003688.py:66: RuntimeWarning: covariance is not symmetric positive-semidefinite.\n",
            "  ts = np.random.multivariate_normal(\n",
            "/tmp/ipython-input-3033003688.py:66: RuntimeWarning: covariance is not symmetric positive-semidefinite.\n",
            "  ts = np.random.multivariate_normal(\n",
            "/tmp/ipython-input-3033003688.py:66: RuntimeWarning: covariance is not symmetric positive-semidefinite.\n",
            "  ts = np.random.multivariate_normal(\n",
            "/tmp/ipython-input-3033003688.py:66: RuntimeWarning: covariance is not symmetric positive-semidefinite.\n",
            "  ts = np.random.multivariate_normal(\n",
            "/tmp/ipython-input-3033003688.py:66: RuntimeWarning: covariance is not symmetric positive-semidefinite.\n",
            "  ts = np.random.multivariate_normal(\n",
            "/tmp/ipython-input-3033003688.py:66: RuntimeWarning: covariance is not symmetric positive-semidefinite.\n",
            "  ts = np.random.multivariate_normal(\n",
            "/tmp/ipython-input-3033003688.py:66: RuntimeWarning: covariance is not symmetric positive-semidefinite.\n",
            "  ts = np.random.multivariate_normal(\n",
            "/tmp/ipython-input-3033003688.py:66: RuntimeWarning: covariance is not symmetric positive-semidefinite.\n",
            "  ts = np.random.multivariate_normal(\n",
            "/tmp/ipython-input-3033003688.py:66: RuntimeWarning: covariance is not symmetric positive-semidefinite.\n",
            "  ts = np.random.multivariate_normal(\n",
            "/tmp/ipython-input-3033003688.py:66: RuntimeWarning: covariance is not symmetric positive-semidefinite.\n",
            "  ts = np.random.multivariate_normal(\n",
            "/tmp/ipython-input-3033003688.py:66: RuntimeWarning: covariance is not symmetric positive-semidefinite.\n",
            "  ts = np.random.multivariate_normal(\n",
            "/tmp/ipython-input-3033003688.py:66: RuntimeWarning: covariance is not symmetric positive-semidefinite.\n",
            "  ts = np.random.multivariate_normal(\n",
            "/tmp/ipython-input-3033003688.py:66: RuntimeWarning: covariance is not symmetric positive-semidefinite.\n",
            "  ts = np.random.multivariate_normal(\n",
            "/tmp/ipython-input-3033003688.py:66: RuntimeWarning: covariance is not symmetric positive-semidefinite.\n",
            "  ts = np.random.multivariate_normal(\n",
            "/tmp/ipython-input-3033003688.py:66: RuntimeWarning: covariance is not symmetric positive-semidefinite.\n",
            "  ts = np.random.multivariate_normal(\n",
            "/tmp/ipython-input-3033003688.py:66: RuntimeWarning: covariance is not symmetric positive-semidefinite.\n",
            "  ts = np.random.multivariate_normal(\n",
            "/tmp/ipython-input-3033003688.py:66: RuntimeWarning: covariance is not symmetric positive-semidefinite.\n",
            "  ts = np.random.multivariate_normal(\n",
            "/tmp/ipython-input-3033003688.py:66: RuntimeWarning: covariance is not symmetric positive-semidefinite.\n",
            "  ts = np.random.multivariate_normal(\n",
            "/tmp/ipython-input-3033003688.py:66: RuntimeWarning: covariance is not symmetric positive-semidefinite.\n",
            "  ts = np.random.multivariate_normal(\n",
            "/tmp/ipython-input-3033003688.py:66: RuntimeWarning: covariance is not symmetric positive-semidefinite.\n",
            "  ts = np.random.multivariate_normal(\n",
            "/tmp/ipython-input-3033003688.py:66: RuntimeWarning: covariance is not symmetric positive-semidefinite.\n",
            "  ts = np.random.multivariate_normal(\n",
            "/tmp/ipython-input-3033003688.py:66: RuntimeWarning: covariance is not symmetric positive-semidefinite.\n",
            "  ts = np.random.multivariate_normal(\n",
            "/tmp/ipython-input-3033003688.py:66: RuntimeWarning: covariance is not symmetric positive-semidefinite.\n",
            "  ts = np.random.multivariate_normal(\n",
            "/tmp/ipython-input-3033003688.py:66: RuntimeWarning: covariance is not symmetric positive-semidefinite.\n",
            "  ts = np.random.multivariate_normal(\n",
            "/tmp/ipython-input-3033003688.py:66: RuntimeWarning: covariance is not symmetric positive-semidefinite.\n",
            "  ts = np.random.multivariate_normal(\n",
            "/tmp/ipython-input-3033003688.py:66: RuntimeWarning: covariance is not symmetric positive-semidefinite.\n",
            "  ts = np.random.multivariate_normal(\n",
            "/tmp/ipython-input-3033003688.py:66: RuntimeWarning: covariance is not symmetric positive-semidefinite.\n",
            "  ts = np.random.multivariate_normal(\n",
            "/tmp/ipython-input-3033003688.py:66: RuntimeWarning: covariance is not symmetric positive-semidefinite.\n",
            "  ts = np.random.multivariate_normal(\n",
            "/tmp/ipython-input-3033003688.py:66: RuntimeWarning: covariance is not symmetric positive-semidefinite.\n",
            "  ts = np.random.multivariate_normal(\n",
            "/tmp/ipython-input-3033003688.py:66: RuntimeWarning: covariance is not symmetric positive-semidefinite.\n",
            "  ts = np.random.multivariate_normal(\n",
            "/tmp/ipython-input-3033003688.py:66: RuntimeWarning: covariance is not symmetric positive-semidefinite.\n",
            "  ts = np.random.multivariate_normal(\n",
            "/tmp/ipython-input-3033003688.py:66: RuntimeWarning: covariance is not symmetric positive-semidefinite.\n",
            "  ts = np.random.multivariate_normal(\n",
            "/tmp/ipython-input-3033003688.py:66: RuntimeWarning: covariance is not symmetric positive-semidefinite.\n",
            "  ts = np.random.multivariate_normal(\n",
            "/tmp/ipython-input-3033003688.py:66: RuntimeWarning: covariance is not symmetric positive-semidefinite.\n",
            "  ts = np.random.multivariate_normal(\n",
            "/tmp/ipython-input-3033003688.py:66: RuntimeWarning: covariance is not symmetric positive-semidefinite.\n",
            "  ts = np.random.multivariate_normal(\n",
            "/tmp/ipython-input-3033003688.py:66: RuntimeWarning: covariance is not symmetric positive-semidefinite.\n",
            "  ts = np.random.multivariate_normal(\n",
            "/tmp/ipython-input-3033003688.py:66: RuntimeWarning: covariance is not symmetric positive-semidefinite.\n",
            "  ts = np.random.multivariate_normal(\n",
            "/tmp/ipython-input-3033003688.py:66: RuntimeWarning: covariance is not symmetric positive-semidefinite.\n",
            "  ts = np.random.multivariate_normal(\n",
            "/tmp/ipython-input-3033003688.py:66: RuntimeWarning: covariance is not symmetric positive-semidefinite.\n",
            "  ts = np.random.multivariate_normal(\n",
            "/tmp/ipython-input-3033003688.py:66: RuntimeWarning: covariance is not symmetric positive-semidefinite.\n",
            "  ts = np.random.multivariate_normal(\n",
            "/tmp/ipython-input-3033003688.py:66: RuntimeWarning: covariance is not symmetric positive-semidefinite.\n",
            "  ts = np.random.multivariate_normal(\n",
            "/tmp/ipython-input-3033003688.py:66: RuntimeWarning: covariance is not symmetric positive-semidefinite.\n",
            "  ts = np.random.multivariate_normal(\n",
            "/tmp/ipython-input-3033003688.py:66: RuntimeWarning: covariance is not symmetric positive-semidefinite.\n",
            "  ts = np.random.multivariate_normal(\n",
            "/tmp/ipython-input-3033003688.py:66: RuntimeWarning: covariance is not symmetric positive-semidefinite.\n",
            "  ts = np.random.multivariate_normal(\n",
            "/tmp/ipython-input-3033003688.py:66: RuntimeWarning: covariance is not symmetric positive-semidefinite.\n",
            "  ts = np.random.multivariate_normal(\n",
            "/tmp/ipython-input-3033003688.py:66: RuntimeWarning: covariance is not symmetric positive-semidefinite.\n",
            "  ts = np.random.multivariate_normal(\n",
            "/tmp/ipython-input-3033003688.py:66: RuntimeWarning: covariance is not symmetric positive-semidefinite.\n",
            "  ts = np.random.multivariate_normal(\n",
            "/tmp/ipython-input-3033003688.py:66: RuntimeWarning: covariance is not symmetric positive-semidefinite.\n",
            "  ts = np.random.multivariate_normal(\n",
            "/tmp/ipython-input-3033003688.py:66: RuntimeWarning: covariance is not symmetric positive-semidefinite.\n",
            "  ts = np.random.multivariate_normal(\n",
            "/tmp/ipython-input-3033003688.py:66: RuntimeWarning: covariance is not symmetric positive-semidefinite.\n",
            "  ts = np.random.multivariate_normal(\n",
            "/tmp/ipython-input-3033003688.py:66: RuntimeWarning: covariance is not symmetric positive-semidefinite.\n",
            "  ts = np.random.multivariate_normal(\n",
            "/tmp/ipython-input-3033003688.py:66: RuntimeWarning: covariance is not symmetric positive-semidefinite.\n",
            "  ts = np.random.multivariate_normal(\n",
            "/tmp/ipython-input-3033003688.py:66: RuntimeWarning: covariance is not symmetric positive-semidefinite.\n",
            "  ts = np.random.multivariate_normal(\n",
            "/tmp/ipython-input-3033003688.py:66: RuntimeWarning: covariance is not symmetric positive-semidefinite.\n",
            "  ts = np.random.multivariate_normal(\n",
            "/tmp/ipython-input-3033003688.py:66: RuntimeWarning: covariance is not symmetric positive-semidefinite.\n",
            "  ts = np.random.multivariate_normal(\n",
            "/tmp/ipython-input-3033003688.py:66: RuntimeWarning: covariance is not symmetric positive-semidefinite.\n",
            "  ts = np.random.multivariate_normal(\n",
            "/tmp/ipython-input-3033003688.py:66: RuntimeWarning: covariance is not symmetric positive-semidefinite.\n",
            "  ts = np.random.multivariate_normal(\n",
            "/tmp/ipython-input-3033003688.py:66: RuntimeWarning: covariance is not symmetric positive-semidefinite.\n",
            "  ts = np.random.multivariate_normal(\n",
            "/tmp/ipython-input-3033003688.py:66: RuntimeWarning: covariance is not symmetric positive-semidefinite.\n",
            "  ts = np.random.multivariate_normal(\n",
            "/tmp/ipython-input-3033003688.py:66: RuntimeWarning: covariance is not symmetric positive-semidefinite.\n",
            "  ts = np.random.multivariate_normal(\n",
            "/tmp/ipython-input-3033003688.py:66: RuntimeWarning: covariance is not symmetric positive-semidefinite.\n",
            "  ts = np.random.multivariate_normal(\n",
            "/tmp/ipython-input-3033003688.py:66: RuntimeWarning: covariance is not symmetric positive-semidefinite.\n",
            "  ts = np.random.multivariate_normal(\n",
            "/tmp/ipython-input-3033003688.py:66: RuntimeWarning: covariance is not symmetric positive-semidefinite.\n",
            "  ts = np.random.multivariate_normal(\n",
            "/tmp/ipython-input-3033003688.py:66: RuntimeWarning: covariance is not symmetric positive-semidefinite.\n",
            "  ts = np.random.multivariate_normal(\n",
            "/tmp/ipython-input-3033003688.py:66: RuntimeWarning: covariance is not symmetric positive-semidefinite.\n",
            "  ts = np.random.multivariate_normal(\n",
            "/tmp/ipython-input-3033003688.py:66: RuntimeWarning: covariance is not symmetric positive-semidefinite.\n",
            "  ts = np.random.multivariate_normal(\n",
            "/tmp/ipython-input-3033003688.py:66: RuntimeWarning: covariance is not symmetric positive-semidefinite.\n",
            "  ts = np.random.multivariate_normal(\n",
            "/tmp/ipython-input-3033003688.py:66: RuntimeWarning: covariance is not symmetric positive-semidefinite.\n",
            "  ts = np.random.multivariate_normal(\n",
            "/tmp/ipython-input-3033003688.py:66: RuntimeWarning: covariance is not symmetric positive-semidefinite.\n",
            "  ts = np.random.multivariate_normal(\n",
            "/tmp/ipython-input-3033003688.py:66: RuntimeWarning: covariance is not symmetric positive-semidefinite.\n",
            "  ts = np.random.multivariate_normal(\n",
            "/tmp/ipython-input-3033003688.py:66: RuntimeWarning: covariance is not symmetric positive-semidefinite.\n",
            "  ts = np.random.multivariate_normal(\n",
            "/tmp/ipython-input-3033003688.py:66: RuntimeWarning: covariance is not symmetric positive-semidefinite.\n",
            "  ts = np.random.multivariate_normal(\n",
            "/tmp/ipython-input-3033003688.py:66: RuntimeWarning: covariance is not symmetric positive-semidefinite.\n",
            "  ts = np.random.multivariate_normal(\n",
            "/tmp/ipython-input-3033003688.py:66: RuntimeWarning: covariance is not symmetric positive-semidefinite.\n",
            "  ts = np.random.multivariate_normal(\n",
            "/tmp/ipython-input-3033003688.py:66: RuntimeWarning: covariance is not symmetric positive-semidefinite.\n",
            "  ts = np.random.multivariate_normal(\n",
            "/tmp/ipython-input-3033003688.py:66: RuntimeWarning: covariance is not symmetric positive-semidefinite.\n",
            "  ts = np.random.multivariate_normal(\n",
            "/tmp/ipython-input-3033003688.py:66: RuntimeWarning: covariance is not symmetric positive-semidefinite.\n",
            "  ts = np.random.multivariate_normal(\n",
            "/tmp/ipython-input-3033003688.py:66: RuntimeWarning: covariance is not symmetric positive-semidefinite.\n",
            "  ts = np.random.multivariate_normal(\n",
            "/tmp/ipython-input-3033003688.py:66: RuntimeWarning: covariance is not symmetric positive-semidefinite.\n",
            "  ts = np.random.multivariate_normal(\n",
            "/tmp/ipython-input-3033003688.py:66: RuntimeWarning: covariance is not symmetric positive-semidefinite.\n",
            "  ts = np.random.multivariate_normal(\n",
            "/tmp/ipython-input-3033003688.py:66: RuntimeWarning: covariance is not symmetric positive-semidefinite.\n",
            "  ts = np.random.multivariate_normal(\n",
            "/tmp/ipython-input-3033003688.py:66: RuntimeWarning: covariance is not symmetric positive-semidefinite.\n",
            "  ts = np.random.multivariate_normal(\n",
            "/tmp/ipython-input-3033003688.py:66: RuntimeWarning: covariance is not symmetric positive-semidefinite.\n",
            "  ts = np.random.multivariate_normal(\n",
            "/tmp/ipython-input-3033003688.py:66: RuntimeWarning: covariance is not symmetric positive-semidefinite.\n",
            "  ts = np.random.multivariate_normal(\n",
            "/tmp/ipython-input-3033003688.py:66: RuntimeWarning: covariance is not symmetric positive-semidefinite.\n",
            "  ts = np.random.multivariate_normal(\n",
            "/tmp/ipython-input-3033003688.py:66: RuntimeWarning: covariance is not symmetric positive-semidefinite.\n",
            "  ts = np.random.multivariate_normal(\n",
            "/tmp/ipython-input-3033003688.py:66: RuntimeWarning: covariance is not symmetric positive-semidefinite.\n",
            "  ts = np.random.multivariate_normal(\n",
            "/tmp/ipython-input-3033003688.py:66: RuntimeWarning: covariance is not symmetric positive-semidefinite.\n",
            "  ts = np.random.multivariate_normal(\n",
            "/tmp/ipython-input-3033003688.py:66: RuntimeWarning: covariance is not symmetric positive-semidefinite.\n",
            "  ts = np.random.multivariate_normal(\n",
            "/tmp/ipython-input-3033003688.py:66: RuntimeWarning: covariance is not symmetric positive-semidefinite.\n",
            "  ts = np.random.multivariate_normal(\n",
            "/tmp/ipython-input-3033003688.py:66: RuntimeWarning: covariance is not symmetric positive-semidefinite.\n",
            "  ts = np.random.multivariate_normal(\n",
            "/tmp/ipython-input-3033003688.py:66: RuntimeWarning: covariance is not symmetric positive-semidefinite.\n",
            "  ts = np.random.multivariate_normal(\n",
            "/tmp/ipython-input-3033003688.py:66: RuntimeWarning: covariance is not symmetric positive-semidefinite.\n",
            "  ts = np.random.multivariate_normal(\n",
            "/tmp/ipython-input-3033003688.py:66: RuntimeWarning: covariance is not symmetric positive-semidefinite.\n",
            "  ts = np.random.multivariate_normal(\n",
            "/tmp/ipython-input-3033003688.py:66: RuntimeWarning: covariance is not symmetric positive-semidefinite.\n",
            "  ts = np.random.multivariate_normal(\n",
            "/tmp/ipython-input-3033003688.py:66: RuntimeWarning: covariance is not symmetric positive-semidefinite.\n",
            "  ts = np.random.multivariate_normal(\n",
            "/tmp/ipython-input-3033003688.py:66: RuntimeWarning: covariance is not symmetric positive-semidefinite.\n",
            "  ts = np.random.multivariate_normal(\n",
            "/tmp/ipython-input-3033003688.py:66: RuntimeWarning: covariance is not symmetric positive-semidefinite.\n",
            "  ts = np.random.multivariate_normal(\n",
            "/tmp/ipython-input-3033003688.py:66: RuntimeWarning: covariance is not symmetric positive-semidefinite.\n",
            "  ts = np.random.multivariate_normal(\n",
            "/tmp/ipython-input-3033003688.py:66: RuntimeWarning: covariance is not symmetric positive-semidefinite.\n",
            "  ts = np.random.multivariate_normal(\n",
            "/tmp/ipython-input-3033003688.py:66: RuntimeWarning: covariance is not symmetric positive-semidefinite.\n",
            "  ts = np.random.multivariate_normal(\n",
            "/tmp/ipython-input-3033003688.py:66: RuntimeWarning: covariance is not symmetric positive-semidefinite.\n",
            "  ts = np.random.multivariate_normal(\n",
            "/tmp/ipython-input-3033003688.py:66: RuntimeWarning: covariance is not symmetric positive-semidefinite.\n",
            "  ts = np.random.multivariate_normal(\n",
            "/tmp/ipython-input-3033003688.py:66: RuntimeWarning: covariance is not symmetric positive-semidefinite.\n",
            "  ts = np.random.multivariate_normal(\n",
            "/tmp/ipython-input-3033003688.py:66: RuntimeWarning: covariance is not symmetric positive-semidefinite.\n",
            "  ts = np.random.multivariate_normal(\n",
            "/tmp/ipython-input-3033003688.py:66: RuntimeWarning: covariance is not symmetric positive-semidefinite.\n",
            "  ts = np.random.multivariate_normal(\n",
            "/tmp/ipython-input-3033003688.py:66: RuntimeWarning: covariance is not symmetric positive-semidefinite.\n",
            "  ts = np.random.multivariate_normal(\n",
            "/tmp/ipython-input-3033003688.py:66: RuntimeWarning: covariance is not symmetric positive-semidefinite.\n",
            "  ts = np.random.multivariate_normal(\n",
            "/tmp/ipython-input-3033003688.py:66: RuntimeWarning: covariance is not symmetric positive-semidefinite.\n",
            "  ts = np.random.multivariate_normal(\n",
            "/tmp/ipython-input-3033003688.py:66: RuntimeWarning: covariance is not symmetric positive-semidefinite.\n",
            "  ts = np.random.multivariate_normal(\n",
            "/tmp/ipython-input-3033003688.py:66: RuntimeWarning: covariance is not symmetric positive-semidefinite.\n",
            "  ts = np.random.multivariate_normal(\n",
            "/tmp/ipython-input-3033003688.py:66: RuntimeWarning: covariance is not symmetric positive-semidefinite.\n",
            "  ts = np.random.multivariate_normal(\n",
            "/tmp/ipython-input-3033003688.py:66: RuntimeWarning: covariance is not symmetric positive-semidefinite.\n",
            "  ts = np.random.multivariate_normal(\n",
            "/tmp/ipython-input-3033003688.py:66: RuntimeWarning: covariance is not symmetric positive-semidefinite.\n",
            "  ts = np.random.multivariate_normal(\n",
            "/tmp/ipython-input-3033003688.py:66: RuntimeWarning: covariance is not symmetric positive-semidefinite.\n",
            "  ts = np.random.multivariate_normal(\n",
            "/tmp/ipython-input-3033003688.py:66: RuntimeWarning: covariance is not symmetric positive-semidefinite.\n",
            "  ts = np.random.multivariate_normal(\n",
            "/tmp/ipython-input-3033003688.py:66: RuntimeWarning: covariance is not symmetric positive-semidefinite.\n",
            "  ts = np.random.multivariate_normal(\n",
            "/tmp/ipython-input-3033003688.py:66: RuntimeWarning: covariance is not symmetric positive-semidefinite.\n",
            "  ts = np.random.multivariate_normal(\n",
            "/tmp/ipython-input-3033003688.py:66: RuntimeWarning: covariance is not symmetric positive-semidefinite.\n",
            "  ts = np.random.multivariate_normal(\n",
            "/tmp/ipython-input-3033003688.py:66: RuntimeWarning: covariance is not symmetric positive-semidefinite.\n",
            "  ts = np.random.multivariate_normal(\n",
            "/tmp/ipython-input-3033003688.py:66: RuntimeWarning: covariance is not symmetric positive-semidefinite.\n",
            "  ts = np.random.multivariate_normal(\n",
            "/tmp/ipython-input-3033003688.py:66: RuntimeWarning: covariance is not symmetric positive-semidefinite.\n",
            "  ts = np.random.multivariate_normal(\n",
            "/tmp/ipython-input-3033003688.py:66: RuntimeWarning: covariance is not symmetric positive-semidefinite.\n",
            "  ts = np.random.multivariate_normal(\n",
            "/tmp/ipython-input-3033003688.py:66: RuntimeWarning: covariance is not symmetric positive-semidefinite.\n",
            "  ts = np.random.multivariate_normal(\n",
            "/tmp/ipython-input-3033003688.py:66: RuntimeWarning: covariance is not symmetric positive-semidefinite.\n",
            "  ts = np.random.multivariate_normal(\n",
            "/tmp/ipython-input-3033003688.py:66: RuntimeWarning: covariance is not symmetric positive-semidefinite.\n",
            "  ts = np.random.multivariate_normal(\n",
            "/tmp/ipython-input-3033003688.py:66: RuntimeWarning: covariance is not symmetric positive-semidefinite.\n",
            "  ts = np.random.multivariate_normal(\n",
            "/tmp/ipython-input-3033003688.py:66: RuntimeWarning: covariance is not symmetric positive-semidefinite.\n",
            "  ts = np.random.multivariate_normal(\n",
            "/tmp/ipython-input-3033003688.py:66: RuntimeWarning: covariance is not symmetric positive-semidefinite.\n",
            "  ts = np.random.multivariate_normal(\n",
            "/tmp/ipython-input-3033003688.py:66: RuntimeWarning: covariance is not symmetric positive-semidefinite.\n",
            "  ts = np.random.multivariate_normal(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Synthetic ADNI-style dataset created\n",
            "Subjects: 240\n",
            "ROIs: 90\n",
            "Saved to: data\n",
            "Detected N_ROI: 90, F_node_feats: 4\n",
            "\n",
            "Starting training...\n",
            "Epoch 01: Train Loss=257.5094 | Val Loss=247.8155\n",
            "Epoch 02: Train Loss=246.4783 | Val Loss=249.5082\n",
            "Epoch 03: Train Loss=235.3674 | Val Loss=224.7739\n",
            "Epoch 04: Train Loss=218.2609 | Val Loss=201.8700\n",
            "Epoch 05: Train Loss=190.2428 | Val Loss=157.9890\n",
            "Epoch 06: Train Loss=151.3968 | Val Loss=119.3452\n",
            "Epoch 07: Train Loss=108.8405 | Val Loss=93.7289\n",
            "Epoch 08: Train Loss=85.8454 | Val Loss=69.2197\n",
            "Epoch 09: Train Loss=72.5778 | Val Loss=55.4966\n",
            "Epoch 10: Train Loss=63.4773 | Val Loss=50.6472\n",
            "Epoch 11: Train Loss=62.9777 | Val Loss=50.9539\n",
            "Epoch 12: Train Loss=52.8257 | Val Loss=44.9058\n",
            "Epoch 13: Train Loss=59.3330 | Val Loss=46.4434\n",
            "Epoch 14: Train Loss=52.8576 | Val Loss=40.3788\n",
            "Epoch 15: Train Loss=51.1982 | Val Loss=44.6345\n",
            "Epoch 16: Train Loss=52.7767 | Val Loss=49.8825\n",
            "Epoch 17: Train Loss=49.9675 | Val Loss=42.2542\n",
            "Epoch 18: Train Loss=51.8777 | Val Loss=49.0345\n",
            "Epoch 19: Train Loss=49.0138 | Val Loss=44.2382\n",
            "Epoch 20: Train Loss=45.4856 | Val Loss=47.4116\n",
            "Epoch 21: Train Loss=47.1817 | Val Loss=38.4649\n",
            "Epoch 22: Train Loss=49.8618 | Val Loss=39.3755\n",
            "Epoch 23: Train Loss=47.4722 | Val Loss=38.9240\n",
            "Epoch 24: Train Loss=46.5166 | Val Loss=39.2134\n",
            "Epoch 25: Train Loss=46.0589 | Val Loss=38.7846\n",
            "Epoch 26: Train Loss=46.7801 | Val Loss=37.4221\n",
            "Epoch 27: Train Loss=48.3982 | Val Loss=38.6403\n",
            "Epoch 28: Train Loss=46.9658 | Val Loss=39.2006\n",
            "Epoch 29: Train Loss=49.7739 | Val Loss=38.2670\n",
            "Epoch 30: Train Loss=47.6614 | Val Loss=40.5654\n",
            "\n",
            "Training complete!\n",
            "Best validation loss: 37.4221\n",
            "Validation Clinical MSE: 64.5977\n",
            "Validation Clinical MAE: 6.7516\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "DDGModel(\n",
              "  (encoder): GraphEncoder(\n",
              "    (gat): GATEncoder(\n",
              "      (fc): Linear(in_features=4, out_features=32, bias=False)\n",
              "      (leaky): LeakyReLU(negative_slope=0.2)\n",
              "      (out_proj): Linear(in_features=32, out_features=32, bias=True)\n",
              "    )\n",
              "  )\n",
              "  (z_encoder): VAEZEncoder(\n",
              "    (gru): GRU(32, 64, batch_first=True)\n",
              "    (fc_mu): Linear(in_features=64, out_features=12, bias=True)\n",
              "    (fc_logvar): Linear(in_features=64, out_features=12, bias=True)\n",
              "  )\n",
              "  (reconstructor): VAEReconstructor(\n",
              "    (z_to_hidden): Linear(in_features=12, out_features=128, bias=True)\n",
              "    (gru): GRU(32, 128, batch_first=True)\n",
              "    (readout): Sequential(\n",
              "      (0): Linear(in_features=160, out_features=128, bias=True)\n",
              "      (1): ReLU()\n",
              "      (2): Linear(in_features=128, out_features=4, bias=True)\n",
              "    )\n",
              "  )\n",
              "  (transformer): GraphTransformer(\n",
              "    (query_proj): Linear(in_features=32, out_features=32, bias=True)\n",
              "    (key_proj): Linear(in_features=32, out_features=32, bias=True)\n",
              "    (value_proj): Linear(in_features=32, out_features=32, bias=True)\n",
              "    (out_proj): Linear(in_features=32, out_features=32, bias=True)\n",
              "    (norm): LayerNorm((32,), eps=1e-05, elementwise_affine=True)\n",
              "    (feed_forward): Sequential(\n",
              "      (0): Linear(in_features=32, out_features=64, bias=True)\n",
              "      (1): ReLU()\n",
              "      (2): Linear(in_features=64, out_features=32, bias=True)\n",
              "    )\n",
              "    (latent_to_attention_scale): Sequential(\n",
              "      (0): Linear(in_features=12, out_features=64, bias=True)\n",
              "      (1): ReLU()\n",
              "      (2): Linear(in_features=64, out_features=4, bias=True)\n",
              "      (3): Softplus(beta=1.0, threshold=20.0)\n",
              "    )\n",
              "  )\n",
              "  (evolution): PerEdgeMLP(\n",
              "    (mlp): Sequential(\n",
              "      (0): Linear(in_features=77, out_features=64, bias=True)\n",
              "      (1): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (2): ReLU()\n",
              "      (3): Dropout(p=0.25, inplace=False)\n",
              "      (4): Linear(in_features=64, out_features=64, bias=True)\n",
              "      (5): ReLU()\n",
              "      (6): Dropout(p=0.2, inplace=False)\n",
              "      (7): Linear(in_features=64, out_features=1, bias=True)\n",
              "    )\n",
              "  )\n",
              "  (edge_decoder): EdgeDecoder(\n",
              "    (readout): Sequential(\n",
              "      (0): Linear(in_features=76, out_features=128, bias=True)\n",
              "      (1): ReLU()\n",
              "      (2): Dropout(p=0.2, inplace=False)\n",
              "      (3): Linear(in_features=128, out_features=1, bias=True)\n",
              "    )\n",
              "  )\n",
              "  (clinical_decoder): ClinicalDecoder(\n",
              "    (mlp): Sequential(\n",
              "      (0): Linear(in_features=44, out_features=64, bias=True)\n",
              "      (1): ReLU()\n",
              "      (2): Linear(in_features=64, out_features=1, bias=True)\n",
              "    )\n",
              "  )\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    }
  ]
}
